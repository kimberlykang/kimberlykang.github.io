<!DOCTYPE html>
<html lang="en" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="In this post, we review Microsoft&rsquo;s 2020 paper &ldquo;UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training&rdquo;. http://proceedings.mlr.press/v119/bao20a/bao20a.pdf Table of Contents Unified Language Model Pre-Train Fine-tuning Result Unified Language Model Before this work, language model studies were largely split between bidirectional models good at Natural Language Understanding (NLU), and autoregressive models good at Natural Language Generation (NLG). This paper proposes a Unified Language Model called Pseudo-Masked Language Models (PMLM), which can perform both AutoEncoding (AE) and Partially AutoRegressive (PAR) language modeling at the same time using pseudo-masks on masked tokens—enabling both strong understanding and generation.">
<title>UniLMv2 Review</title>

<link rel='canonical' href='https://kimberlykang.github.io/p/unilmv2_review/'>

<link rel="stylesheet" href="/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css"><meta property='og:title' content="UniLMv2 Review">
<meta property='og:description' content="In this post, we review Microsoft&rsquo;s 2020 paper &ldquo;UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training&rdquo;. http://proceedings.mlr.press/v119/bao20a/bao20a.pdf Table of Contents Unified Language Model Pre-Train Fine-tuning Result Unified Language Model Before this work, language model studies were largely split between bidirectional models good at Natural Language Understanding (NLU), and autoregressive models good at Natural Language Generation (NLG). This paper proposes a Unified Language Model called Pseudo-Masked Language Models (PMLM), which can perform both AutoEncoding (AE) and Partially AutoRegressive (PAR) language modeling at the same time using pseudo-masks on masked tokens—enabling both strong understanding and generation.">
<meta property='og:url' content='https://kimberlykang.github.io/p/unilmv2_review/'>
<meta property='og:site_name' content='Kimberly&#39;s Blog'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='UniLMv2' /><meta property='article:published_time' content='2023-10-16T00:17:41&#43;09:00'/><meta property='article:modified_time' content='2023-10-16T00:17:41&#43;09:00'/>
<meta name="twitter:title" content="UniLMv2 Review">
<meta name="twitter:description" content="In this post, we review Microsoft&rsquo;s 2020 paper &ldquo;UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training&rdquo;. http://proceedings.mlr.press/v119/bao20a/bao20a.pdf Table of Contents Unified Language Model Pre-Train Fine-tuning Result Unified Language Model Before this work, language model studies were largely split between bidirectional models good at Natural Language Understanding (NLU), and autoregressive models good at Natural Language Generation (NLG). This paper proposes a Unified Language Model called Pseudo-Masked Language Models (PMLM), which can perform both AutoEncoding (AE) and Partially AutoRegressive (PAR) language modeling at the same time using pseudo-masks on masked tokens—enabling both strong understanding and generation."><style>
     
    :root {
        --article-font-family: "Open Sans", var(--base-font-family);
    }

     
    * {
        font-family: "Open Sans", var(--base-font-family);
    }

     
    *:lang(ko),
    *[lang="ko"],
    *[lang="ko-KR"],
    *[lang="ko-KP"] {
        font-family: "Nanum Gothic", var(--base-font-family);
    }

     
    *:lang(ko) *:not(:lang(ko)),
    *[lang="ko"] *:not([lang="ko"]),
    *[lang="ko-KR"] *:not([lang="ko-KR"]),
    *[lang="ko-KP"] *:not([lang="ko-KP"]) {
        font-family: "Open Sans", var(--base-font-family);
    }
</style>

<script>
    (function () {
        
        const nanumGothic = document.createElement('link');
        nanumGothic.href = "https://fonts.googleapis.com/css?family=Nanum+Gothic:400";
        nanumGothic.type = "text/css";
        nanumGothic.rel = "stylesheet";
        document.head.appendChild(nanumGothic);

        
        const openSans = document.createElement('link');
        openSans.href = "https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap";
        openSans.type = "text/css";
        openSans.rel = "stylesheet";
        document.head.appendChild(openSans);
    }());
</script>
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/kimberly_avatar_hu92a1a2905f7ef83b94672016a24ef023_184760_300x0_resize_q75_box.jpeg" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">Kimberly&#39;s Blog</a></h1>
            <h2 class="site-description">Vision Engineer</h2>
        </div>
    </header><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
        
        <li >
            <a href='/links/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>Links</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">
                    
                        <li id="i18n-switch">  
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M4 5h7" />
  <path d="M9 3v2c0 4.418 -2.239 8 -5 8" />
  <path d="M5 9c-.003 2.144 2.952 3.908 6.7 4" />
  <path d="M12 20l4 -9l4 9" />
  <path d="M19.1 18h-6.2" />
</svg>



                            <select name="language" title="language" onchange="window.location.href = this.selectedOptions[0].value">
                                
                                    <option value="https://kimberlykang.github.io/" selected>English</option>
                                
                                    <option value="https://kimberlykang.github.io/ko/" >Korean</option>
                                
                            </select>
                        </li>
                    
                

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>Dark Mode</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#unified-language-model">Unified Language Model</a></li>
    <li><a href="#pre-train">Pre-Train</a>
      <ol>
        <li><a href="#ae-modeling">AE Modeling</a></li>
        <li><a href="#par-modeling">PAR Modeling</a></li>
        <li><a href="#pseudo-masked-lm">Pseudo-Masked LM</a></li>
      </ol>
    </li>
    <li><a href="#fine-tuning">Fine-tuning</a></li>
    <li><a href="#result">Result</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/paper-review/" style="background-color: #2a9d8f; color: #fff;">
                Paper review
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/unilmv2_review/">UniLMv2 Review</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Oct 16, 2023</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    5 minute read
                </time>
            </div>
        
    </footer>
    

    
        <footer class="article-translations">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M4 5h7" />
  <path d="M9 3v2c0 4.418 -2.239 8 -5 8" />
  <path d="M5 9c-.003 2.144 2.952 3.908 6.7 4" />
  <path d="M12 20l4 -9l4 9" />
  <path d="M19.1 18h-6.2" />
</svg>



            <div>
                
                    <a href="https://kimberlykang.github.io/ko/p/unilmv2_review/" class="link">Korean</a>
                
            </div>
        </footer>
    
</div>

</header>

    <section class="article-content">
    
    
    <p>In this post, we review Microsoft&rsquo;s 2020 paper &ldquo;UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training&rdquo;.<br>
<a class="link" href="http://proceedings.mlr.press/v119/bao20a/bao20a.pdf"  target="_blank" rel="noopener"
    >http://proceedings.mlr.press/v119/bao20a/bao20a.pdf</a></p>
<h1 id="table-of-contents">Table of Contents
</h1><ol>
<li><a class="link" href="#unified-language-model" >Unified Language Model</a></li>
<li><a class="link" href="#pre-train" >Pre-Train</a></li>
<li><a class="link" href="#fine-tuning" >Fine-tuning</a></li>
<li><a class="link" href="#result" >Result</a></li>
</ol>
<h2 id="unified-language-model">Unified Language Model
</h2><ul>
<li>
<p>Before this work, language model studies were largely split between bidirectional models good at Natural Language Understanding (NLU), and autoregressive models good at Natural Language Generation (NLG).</p>
</li>
<li>
<p>This paper proposes a Unified Language Model called Pseudo-Masked Language Models (PMLM), which can perform both AutoEncoding (AE) and Partially AutoRegressive (PAR) language modeling at the same time using pseudo-masks on masked tokens—enabling both strong understanding and generation.</p>
</li>
<li>
<p>AE enables bi-directional LM training and learns the inter-relation between masked tokens and the context.</p>
</li>
<li>
<p>PAR enables sequence-to-sequence LM training and learns the intra-relation within masked spans.</p>
</li>
<li>
<p>The unified model shares parameters for both bi-directional (AE) and sequence-to-sequence (PAR) pre-training.</p>
</li>
<li>
<p>Parameters are shared as shown in the figure below for the two language model tasks:
<img src="/p/unilmv2_review/pseudo_masked_lm.png"
	width="684"
	height="814"
	srcset="/p/unilmv2_review/pseudo_masked_lm_huda717bd749e912eedfbb4ab676601336_77841_480x0_resize_box_3.png 480w, /p/unilmv2_review/pseudo_masked_lm_huda717bd749e912eedfbb4ab676601336_77841_1024x0_resize_box_3.png 1024w"
	loading="lazy"
	
		alt="Pseudo Masked  LM"
	
	
		class="gallery-image" 
		data-flex-grow="84"
		data-flex-basis="201px"
	
><br>
<span style="font-size:70%"> Source: <a class="link" href="http://proceedings.mlr.press/v119/bao20a/bao20a.pdf"  target="_blank" rel="noopener"
    ><em>Bao, Hangbo, et al. &ldquo;Unilmv2: Pseudo-masked language models for unified language model pre-training.&rdquo; International conference on machine learning. PMLR, 2020, Figure 1.</em></a></span></p>
</li>
<li>
<p>AE uses masked tokens <code>[MASK]</code> or <code>[M]</code>.</p>
</li>
<li>
<p>PAR uses additional masks <code>[Pseudo]</code> or <code>[P]</code> or sometimes the original token, depending on the factorization step.</p>
</li>
<li>
<p>The model can be trained with both modes in a single forward pass.</p>
</li>
</ul>
<h2 id="pre-train">Pre-Train
</h2><ul>
<li>Input Representation: <code>[SOS]</code> <code>s1</code> <code>[EOS]</code> <code>s2</code> <code>[EOS]</code></li>
<li>The paper divides Masked Language Modeling (MLM) into AE, AutoRegressive (AR), and PAR models, which are differentiated by how the probabilities of masked tokens are factorized. PMLM uses both AE and PAR.
<img src="/p/unilmv2_review/masked_language_model_factorization.png"
	width="1549"
	height="314"
	srcset="/p/unilmv2_review/masked_language_model_factorization_hudeb839cc78e377e439641c0d898b8cfa_114452_480x0_resize_box_3.png 480w, /p/unilmv2_review/masked_language_model_factorization_hudeb839cc78e377e439641c0d898b8cfa_114452_1024x0_resize_box_3.png 1024w"
	loading="lazy"
	
		alt="Masked Language Model Factorization"
	
	
		class="gallery-image" 
		data-flex-grow="493"
		data-flex-basis="1183px"
	
><br>
<span style="font-size:70%"> Source: <a class="link" href="http://proceedings.mlr.press/v119/bao20a/bao20a.pdf"  target="_blank" rel="noopener"
    ><em>Bao, Hangbo, et al. &ldquo;Unilmv2: Pseudo-masked language models for unified language model pre-training.&rdquo; International conference on machine learning. PMLR, 2020, Table 1.</em></a></span></li>
</ul>
<h3 id="ae-modeling">AE Modeling
</h3><ul>
<li>Some tokens in the sentence are masked (<code>[MASK]</code>), and the model tries to predict these.</li>
<li>For input $ x=x_1 &hellip; x_{|x|} $, mask positions $ M={m_1, &hellip;, m_{|M|}} $, the probability of the masked tokens is $ \prod\limits_{m∈M} p(x_m|x_{\backslash M})$.
<ul>
<li>$x_M={\lbrace x_m \rbrace}_{m∈M}$: all masked tokens.</li>
<li>$x_{\backslash M}$: all unmasked input tokens. $\backslash$ indicates set difference.</li>
</ul>
</li>
<li>AE pre-training loss:
<ul>
<li>$ {\cal L_{\rm AE}}= - \displaystyle\sum_{x∈{\cal D}} \log \prod\limits_{m∈M} p(x_m|x_{\backslash M}) $
<ul>
<li>$ {\cal D} $: training corpus.</li>
<li>Uses cross-entropy loss.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="par-modeling">PAR Modeling
</h3><ul>
<li>Multiple tokens are masked and predicted sequentially.</li>
<li>$ p(x_M | x_{\backslash M}) = \displaystyle\prod\limits_{i=1}^{|M|} p(x_{M_i}|x_{\backslash M_{\ge i}}) \\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  = \displaystyle\prod\limits_{i=1}^{|M|} \displaystyle\prod\limits_{m∈M_i} p(x_m|x_{\backslash M_{\ge i}})$
<ul>
<li>At each factorization step, the model predicts one or more tokens.</li>
<li>$ M= \langle M_1, &hellip;, M_{|M|} \rangle $: the sequence in which factors (sets of masks) are revealed.</li>
<li>$ M_i= \lbrace m_1^i, &hellip;, m_{|M_i|}^i \rbrace $: set of positions masked at the $i$-th step.</li>
<li>$x_{M_i}={\lbrace x_m \rbrace}_{m∈M_i}$: masked tokens at the $i$-th step.</li>
</ul>
</li>
<li>PAR pre-training loss:
<ul>
<li>$ {\cal L_{\rm AE}}= - \displaystyle\sum_{x∈{\cal D}} \Bbb E_M \log p(x_M|x_{\backslash M}) $
<ul>
<li>$M$ is randomly chosen for each sequence.</li>
</ul>
</li>
</ul>
</li>
<li>Blockwise Masking and Factorization:
<ul>
<li>15% of tokens are selected. For 40% of these, an n-gram block is masked; for 60%, a single token is masked.</li>
<li>Factorization steps can be spans, making the approach partially autoregressive.
<img src="/p/unilmv2_review/algorithm1.png"
	width="644"
	height="340"
	srcset="/p/unilmv2_review/algorithm1_hu84c523000ec854e4c0f493801b44bae1_59471_480x0_resize_box_3.png 480w, /p/unilmv2_review/algorithm1_hu84c523000ec854e4c0f493801b44bae1_59471_1024x0_resize_box_3.png 1024w"
	loading="lazy"
	
		alt="Blockwise Masking"
	
	
		class="gallery-image" 
		data-flex-grow="189"
		data-flex-basis="454px"
	
><br>
<span style="font-size:70%"> Source: <a class="link" href="http://proceedings.mlr.press/v119/bao20a/bao20a.pdf"  target="_blank" rel="noopener"
    ><em>Bao, Hangbo, et al. &ldquo;Unilmv2: Pseudo-masked language models for unified language model pre-training.&rdquo; International conference on machine learning. PMLR, 2020, Algorithm 1.</em></a></span>
<img src="/p/unilmv2_review/pre_training.png"
	width="1577"
	height="801"
	srcset="/p/unilmv2_review/pre_training_hu4ce82dbf2f4858d51fb3ef4a55142683_261062_480x0_resize_box_3.png 480w, /p/unilmv2_review/pre_training_hu4ce82dbf2f4858d51fb3ef4a55142683_261062_1024x0_resize_box_3.png 1024w"
	loading="lazy"
	
		alt="Pre-training"
	
	
		class="gallery-image" 
		data-flex-grow="196"
		data-flex-basis="472px"
	
><br>
<span style="font-size:70%"> Source: <a class="link" href="http://proceedings.mlr.press/v119/bao20a/bao20a.pdf"  target="_blank" rel="noopener"
    ><em>Bao, Hangbo, et al. &ldquo;Unilmv2: Pseudo-masked language models for unified language model pre-training.&rdquo; International conference on machine learning. PMLR, 2020, Figure 2.</em></a></span></li>
</ul>
</li>
</ul>
<h3 id="pseudo-masked-lm">Pseudo-Masked LM
</h3><ul>
<li>The AE (AutoEncoding) method uses <code>[MASK]</code> at prediction positions and treats all other tokens as context to predict all at once.</li>
<li>In AR and PAR, the model predicts tokens one (or several) at a time in sequence, and the mask positions change according to the factorization order each time.</li>
<li>As the image below shows, AE uses a single mask, but AR and PAR use different masks at each factorization step. To reflect these and facilitate learning, pseudo-masks are used.
<img src="/p/unilmv2_review/comparison_ae_ar_par.png"
	width="704"
	height="653"
	srcset="/p/unilmv2_review/comparison_ae_ar_par_hucf9af87b7af5a70e9cea3dc68645f698_52849_480x0_resize_box_3.png 480w, /p/unilmv2_review/comparison_ae_ar_par_hucf9af87b7af5a70e9cea3dc68645f698_52849_1024x0_resize_box_3.png 1024w"
	loading="lazy"
	
		alt="AE vs. AR vs. PAR"
	
	
		class="gallery-image" 
		data-flex-grow="107"
		data-flex-basis="258px"
	
><br>
<span style="font-size:70%"> Source: <a class="link" href="http://proceedings.mlr.press/v119/bao20a/bao20a.pdf"  target="_blank" rel="noopener"
    ><em>Bao, Hangbo, et al. &ldquo;Unilmv2: Pseudo-masked language models for unified language model pre-training.&rdquo; International conference on machine learning. PMLR, 2020, Figure 3.</em></a></span></li>
<li>The following image illustrates the PAR prediction process with pseudo-masks.
<img src="/p/unilmv2_review/factorization_step_viz.png"
	width="497"
	height="465"
	srcset="/p/unilmv2_review/factorization_step_viz_huc910acaf3ad2cb3e66aa8d9dd88ca1b0_63254_480x0_resize_box_3.png 480w, /p/unilmv2_review/factorization_step_viz_huc910acaf3ad2cb3e66aa8d9dd88ca1b0_63254_1024x0_resize_box_3.png 1024w"
	loading="lazy"
	
		alt="Factorization Steps"
	
	
		class="gallery-image" 
		data-flex-grow="106"
		data-flex-basis="256px"
	
><br>
<span style="font-size:70%"> Source: <a class="link" href="http://proceedings.mlr.press/v119/bao20a/bao20a.pdf"  target="_blank" rel="noopener"
    ><em>Bao, Hangbo, et al. &ldquo;Unilmv2: Pseudo-masked language models for unified language model pre-training.&rdquo; International conference on machine learning. PMLR, 2020, Figure 4.</em></a></span>
<ul>
<li>A pseudo-mask <code>[P]</code> is appended to the input sequence, while keeping the original tokens.</li>
<li>Position embeddings are the same as for the original tokens, so the location in the input sequence does not matter.</li>
<li>MLM prediction is performed by applying a softmax classifier to the final hidden state of <code>[P]</code>.</li>
<li>The example factorization order is 4,5 → 2.</li>
<li>At t=1, the pseudo mask attends to $ x_1, x_3, x_6 $ and $ x_4, x_5 $.</li>
<li>At t=2, the pseudo mask attends to $ x_1, x_3, x_4, x_5, x_6 $ and $ x_2 $.</li>
<li>The self-attention mask for the above factorization is shown below.
<img src="/p/unilmv2_review/attention_mask.png"
	width="605"
	height="541"
	srcset="/p/unilmv2_review/attention_mask_hu8d9895acad4c2d2ea48a55b2e0c531e3_44468_480x0_resize_box_3.png 480w, /p/unilmv2_review/attention_mask_hu8d9895acad4c2d2ea48a55b2e0c531e3_44468_1024x0_resize_box_3.png 1024w"
	loading="lazy"
	
		alt="Self-Attention mask"
	
	
		class="gallery-image" 
		data-flex-grow="111"
		data-flex-basis="268px"
	
><br>
<span style="font-size:70%"> Source: <a class="link" href="http://proceedings.mlr.press/v119/bao20a/bao20a.pdf"  target="_blank" rel="noopener"
    ><em>Bao, Hangbo, et al. &ldquo;Unilmv2: Pseudo-masked language models for unified language model pre-training.&rdquo; International conference on machine learning. PMLR, 2020, Figure 5.</em></a></span></li>
</ul>
</li>
</ul>
<h2 id="fine-tuning">Fine-tuning
</h2><ul>
<li><strong>Natural Language Understanding (NLU) - Classification</strong>
<ul>
<li><code>[SOS]</code> TEXT <code>[EOS]</code></li>
<li>Apply a softmax classifier to <code>[SOS]</code> to determine the class with the highest likelihood.</li>
</ul>
</li>
<li><strong>Natural Language Generation (NLG)</strong>
<ul>
<li><code>[SOS]</code> SRC <code>[EOS]</code> TGT <code>[EOS]</code></li>
<li>Source sequence: bidirectional (tokens can attend to each other).</li>
<li>Target sequence: autoregressive. Add <code>[P]</code> to the target tokens.</li>
<li>Select the token that maximizes the likelihood in the target sequence.</li>
</ul>
</li>
</ul>
<h2 id="result">Result
</h2><p><img src="/p/unilmv2_review/result.png"
	width="1600"
	height="320"
	srcset="/p/unilmv2_review/result_huf82132bd036f38b8a7a3f867c4dc6e22_129150_480x0_resize_box_3.png 480w, /p/unilmv2_review/result_huf82132bd036f38b8a7a3f867c4dc6e22_129150_1024x0_resize_box_3.png 1024w"
	loading="lazy"
	
		alt="Result"
	
	
		class="gallery-image" 
		data-flex-grow="500"
		data-flex-basis="1200px"
	
><br>
<span style="font-size:70%"> Source: <a class="link" href="http://proceedings.mlr.press/v119/bao20a/bao20a.pdf"  target="_blank" rel="noopener"
    ><em>Bao, Hangbo, et al. &ldquo;Unilmv2: Pseudo-masked language models for unified language model pre-training.&rdquo; International conference on machine learning. PMLR, 2020, Table 2.</em></a></span></p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/unilmv2/">UniLMv2</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
	const mainArticleElement = document.querySelector(".main-article");
        renderMathInElement(mainArticleElement, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>

    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/p/compare_donut_pix2struct/">
        
        

        <div class="article-details">
            <h2 class="article-title">Donut vs. Pix2Struct Comparison</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/p/layoutlmv2_review/">
        
        

        <div class="article-details">
            <h2 class="article-title">LayoutLMv2 Review</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/p/xlnet_review/">
        
        

        <div class="article-details">
            <h2 class="article-title">XLNet Review</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/p/bert_review/">
        
        

        <div class="article-details">
            <h2 class="article-title">BERT Review</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/p/yolov3_review/">
        
        

        <div class="article-details">
            <h2 class="article-title">YOLOv3 Review</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    <script src="https://utteranc.es/client.js" 
        repo="kimberlykang/utterances"
        issue-term="pathname"
        
        crossorigin="anonymous"
        async
        >
</script>

<style>
    .utterances {
        max-width: unset;
    }
</style>

<script>
    let utterancesLoaded = false;

    function setUtterancesTheme(theme) {
        let utterances = document.querySelector('.utterances iframe');
        if (utterances) {
            utterances.contentWindow.postMessage(
                {
                    type: 'set-theme',
                    theme: `github-${theme}`
                },
                'https://utteranc.es'
            );
        }
    }

    addEventListener('message', event => {
        if (event.origin !== 'https://utteranc.es') return;

        
        utterancesLoaded = true;
        setUtterancesTheme(document.documentElement.dataset.scheme)
    });

    window.addEventListener('onColorSchemeChange', (e) => {
        if (!utterancesLoaded) return;
        setUtterancesTheme(e.detail)
    })
</script>


    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2023 - 
        
        2025 Kimberly&#39;s Blog
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.30.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
