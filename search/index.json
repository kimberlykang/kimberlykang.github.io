[{"content":"In this post, we’ll look at \u0026ldquo;XLNet: Generalized Autoregressive Pretraining for Language Understanding\u0026rdquo;, published by Google in 2019.\nhttps://arxiv.org/pdf/1906.08237.pdf\nTable of Contents  AutoRegressive vs. AutoEncoding Permutation Language Modeling Two-Stream Self-Attention  AutoRegressive vs. AutoEncoding AutoRegressive (AR) Language Modeling  Given an input token sequence, predicts the next token. Pre-training maximizes the following likelihood:  $ \\underset{θ}{\\max} \\log p_θ ({\\bf x} ) = \\displaystyle{\\sum_{t=1}^{T}} \\log p_θ (x_t | {\\bf x_{\\rm \u0026lt; t}}) = \\displaystyle{\\sum_{t=1}^{T}} \\log \\frac{exp(h_θ({\\bf x_{\\rm 1:t-1}})_t^{\\top} e(x_t))}{ \\sum _{x'} exp(h_θ({\\bf x _{\\rm 1:t-1}})^{\\top} e(x'))} $  $ {\\bf x} = [x_1, \u0026hellip;, x_T]$: text sequence $ h_θ({\\bf x _{\\rm 1:t-1}}) $: context representation from a neural model like RNNs or Transformers $ e(x) $: embedding of x exp is used for softmax Each word is predicted by referring only to the words that come before it.      AutoEncoding (AE)  Example: BERT Randomly masks some tokens in the input sequence, and predicts the original tokens for those masked positions. Pre-training maximizes the following likelihood:  $ \\underset{θ}{\\max} \\log p_θ ({\\bf \\bar x} | {\\bf \\hat x}) ≈ \\displaystyle{\\sum_{t=1}^{T}}m_t \\log p_θ (x_t | {\\bf \\hat x}) = \\displaystyle{\\sum_{t=1}^{T}}m_t \\log \\frac{exp(H_θ({\\bf \\hat x})_t^{\\top} e(x_t))}{ \\sum _{x'} exp(H_θ({\\bf \\hat x})_t^{\\top} e(x'))} $  $ {\\bf x} = [x_1, \u0026hellip;, x_T]$: text sequence $ {\\bf \\hat x} $: corrupted version where some tokens from $ {\\bf x} $ are randomly replaced with [MASK] $ {\\bf \\bar x} $: masked tokens $ m_t=1 $ if $x_t$ is masked $ H_θ $: Transformer that maps text sequence to hidden vectors $ e(x) $: embedding of x exp is used for softmax This is an approximate factorization.   The training objective is to recover $ {\\bf \\bar x} $ from $ {\\bf \\hat x} $.    Permutation Language Modeling Independence Assumption  In BERT, all masked tokens are predicted independently. For example, given [New, York, is, a, city] and [New, York] as prediction targets, the objectives for BERT and XLNet are:  $ {\\cal J}_{BERT} = \\log p(\\text{New} | \\text{is a city}) + \\log p(\\text{York} | \\text{is a city})$ $ {\\cal J}_{XLNet} = \\log p(\\text{New} | \\text{is a city}) + \\log p(\\text{York} | \\textcolor{red}{\\text{New}} \\text{, is a city})$ In BERT, the prediction for ‘York’ is independent of ‘New’. BERT cannot model the dependency between (‘New’, ‘York’), but XLNet considers these dependencies during training. Using AutoRegressive, you’d have to predict ‘a’ after seeing [New, York, is]. XLNet overcomes such limitations through permutation.    Permutation Language Modeling Objective  For a sequence x of length T, there are T! possible orders. By sharing parameters across all factorization orders, the model can learn information in both directions at all positions. Without the independence assumption; and since [MASK] is not used, there is no pretrain-finetune discrepancy. Only the factorization order is permuted; the input sequence order remains unchanged, so positional encoding is based on the original sequence order. The objective:  $ \\underset{θ}{\\max} \\Bbb E_{{\\bf z} \\sim \\cal Z_t} [\\displaystyle{\\sum_{t=1}^{T}} \\log p_θ (x_{z_t} | {\\bf x_{z \\rm \u0026lt; t}})] $ $ \\cal Z_t $: all possible permutations. $ z_t $: the t-th value in the permutation. After sampling a factorization order ${\\bf z}$, likelihood $ \\log p_θ $ is computed according to that order.   Example: New York is a city  Permutation 1: [New, York, is, a, city]. After seeing New → York → is, predict a. Permutation 2: [city, York, New, is, a]. After seeing city → York → New → is, predict a. The model can see city (which comes after a in the sequence) and use it to predict a. By using permutations, the model learns bidirectional context.    Source: Yang, Zhilin, et al. \u0026ldquo;Xlnet: Generalized autoregressive pretraining for language understanding.\u0026rdquo; Advances in neural information processing systems 32 (2019), Figure 4.\n After calculating $ \\log p_θ $ with the given factorization order sampled by permutation, the expectation is computed. ‘mem’ is a feature of Transformer-XL, which enables learning of long sequences; gradients are not applied to it.  Two-Stream Self-Attention Re-parameterization Considering Target Position  In a vanilla transformer, if two permutations $ {\\bf z}^{(1)} $ and $ {\\bf z}^{(2)} $ are identical up to position t, but differ at t, the following problem arises:  $ {\\bf z}^{(1)}_t $ and $ {\\bf z}^{(2)}_t $ have different target positions and ground truths, but yield the same model prediction.   To address this, instead of only using $ {\\bf x_{z \\rm \u0026lt; t}} $, XLNet introduces a new representation $ g_θ({\\bf x_{z \\rm \u0026lt; t}}, z_t) $ that also receives the target position $ z_t $ as input:  $ p_θ (X_{x_{z_t}}=x | x_{z \u0026lt; t}) = \\frac{exp(e(x)^{\\top} g_θ({\\bf x_{z \\rm \u0026lt; t}}, z_t))}{ \\sum _{x'} exp(e(x')^{\\top} g_θ({\\bf x _{z \\rm \u0026lt; t}}, z_t) )}$    Using Two Hidden Representations  For $ g_θ({\\bf x_{z \u0026lt; t}}, z_t) $:  If the content $ x_{z_t} $ is used, no learning will occur, so only the position $ z_t $ is used, not the content $ x_{z_t} $. To predict $ x_{z_j} $ for $j\u0026gt;t$, the content $ x_{z_t}$ is not used, but the contextual information must be available.   Hidden representations:  Content representation $ h_θ({\\bf x_{z \\leq t}}) $: Acts similarly to the hidden state in a standard Transformer, encoding both the context and $ x_{z_t} $. Query representation $ g_θ({\\bf x_{z \u0026lt; t}}, z_t) $: Encodes the contextual information prior to $ t $ and the position $ z_t $. $ h_i^{(0)}=e(x_i) $. For the content stream, the first layer is the word embedding. $ g_i^{(0)}=w $. For the query stream, the first layer is initialized as a learnable vector.   Update:  Query stream: $ g_{z_t}^{(m)} \\leftarrow \\text{Attention}(Q=g_{z_t}^{(m-1)}, KV=h_{{\\bf z} \u0026lt; t}^{(m-1)}; θ)$ Content stream: $ h_{z_t}^{(m)} \\leftarrow \\text{Attention}(Q=h_{z_t}^{(m-1)}, KV=h_{{\\bf z} \\leq t}^{(m-1)}; θ)$ In the query stream, $z_t$ is used, but $ x_{z_t} $ is not. On the other hand, in the content stream, both $z_t$ and $ x_{z_t} $ are used.    Source: Yang, Zhilin, et al. \u0026ldquo;Xlnet: Generalized autoregressive pretraining for language understanding.\u0026rdquo; Advances in neural information processing systems 32 (2019), Figure 1.\n When the factorization order is 3 → 2 → 4 → 1: In the next layer, $h_3^{(1)}$ attends to all previous layer states up to and including itself; i.e., it attends to $h_1^{(0)}, h_2^{(0)}, h_3^{(0)}$. In the next layer, $g_3^{(1)}$ only attends to prior positions in the previous layer; i.e., $h_1^{(0)}, h_2^{(0)}$. In the final layer, predictions are made from g. With this attention mechanism, the prediction layer cannot access the current word embedding directly, which encourages better learning.  Source: Yang, Zhilin, et al. \u0026ldquo;Xlnet: Generalized autoregressive pretraining for language understanding.\u0026rdquo; Advances in neural information processing systems 32 (2019), Figure 5.\nSource: Yang, Zhilin, et al. \u0026ldquo;Xlnet: Generalized autoregressive pretraining for language understanding.\u0026rdquo; Advances in neural information processing systems 32 (2019), Figure 6.\n","date":"2023-08-31T00:17:41+09:00","permalink":"https://kimberlykang.github.io/p/xlnet_review/","title":"XLNet Review"},{"content":"In this post, we’ll take a look at \u0026ldquo;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u0026rdquo;, published by Google in 2018.\nhttps://arxiv.org/pdf/1810.04805.pdf\nTable of Contents  Model Architecture Input Pre-training Fine-tuning Network Structure Training Testing  Model Architecture Source: Devlin, Jacob, et al. \u0026ldquo;Bert: Pre-training of deep bidirectional transformers for language understanding.\u0026rdquo; arXiv preprint arXiv:1810.04805 (2018), Figure 3.\n  GPT: Uses a left-to-right Transformer.\n  ELMo: Trains left-to-right and right-to-left LSTMs separately and then concatenates them. This is a feature-based approach.\n  BERT\n Uses a bi-directional Transformer. Overcomes the challenge of a bi-directional model referencing its own token (which makes naive predictions impossible) by introducing two pre-training tasks. BERT base has 110M parameters, and BERT large has 340M parameters.    Differences between GPT and BERT:\n GPT was trained only on BooksCorpus (800M words), while BERT used both BooksCorpus and Wikipedia (2,500M words). GPT used [SEP] and [CLS] tokens only for fine-tuning, but BERT also used these in pre-training, along with sentence A/B embeddings. GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for the same number of steps, but with a batch size of 128,000 words. GPT used the same learning rate for all fine-tuning tasks, while BERT used task-specific learning rates.    Input Source: Devlin, Jacob, et al. \u0026ldquo;Bert: Pre-training of deep bidirectional transformers for language understanding.\u0026rdquo; arXiv preprint arXiv:1810.04805 (2018), Figure 2.\n Token embeddings:  Uses WordPiece embeddings (with a 30,000 token vocabulary). The vocab_size is 30,522 (it looks like 522 tokens are reserved/unused). [CLS]: Every sequence starts with a [CLS] token, which is used for classification tasks. [SEP]: Used to separate sentences within a sequence.   Segment Embeddings: Distinguishes between the first sentence and the second sentence. Position Embeddings: Indicates the positional distance of each token.  Pre-training   Pre-training uses unlabeled data with two unsupervised tasks.\nTask 1: Masked LM (MLM)  Randomly masks 15% of the input tokens, then predicts those masked tokens using a softmax over the vocabulary based on the final hidden vector. Since there are no [MASK] tokens during fine-tuning, of the chosen 15%, 80% are replaced by [MASK], 10% by a random token, and the remaining 10% are left unchanged for prediction. The figure below shows an example where the sequence \u0026ldquo;my dog is cutep [SEP] he likes play ###ing[SEP]\u0026rdquo; is masked and predicted: Source: https://wikidocs.net/115055  \u0026lsquo;dog\u0026rsquo; is replaced with [MASK]. \u0026lsquo;he\u0026rsquo; is replaced with \u0026lsquo;king\u0026rsquo;. \u0026lsquo;play\u0026rsquo; is left unchanged. The last hidden vectors for \u0026lsquo;dog\u0026rsquo;, \u0026lsquo;he\u0026rsquo;, and \u0026lsquo;play\u0026rsquo; go through a fully connected layer and softmax to predict probabilities over the vocabulary.    Task 2: Next Sentence Prediction (NSP)   When choosing sentences A and B, 50% of the data uses the actual next sentence, while the other 50% uses a random sentence from the corpus as B.\n  Sample:\n Input = [CLS] the man went to [MASK] store [SEP]  he bought a gallon [MASK] milk [SEP]  Label = IsNext \nInput = [CLS] the man [MASK] to the store [SEP]  penguin [MASK] are flight ##less birds [SEP]  Label = NotNext\n   The final hidden vector of the [CLS] token is used for IsNext/NotNext classification. Source: https://wikidocs.net/115055\n    Fine-tuning   After initializing with pre-trained parameters, BERT is fine-tuned with labeled data for downstream tasks.\n  Only a small number of parameters are newly learned, since just a single output layer is added on top.\nSentence Pair Classification Tasks  Datasets  MNLI: Classifies the relationship of the second sentence to the first as entailment, contradiction, or neutral. QQP: Checks if two questions are semantically equivalent. QNLI: Checks whether there is a correct answer to the question. STS-B: Scores sentence similarity on a scale from 1 to 5. MRPC: Checks if two sentences are semantically equivalent. RTE: Determines if the second sentence is true, false, or unknown, given the first. SWAG: Given a sentence, chooses which of four options is the most likely next sentence.   C is the final hidden vector of the [CLS] token, $C\\in\\mathbb{R}^H$. H is the hidden size (768 for BERT_BASE, 1024 for BERT_LARGE). The only new parameters to be trained are the classification layer weights $W\\in\\mathbb R^{K \\times H}$, where K is the number of classes. Source: Devlin, Jacob, et al. \u0026ldquo;Bert: Pre-training of deep bidirectional transformers for language understanding.\u0026rdquo; arXiv preprint arXiv:1810.04805 (2018), Figure 4, (a). Results Source: Devlin, Jacob, et al. \u0026ldquo;Bert: Pre-training of deep bidirectional transformers for language understanding.\u0026rdquo; arXiv preprint arXiv:1810.04805 (2018), Table 1.\nSource: Devlin, Jacob, et al. \u0026ldquo;Bert: Pre-training of deep bidirectional transformers for language understanding.\u0026rdquo; arXiv preprint arXiv:1810.04805 (2018), Table 4.  Single Sentence Classification Tasks  Datasets  SST-2: Sentiment classification. CoLA: Determines if the sentence is linguistically \u0026ldquo;acceptable\u0026rdquo;.   Just like in sentence pair tasks, the classification layer weights $W\\in\\mathbb R^{K \\times H}$ are trained during fine-tuning.\nSource: Devlin, Jacob, et al. \u0026ldquo;Bert: Pre-training of deep bidirectional transformers for language understanding.\u0026rdquo; arXiv preprint arXiv:1810.04805 (2018), Figure 4, (b).  Question Answering Tasks  Datasets  SQuAD v1.1: Answers are found in the passage for each question. SQuAD v2.0   The question is represented as an A embedding, and the passage as a B embedding. During fine-tuning, the start vector $S \\in \\mathbb{R}^H$ and end vector $E \\in \\mathbb{R}^H$ are learned. To determine start and end positions, the dot product and softmax are calculated between each token\u0026rsquo;s final hidden vector T and S or E, respectively.\nSource: Devlin, Jacob, et al. \u0026ldquo;Bert: Pre-training of deep bidirectional transformers for language understanding.\u0026rdquo; arXiv preprint arXiv:1810.04805 (2018), Figure 4, (c).  Single Sentence Tagging Tasks  Dataset  CoNLL-2003 NER\nSource: Devlin, Jacob, et al. \u0026ldquo;Bert: Pre-training of deep bidirectional transformers for language understanding.\u0026rdquo; arXiv preprint arXiv:1810.04805 (2018), Figure 4, (d).   Results\nSource: Devlin, Jacob, et al. \u0026ldquo;Bert: Pre-training of deep bidirectional transformers for language understanding.\u0026rdquo; arXiv preprint arXiv:1810.04805 (2018), Table 7.    ","date":"2023-08-02T00:17:41+09:00","permalink":"https://kimberlykang.github.io/p/bert_review/","title":"BERT Review"},{"content":"In this post, we will look at the paper \u0026ldquo;YOLOv3: An Incremental Improvement\u0026rdquo;, published in 2018.\nYOLOv3: An Incremental Improvement\nTable of Contents  Prediction at Multiple Scales Results  Prediction at Multiple Scales Source: https://wikidocs.net/174008\n YOLOv3 predicts bounding boxes at three different scales, similar to a Feature Pyramid Network. Several convolutional layers are attached to the base feature extractor, with the final layer predicting bounding boxes, objectness, and class probabilities. In this paper, 3 anchor boxes are used per grid cell. Therefore, if N is the number of grids in the feature map, each feature map predicts N×N×[3 * (4+1+80)] values. Since 3 types of anchors are used at 3 different scales, there are a total of 9 anchors. Anchors are obtained using k-means clustering with k=9. For the COCO dataset, the following anchor boxes are used: (10×13), (16×30), (33×23), (30×61), (62×45), (59×119), (116×90), (159×198), (373×326).  Results Source: Redmon, Joseph, and Ali Farhadi. \u0026ldquo;Yolov3: An incremental improvement.\u0026rdquo; arXiv preprint arXiv:1804.02767 (2018), Table 3.\n","date":"2023-07-04T00:09:40+09:00","permalink":"https://kimberlykang.github.io/p/yolov3_review/","title":"YOLOv3 Review"},{"content":"In this post, we’ll take a look at the paper \u0026ldquo;YOLO9000: Better, Faster, Stronger\u0026rdquo; presented at CVPR 2017.\nYOLO9000: Better, Faster, Stronger\nTable of Contents  Motivation Batch Normalization Classifier Training on High Resolution Use of Anchor Boxes Dimension Clusters Predicted Values Fine-Grained Features Network Results  Motivation YOLOv1, compared to the then state-of-the-art Fast-RCNN, had considerable localization errors and relatively low recall, frequently missing objects. In YOLOv2, several methods were designed to solve these problems of localization error and low recall, while maintaining a fast inference speed and without increasing network size.\nBatch Normalization  See Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Batch Normalization was proposed to address covariate shift by normalizing the input to each layer within a mini-batch to the mini-batch’s mean and variance. Since its introduction in 2015 (PMLR), it has been widely used. In YOLOv2, adding Batch Normalization increased mAP by 2%. Batch normalization also acted as a regularizer, so dropout could be removed without causing overfitting.  Classifier Training on High Resolution  Object detection models are usually pre-trained as classifiers on ImageNet, then fine-tuned on detection tasks using transfer learning. YOLOv1 also followed this, using 224×224 images for classification and 448×448 images for detection. This means that during detection training, the network needs to learn not only detection but also to adapt to the changed input resolution. To overcome this, YOLOv2 pre-trained the classification model on 448×448 ImageNet images for 10 epochs before detection training. This simple step improved mAP by 4%.  Use of Anchor Boxes  YOLOv1 directly predicted bounding box coordinates by attaching a Fully Connected layer to the feature extractor. YOLOv2 removes the Fully Connected layer and, instead, uses anchor boxes to learn offsets. For each anchor, the model predicts class and objectness. As a result, mAP slightly dropped from 69.5 to 69.2, but recall significantly increased from 81% to 88%.  Dimension Clusters   Instead of manually selecting anchor boxes, YOLOv2 uses k-means clustering on training set bounding boxes to derive anchor boxes. Instead of Euclidean distance, it uses a custom metric to avoid a bias for larger boxes:\n $ d(box, centroid) = 1 - IOU(box, centroid) $    Below is a graph of mean IOU (between dataset bounding boxes and their nearest centroid) and the anchor boxes for k=5:\nSource: Redmon, Joseph, and Ali Farhadi. \u0026ldquo;YOLO9000: better, faster, stronger.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, Figure 2\n  Increasing k improves performance but increases complexity; the paper uses k=5 as a tradeoff.\n  The white boxes in the above figure show VOC 2007 anchors and the blue boxes COCO anchors at k=5. This shows anchor shapes differ by dataset.\nSource: Redmon, Joseph, and Ali Farhadi. \u0026ldquo;YOLO9000: better, faster, stronger.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, Table 1\n  The table above shows average IOU between ground truth and the closest anchor for various anchor generation methods on VOC 2007.\n  \u0026ldquo;Cluster SSE\u0026rdquo; refers to k-means using sum of squared errors (Euclidean distance); \u0026ldquo;Cluster IOU\u0026rdquo; refers to k-means using IOU-based distance; \u0026ldquo;Anchor Boxes\u0026rdquo; refers to the nine manually chosen anchors in 15.\n  IOU-based k-means yields higher average IOU than both SSE-based k-means and the manually-chosen anchor boxes.\n  Predicted Values Before getting into the outputs YOLOv2 predicts, let’s define the notation:\n $ (c_x, c_y) $: coordinates of the top-left corner of the grid cell $ (b_x, b_y) $: center coordinates of the bounding box $ (b_w, b_h) $: width and height of the bounding box $ (p_w, p_h) $: prior width and height of the anchor box $ (t_x, t_y, t_w, t_h, t_o) $: YOLOv2 network outputs (denoted with t for “target”)  The predictions:\n $ b_x = \\sigma(t_x) + c_x $ $ b_y = \\sigma(t_y) + c_y $ $ b_w = p_w e^{t_w}$ $ b_h = p_h e^{t_h}$ $ Pr(object) \\cdot IOU(b, object) = \\sigma(t_o)$\nSource: Redmon, Joseph, and Ali Farhadi. \u0026ldquo;YOLO9000: better, faster, stronger.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, Figure 3   For bounding box centers, a logistic activation ($\\sigma$) is used, so the predictions are between 0 and 1 and specify a relative position in the cell. For width and height, the output is exponentiated and then multiplied by the prior (anchor) size.  Fine-Grained Features A passthrough layer was added to allow detection to use features at different resolutions.\n The 13×13 feature map for detection is connected to the previous 26×26 feature map via a passthrough layer. The 26×26×512 feature map is reshaped to 13×13×2048 and concatenated with the 13×13 feature map.  Network  VGG-16 was used as the base feature extractor. It requires 30.69 billion floating point operations per image. The paper proposes Darknet-19, with 19 convolutional layers, requiring only 5.58 billion operations per image.  Results Source: Redmon, Joseph, and Ali Farhadi. \u0026ldquo;YOLO9000: better, faster, stronger.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, Table 2\n This table shows the mAP for different model types on VOC 2007. The mAP is significantly improved compared to YOLOv1.  Source: Redmon, Joseph, and Ali Farhadi. \u0026ldquo;YOLO9000: better, faster, stronger.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, Table 3\nSource: [Redmon, Joseph, and Ali Farhadi. \u0026ldquo;YOLO9000: better, faster, stronger.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, Figure 4](https://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_F\n","date":"2023-06-18T00:09:40+09:00","permalink":"https://kimberlykang.github.io/p/yolov2_review/","title":"YOLOv2 Review"},{"content":"In this post, we’ll take a look at the paper \u0026ldquo;You Only Look Once: Unified, Real-Time Object Detection\u0026rdquo; presented at CVPR 2016.\nhttps://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\nTable of Contents  Overview Bounding Box Class Probability Output Network Architecture Loss Testing  Overview YOLO uses a grid to detect objects in an image. In short:\n The input image is divided into S×S grid cells. Each grid cell predicts the coordinates (x, y, w, h) for B bounding boxes and a confidence score (c) for each bounding box. Each grid cell also predicts C class probabilities. The final score and class are determined by multiplying the confidence score and class probability. Bounding boxes with scores above a threshold become the final detection results.   The above image shows an example with S=7 and B=2 for a single grid cell. Each grid cell predicts two sets of bounding boxes (x, y, w, h, c). For C=20, each grid cell predicts class probabilities for 20 classes.  This is a brief overview of how YOLO finds objects. Below, we’ll look at how each prediction value is calculated and what they mean.\nBounding Box A bounding box is composed of (x, y, w, h, c).\n (x, y): Coordinates for the center of the bounding box.  Each grid cell has B bounding boxes, meaning the center of each of these B bounding boxes is inside that grid cell. The corners of the box can be outside the grid cell. (x, y) specifies the relative location within the grid cell — values range from 0 to 1.   (w, h): Width and height of the bounding box.  These are relative to the width and height of the entire image, and are values from 0 to 1.   c: Represents how confident the model is that the box contains an object and how accurately the predicted box fits the object.  $ Pr(Object) * IOU^{truth}_{pred} $ The IOU (Intersection Over Union) between the predicted box and the ground truth (GT) box is calculated to measure localization accuracy.    Class Probability  C: The class probabilities for each grid cell when it contains an object.  $ Pr(Class_i|Object) $ Each grid cell predicts only one set of class probabilities, regardless of the number of bounding boxes.    Output  If the image is divided into S×S grid cells, and each grid cell predicts B bounding boxes and C class probabilities, the final output tensor shape is S×S×(B*5+C). For example, in PASCAL VOC, S=7, B=2, C=20, so there are 49 grid cells, each predicting 30 values. Here is how one such prediction might look:  [x, y, w, h, c, probability for background, probability for airplane, \u0026hellip;, probability for monitor] [0.4, 0.3, 0.8, 0.7, 0.9, 0.003, 0.8, \u0026hellip; , 0.012]    Network Architecture  YOLO uses an architecture similar to GoogLeNet. Source: Redmon, Joseph, et al. \u0026ldquo;You only look once: Unified, real-time object detection.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2016, Figure 3.  Loss   Uses sum-squared error.\n  $ \\lambda_{coor} $, $ \\lambda_{noobj} $ are used.\n Most images have more grid cells without objects than with objects. The gradient for the confidence score in cells without objects becomes much larger than for cells with objects when the confidence is near 0, which can cause the model to diverge early in training. To address this, they used $ \\lambda_{coor}=5 $ to increase the bounding box coordinate loss and $ \\lambda_{noobj}=0.5 $ to decrease the confidence loss for boxes without objects.    Predicts $ \\sqrt w $, $ \\sqrt h $\n Small errors for large boxes are less significant, but for small boxes, the impact is much larger. To address this, the width and height values are square-rooted.    Only one bounding box predictor (the one with the highest IOU for each object) is assigned to each object.\n  Loss function\n$ \\lambda_{coor} \\displaystyle{\\sum_{i=0}^{s^2}} \\displaystyle{\\sum_{j=0}^{B}} \\mathbb 1_{ij}^{obj} [(x_i-\\hat x_i)^2 + (y_i-\\hat y_i)^2] $\n$ + \\lambda_{coor} \\displaystyle{\\sum_{i=0}^{s^2}} \\displaystyle{\\sum_{j=0}^{B}} \\mathbb 1_{ij}^{obj} [(\\sqrt{w_i} - \\sqrt{\\hat w_i})^2 + (\\sqrt{h_i} - \\sqrt{\\hat h_i})^2] $\n$ + \\displaystyle{\\sum_{i=0}^{s^2}} \\displaystyle{\\sum_{j=0}^{B}} \\mathbb 1_{ij}^{obj} (C_i-\\hat C_i)^2 $ $ + \\lambda_{noobj} \\displaystyle{\\sum_{i=0}^{s^2}} \\displaystyle{\\sum_{j=0}^{B}} \\mathbb 1_{ij}^{noobj} (C_i-\\hat C_i)^2 $\n$ + \\displaystyle{\\sum_{i=0}^{s^2}} \\mathbb 1_{i}^{obj} \\displaystyle{\\sum_{c∈classwa}} (p_i(c)-\\hat p_i(c))^2 $\n 1obji: Indicates whether there is an object in cell i. 1objij: Indicates whether the j-th bounding box in cell i is responsible for predicting the ground truth in cell i, i.e., whether that bounding box has the highest IOU with the GT.    Testing  $ Pr(Class_i|Object) * Pr(Object) * {IOU}^{truth}_{pred} = Pr({Class}_i) * {IOU}^{truth}_{pred} $ At test time, the final score for each box is computed by multiplying the class probability and the confidence score. The final score reflects both the probability of the class being present in the box, and how well the predicted box fits the object. In the figure below, the thickness of the bounding box lines indicates the confidence. Multiplying this confidence score and the class probability, results above the threshold become the final outcomes. Source: Redmon, Joseph, et al. \u0026ldquo;You only look once: Unified, real-time object detection.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2016, Figure 2.  ","date":"2023-05-29T00:17:41+09:00","permalink":"https://kimberlykang.github.io/p/yolov1_review/","title":"YOLOv1 Review"},{"content":"In this post, we will look at the Loss function of YOLOv5.\nTable of Contents  Information Theory Binary Cross Entropy IoU Loss YOLOv5 Loss  Information Theory   Information\n If the probability of an event A occurring is P(A), the amount of information for this event is $I=-\\log P(A)$. Example: Information when a coin lands on heads  Probability: $ \\displaystyle \\frac{1}{2}$ $I=-\\log \\displaystyle \\frac{1}{2}$      Entropy\n The expected value of the information. $ H(X)=- \\displaystyle \\sum_{x}P(X) \\log P(X)$ Example: Entropy of a coin flip  $Entropy= \\displaystyle{ -(\\frac{1}{2} \\times \\log \\frac{1}{2} + \\frac{1}{2} \\times \\log \\frac{1}{2})=-\\log \\frac{1}{2}}$      Kullback–Leibler divergence (KL Divergence)\n Measures the difference between two probability distributions. $ D_{KL}(P \\Vert Q)= \\displaystyle \\sum_{x}P(X) \\log \\frac{P(X)}{Q(X)} $\n$ = -\\displaystyle \\sum_{x}P(X) \\log Q(X) +\\sum_{x} P(X) \\log P(X)$\n The first term is the entropy calculated using distribution Q for data from distribution P. The second term is the entropy of the distribution P. In other words, KL divergence tells you how much more information you have when using Q to encode P compared to using P itself.      Cross Entropy\n The first term of the KL divergence above, i.e. the expected value of information when using distribution Q for data from distribution P, is called the Cross Entropy. $ H(P, Q)= -\\displaystyle \\sum_{x}P(X)\\log Q(X) $ KL divergence can be rearranged as below:  $ D_{KL}(P \\Vert Q)= H(P, Q) - H(P) $ Here, P is the true distribution, Q is the distribution produced by an approximate model, and H(P) is constant. Therefore, cross-entropy H(P, Q) is often used as a model’s loss when minimizing KL divergence.      Binary Cross Entropy  Cross Entropy Loss  In classification, the data label y is given as a one-hot vector, where y is probability distribution P in the formula for cross entropy above. The model output $\\hat y$ is typically the softmax output probabilities for each class, i.e., distribution Q. When N is the number of data samples, y the labels, and C the number of classes, the Cross Entropy Loss L is:  $ L= - \\displaystyle \\sum_{n}^{N} \\displaystyle \\sum_{c}^{C}y_{n, c} \\log \\hat{y}_{n, c}$   For example, in a classification problem with \u0026lsquo;dog\u0026rsquo;, \u0026lsquo;cat\u0026rsquo;, and \u0026lsquo;frog\u0026rsquo;:  $y=\\begin{bmatrix} 0 \\cr 1 \\cr 0 \\end{bmatrix} $ (True label) $\\hat y=\\begin{bmatrix} 0.1 \\cr 0.7 \\cr 0.2 \\end{bmatrix} $ (Prediction) The cross-entropy loss $l$ for this image is:  $ l= -\\displaystyle \\sum_{c}^{C}y_c\\log \\hat{y}_c$\n$=0⋅\\log 0.1 + 1⋅\\log 0.7 + 0⋅\\log 0.2$ $=\\log 0.7$       Binary Cross Entropy  A special case of cross entropy loss when the number of classes is two. The model’s output $\\hat y$ is usually the probability from a sigmoid function. $ L= - \\displaystyle \\sum_{n}^{N} [y_n\\log \\hat{y}_n + (1-y_n)\\log (1-\\hat{y}_n)]$   BCEWithLogitsLoss  Torch provides the BCEWithLogitsLoss class, which combines the Sigmoid layer and the Binary Cross Entropy. It is more numerically stable than using Sigmoid and BCELoss separately, due to the log-sum-exp trick. If x is the input to Sigmoid, y the label, N the number of samples, and w the weight, the unreduced loss L is:  $ L=\\begin{Bmatrix} l_{1}, \\cr \u0026hellip;, \\cr l_{N} \\end{Bmatrix}$ where $l_n= - w_n [(y_n\\log \\sigma (x_n)) + (1-y_n)\\log (1-\\sigma(x_n))]$   The reduced loss L is:  $ L=\\begin{cases} mean(L), \\text{if reduction=\u0026lsquo;mean\u0026rsquo;} \\cr sum(L), \\text{if reduction= \u0026lsquo;sum\u0026rsquo;} \\end{cases} $   With c as the class, you can adjust precision and recall by using a positive weight $p_c$. The formula becomes:  $ L_c=\\begin{Bmatrix} l_{1,c}, \\cr \u0026hellip;, \\cr l_{N, c} \\end{Bmatrix}$ where $l_{n, c}= - w_{n,c} [(p_c ​⋅ y_{n, c}\\log \\sigma (x_n)) + (1-y_{n, c})\\log (1-\\sigma(x_{n, c}))]$ $p_c\u0026gt;1$ increases recall, $p_c\u0026lt;1$ increases precision. If there are 100 positive and 300 negative examples for a class, set $p_c=\\frac{300}{100}=3$.   torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction=\u0026lsquo;mean\u0026rsquo;, pos_weight=None)  size_average, reduce are deprecated parameters. 1 2 3 4 5 6  target = torch.ones([10, 64], dtype=torch.float32) # 64 classes, batch size = 10 output = torch.full([10, 64], 1.5) # A prediction (logit) pos_weight = torch.ones([64]) # All weights are equal to 1 criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight) criterion(output, target) # -log(sigmoid(1.5)) # tensor(0.20...)         IoU Loss   Intersection over Union (IoU)\n Scale invariant. When you have convex A and B, IoU is calculated as follows:  $ IoU= \\displaystyle \\frac{|A∩B|}{|A∪B|} $   $L_{IoU}=1-IoU$ When IoU is 0, i.e., when the target box and prediction box do not overlap in object detection, IoU does not indicate how large the error is. As a result, the Gradient Vanishing problem occurs.    Generalized-IoU (GIoU)\n Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression If C is the smallest convex that contains A and B, GIoU is calculated as:  $ GIoU=IoU- \\displaystyle \\frac{|C \\backslash (A∪B)|}{|C|} $ $ C \\backslash (A∪B)$ means set difference.   $L_{GIoU}=1-GIoU$. The range of loss value is 0~2. If the target box contains the prediction box, the penalty term is zero and $L_{GIoU}$ becomes the same as $L_{IoU}$. Source: Zheng, Zhaohui, et al. \u0026ldquo;Distance-IoU loss: Faster and better learning for bounding box regression.\u0026rdquo; Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 07. 2020, Figure 2 Has slow convergence and comparatively low box accuracy.    Distance-IoU (DIoU) \u0026amp; Complete IoU (CIoU)\n Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression In general, IoU-based loss can be defined as $L_{IoU}=1-IoU + \\mathcal{R}(B, B^{gt})$. DIoU  $ \\mathcal{R}_{DIoU} = \\displaystyle \\frac{ρ^2(b, b^{gt})}{c^2}$. ρ is the Euclidean distance between the centers of the two boxes, and c is the diagonal length of the smallest box enclosing both boxes.\nSource: Zheng, Zhaohui, et al. \u0026ldquo;Distance-IoU loss: Faster and better learning for bounding box regression.\u0026rdquo; Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 07. 2020, Figure 5 $L_{DIoU}=1-IoU + \\mathcal{R}_{DIoU}$, range is 0~2. Since the distance between boxes is minimized, it converges faster than GIoU loss.   CIoU  This is an extension of DIoU to additionally consider the aspect ratio. $ \\mathcal{R}_{CIoU} = \\displaystyle \\frac{ρ^2(b, b^{gt})}{c^2} + \\alpha v$  $\\alpha = \\displaystyle \\frac{v}{(1-IoU) + v}$  Gives higher priority to regression in non-overlapping cases.   $v = \\displaystyle \\frac{4}{\\pi^2}(arctan\\frac{w^{gt}}{h^{gt}} - arctan\\frac{w}{h})^2$  Measures the similarity of aspect ratios. $ \\displaystyle \\frac{4}{\\pi^2}$ is to make the maximum calculation result 1 since arctan max is $\\frac{\\pi}{2}$.     $L_{CIoU}=1-IoU + \\mathcal{R}_{CIoU}$   Non-Maximum Suppression (NMS) using DIoU  In traditional NMS, the IoU metric is used to remove unnecessary detection boxes. Since IoU only considers the overlapping area, incorrect boxes can be suppressed in occlusion situations. DIoU considers both the overlapping area and the center point distance between the two boxes, making it more suitable as a criterion for NMS in case of occlusion. Let $\\mathcal{M}$ be the prediction box with the highest score, $B$ other boxes to apply NMS to, $s$ is the classification score, and $\\epsilon$ is the NMS threshold. DIoU-NMS is defined as:  $ s_i= \\begin{cases} \\displaystyle s_i, \u0026amp; IoU -\\mathcal{R}(\\mathcal{M}, B_i) \u0026lt; \\epsilon \\\\ \\displaystyle 0, \u0026amp; IoU -\\mathcal{R}(\\mathcal{M}, B_i) ≥ \\epsilon \\end{cases} $   Below is an example using a YOLOv3 model trained on PASCAL VOC07+12, detecting an image from MS COCO 2017. Source: Zheng, Zhaohui, et al. \u0026ldquo;Distance-IoU loss: Faster and better learning for bounding box regression.\u0026rdquo; Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 07. 2020, Figure 8   Experimental Results  Below are results for training YOLOv3 with $ L_{IoU} $, $ L_{GIoU} $, $ L_{DIoU} $, $ L_{CIoU} $. (D) means IoU-NMS is used during NMS.\nSource: Zheng, Zhaohui, et al. \u0026ldquo;Distance-IoU loss: Faster and better learning for bounding box regression.\u0026rdquo; Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 07. 2020, Table 1 Below are results for training SSD with $ L_{IoU} $, $ L_{GIoU} $, $ L_{DIoU} $, $ L_{CIoU} $. (D) means IoU-NMS is used during NMS.\nSource: Zheng, Zhaohui, et al. \u0026ldquo;Distance-IoU loss: Faster and better learning for bounding box regression.\u0026rdquo; Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 07. 2020, Table 2 Below are results for training Faster-RCNN with $ L_{IoU} $, $ L_{GIoU} $, $ L_{DIoU} $, $ L_{CIoU} $. (D) means IoU-NMS is used during NMS.\nSource: Zheng, Zhaohui, et al. \u0026ldquo;Distance-IoU loss: Faster and better learning for bounding box regression.\u0026rdquo; Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 07. 2020, Table 3      YOLOv5 Loss   Box Loss\n pbox: (number of predictions, number of anchors) iou: (number of predictions)  1 2 3 4 5 6  # Regression pxy = pxy.sigmoid() * 2 - 0.5 pwh = (pwh.sigmoid() * 2) ** 2 * anchors[i] pbox = torch.cat((pxy, pwh), 1) # predicted box  iou = bbox_iou(pbox, tbox[i], CIoU=True).squeeze() # iou lbox += (1.0 - iou).mean() # iou loss     Objectness Loss\n The IoU between the target box and prediction box becomes the ground truth (GT). Objectness Loss is calculated by applying different weights for each scale.  $ L_{obj}=4.0 ⋅L_{obj}^{small} +1.0 ⋅ L_{obj}^{medium} +0.4 ⋅ L_{obj}^{large}$   pi[..., 4]: (batch size, number of anchors, number of grids, number of grids) Represents the objectness predicted by the model. tobj: (batch size, number of anchors, number of grids, number of grids) Represents the target objectness for the corresponding batch, anchor, and grid location.  1 2 3 4 5 6 7 8 9 10 11 12  # Objectness # BCEobj = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h[\u0026#39;obj_pw\u0026#39;]], device=device)) iou = iou.detach().clamp(0).type(tobj.dtype) if self.sort_obj_iou: j = iou.argsort() b, a, gj, gi, iou = b[j], a[j], gj[j], gi[j], iou[j] if self.gr \u0026lt; 1: # default 1.0 iou = (1.0 - self.gr) + self.gr * iou tobj[b, a, gj, gi] = iou # iou ratio obji = self.BCEobj(pi[..., 4], tobj) lobj += obji * self.balance[i] # obj loss     Classification Loss\n t: (number of detections, number of classes)  1 2 3 4 5 6 7  # Classification # BCEcls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h[\u0026#39;cls_pw\u0026#39;]], device=device)) if self.nc \u0026gt; 1: # cls loss (only if multiple classes) t = torch.full_like(pcls, self.cn, device=self.device) # targets t[range(n), tcls[i]] = self.cp lcls += self.BCEcls(pcls, t) # BCE     Total Loss\n $Loss=\\lambda_{box} L_{box} + \\lambda_{obj} L_{obj} + \\lambda_{cls} L_{cls}$  1 2 3 4 5 6  lbox *= self.hyp[\u0026#39;box\u0026#39;] lobj *= self.hyp[\u0026#39;obj\u0026#39;] lcls *= self.hyp[\u0026#39;cls\u0026#39;] bs = tobj.shape[0] # batch size return (lbox + lobj + lcls) * bs, torch.cat((lbox, lobj, lcls)).detach()     ","date":"2023-04-11T00:20:00+09:00","permalink":"https://kimberlykang.github.io/p/yolov5_loss/","title":"YOLOv5 Loss"},{"content":"In this post, we will take a look at Validation in YOLOv5.\nTable of Contents  Validation Non-Maximum Suppression Profile Prediction Matrix AP, mAP Calculation  Validation During training, validation is performed at the end of each epoch to evaluate the training of that epoch.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  if RANK in {-1, 0}: # mAP callbacks.run(\u0026#39;on_train_epoch_end\u0026#39;, epoch=epoch) ema.update_attr(model, include=[\u0026#39;yaml\u0026#39;, \u0026#39;nc\u0026#39;, \u0026#39;hyp\u0026#39;, \u0026#39;names\u0026#39;, \u0026#39;stride\u0026#39;, \u0026#39;class_weights\u0026#39;]) final_epoch = (epoch + 1 == epochs) or stopper.possible_stop if not noval or final_epoch: # Calculate mAP results, maps, _ = validate.run(data_dict, batch_size=batch_size // WORLD_SIZE * 2, imgsz=imgsz, half=amp, model=ema.ema, single_cls=single_cls, dataloader=val_loader, save_dir=save_dir, plots=False, callbacks=callbacks, compute_loss=compute_loss)    In multi-GPU training, validation is performed only on gpu 0. The parameters conf_thres, iou_thres, and max_det not explicitly specified above default to conf_thres=0.001, iou_thres=0.6, and max_det=300, and are later used in Non-Maximum Suppression (NMS).  Non-Maximum Suppression  After performing inference on each validation data sample, Non-Maximum Suppression (NMS) is applied to the prediction results. NMS is a method for removing duplicates among overlapping bounding boxes and selecting a single box. It selects the box to keep based on the bounding boxes’ confidence scores and the IoU between boxes. Below is the code for calling NMS during validation.  1 2 3 4 5 6 7 8  with dt[2]: preds = non_max_suppression(preds, conf_thres, iou_thres, labels=lb, multi_label=True, agnostic=single_cls, max_det=max_det)    The parameters conf_thres, iou_thres, and max_det used as arguments can have different values depending on where they are called. When called in val.py:  conf_thres=0.001: Any detection result with a confidence score less than 0.001 will be removed. iou_thres=0.6: For boxes with IoU greater than or equal to 0.6, only the one with the highest confidence score is selected. max_det=300: Among the remaining boxes, only the top 300 sorted by confidence score are returned.   When called in detect.py:  conf_thres=0.25, iou_thres=0.45, max_det=1000 are used.    Profile When non_max_suppression is called in val.py, it is wrapped with with dt[2], where dt is an instance of the Profile class. To record the time taken for pre-processing, inference, and NMS, Profile objects are created before validation begins as shown below.\n1  dt = Profile(), Profile(), Profile() # profiling times   The Profile class inherits from contextlib.ContextDecorator. A child class inheriting from contextlib.ContextDecorator can define actions to perform when entering a with block in the __enter__ method, and actions to perform when exiting in the __exit__ method. In Profile, the time is recorded upon entering and again upon exiting; the total time spent within the with block is saved in self.t.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  class Profile(contextlib.ContextDecorator): # YOLOv5 Profile class. Usage: @Profile() decorator or \u0026#39;with Profile():\u0026#39; context manager def __init__(self, t=0.0): self.t = t self.cuda = torch.cuda.is_available() def __enter__(self): self.start = self.time() return self def __exit__(self, type, value, traceback): self.dt = self.time() - self.start # delta-time self.t += self.dt # accumulate dt def time(self): if self.cuda: torch.cuda.synchronize() return time.time()   If validation is not called during training, the average execution time is printed after validation as shown below.\n1 2 3 4 5  # Print speeds t = tuple(x.t / seen * 1E3 for x in dt) # speeds per image if not training: shape = (batch_size, 3, imgsz, imgsz) LOGGER.info(f\u0026#39;Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {shape}\u0026#39; % t)   Prediction Matrix  After NMS, the results, labels, and desired IoU thresholds are passed to process_batch, which generates a Prediction Matrix that indicates, for each detection, whether an object was correctly detected.  1  correct = process_batch(predn, labelsn, iouv)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  def process_batch(detections, labels, iouv): \u0026#34;\u0026#34;\u0026#34; Return correct prediction matrix Arguments: detections (array[N, 6]), x1, y1, x2, y2, conf, class labels (array[M, 5]), class, x1, y1, x2, y2 Returns: correct (array[N, 10]), for 10 IoU levels \u0026#34;\u0026#34;\u0026#34; correct = np.zeros((detections.shape[0], iouv.shape[0])).astype(bool) iou = box_iou(labels[:, 1:], detections[:, :4]) correct_class = labels[:, 0:1] == detections[:, 5] for i in range(len(iouv)): x = torch.where((iou \u0026gt;= iouv[i]) \u0026amp; correct_class) # IoU \u0026gt; threshold and classes match if x[0].shape[0]: matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy() # [label, detect, iou] if x[0].shape[0] \u0026gt; 1: matches = matches[matches[:, 2].argsort()[::-1]] matches = matches[np.unique(matches[:, 1], return_index=True)[1]] # matches = matches[matches[:, 2].argsort()[::-1]] matches = matches[np.unique(matches[:, 0], return_index=True)[1]] correct[matches[:, 1].astype(int), i] = True return torch.tensor(correct, dtype=torch.bool, device=iouv.device)     iouv is the list of IOU threshold criteria.\n IoU thresholds from 0.5 to 0.95 are used. tensor([0.50000, 0.55000, 0.60000, 0.65000, 0.70000, 0.75000, 0.80000, 0.85000, 0.90000, 0.95000], device=\u0026lsquo;cuda:0\u0026rsquo;)    iou\n Calculates the IoU between the ground truth labels and the NMS results detections. If there are 5 labels and 300 detections, the shape of iou will be (5, 300).    matches\n Among detection results, selects those with IoU above the threshold and matching class. Sorts by descending IoU. Uses np.unique to remove duplicates in labels and detection results. The shape is (number of matches, 3), where the three columns are label index, detection index, and IoU. For example, the result filtered by IoU threshold and class may look like: 1 2 3 4  [[ 1, 0, 0.51507], [ 1, 3, 0.78271], [ 1, 11, 0.51642], [ 1, 12, 0.54807]]    After removing duplicates, the result looks like: 1  [[ 1, 0, 0.51507]]       correct\n For each detection, whether an object is found at each of 10 IoU thresholds. If there are 300 detections, the shape is (300, 10). For detections matched in matches, mark True, indicating the detection overlaps sufficiently with a ground truth label and is the correct class.    The results of process_batch for all images are stored in stats.\n  AP, mAP Calculation  Using ap_per_class, precision, recall, and AP are calculated.  1  tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats, plot=plots, save_dir=save_dir, names=names)     For each class, precision and recall are calculated for 10 IoU thresholds using the following formulas:\n $Precision = \\frac{TP}{TP+FP} = \\frac{TP}{number\\ of\\ detections}$ $Recall = \\frac{TP}{TP+FN} = \\frac{TP}{number\\ of\\ GTs}$ Since duplicates are filtered using np.unique in the process_batch calculation above, both precision and recall can have a maximum value of 1.    On a coordinate plane where the x-axis is Recall and the y-axis is Precision, each detection result is plotted as a point. Connecting all these points forms the Precision-Recall Curve.\n  In the figure below, the letters represent each image. Source: https://github.com/rafaelpadilla/Object-Detection-Metrics\n  As in the figure below, the entire recall range is divided evenly, and Interpolated Precision is calculated for each region. In this example, it is divided into 11 regions, but in YOLOv5, it is divided into 1000. Source: https://github.com/rafaelpadilla/Object-Detection-Metrics\n  The interpolated precision is plotted on the axis to create a new Precision-Recall Curve as follows. Source: https://github.com/rafaelpadilla/Object-Detection-Metrics\n  The area under this new Precision-Recall Curve (Area Under Curve, AUC) becomes the AP value. Source: https://github.com/rafaelpadilla/Object-Detection-Metrics\n  In this way, YOLOv5 calculates class-wise AP using ap_per_class, and then uses the average of AP values as the object detection performance metric called Mean Average Precision (mAP).\n  ","date":"2023-03-29T00:20:00+09:00","permalink":"https://kimberlykang.github.io/p/yolov5_val/","title":"YOLOv5 Validation"},{"content":"In this post, we will explore the types of augmentation used in YOLOv5.\nTable of Contents  Mosaic Mixup Copy-Paste Random Perspective Albumentation, HSV, Horizontal and Vertical Flip  Mosaic   Whether to use mosaic is decided by sampling according to the mosaic usage probability in the hyperparameter file. If mosaic is not used, the image is placed in the center and the remaining area is padded, resulting in a 640×640 image.\n1 2 3 4 5 6 7 8 9 10 11 12  mosaic = self.mosaic and random.random() \u0026lt; hyp[\u0026#39;mosaic\u0026#39;] if mosaic: # Load mosaic img, labels = self.load_mosaic(index) ... else: # Load image img, (h0, w0), (h, w) = self.load_image(index) # Letterbox shape = self.batch_shapes[self.batch[index]] if self.rect else self.img_size # final letterboxed shape img, ratio, pad = letterbox(img, shape, auto=False, scaleup=self.augment)     Three images are randomly selected, and a total of four images are randomly cropped and then combined.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  def load_mosaic(self, index): # YOLOv5 4-mosaic loader. Loads 1 image + 3 random images into a 4-image mosaic labels4, segments4 = [], [] s = self.img_size yc, xc = (int(random.uniform(-x, 2 * s + x)) for x in self.mosaic_border) # mosaic center x, y indices = [index] + random.choices(self.indices, k=3) # 3 additional image indices random.shuffle(indices) for i, index in enumerate(indices): # Load image img, _, (h, w) = self.load_image(index) # place img in img4 if i == 0: # top left img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8) # base image with 4 tiles x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc # xmin, ymin, xmax, ymax (large image) x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h # xmin, ymin, xmax, ymax (small image) elif i == 1: # top right x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h elif i == 2: # bottom left x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h) x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h) elif i == 3: # bottom right x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h) x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h) img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b] # img4[ymin:ymax, xmin:xmax] padw = x1a - x1b padh = y1a - y1b     Mixup   mixup: BEYOND EMPIRICAL RISK MINIMIZATION\n  Mixup creates a virtual feature-target vector $ (\\tilde x, \\tilde y) $ from two randomly selected feature-target vectors $ (x_i, y_i) $ and $ (x_j, y_j) $ from the training data as follows.\n  Here, λ is sampled from the beta distribution. $$ \\tilde x = \\lambda x_i + (1 - \\lambda) x_j $$ $$ \\tilde y = \\lambda y_i + (1 - \\lambda) y_j $$\n  The image below is a mixup image with λ=0.5.   In YOLOv5, mixup is used in the LoadImagesAndLabels class in the __getitem__ function as shown below.\n1 2 3  # MixUp augmentation if random.random() \u0026lt; hyp[\u0026#39;mixup\u0026#39;]: img, labels = mixup(img, labels, *self.load_mosaic(random.randint(0, self.n - 1)))     The following is YOLOv5\u0026rsquo;s implementation of mixup.\n1 2 3 4 5 6  def mixup(im, labels, im2, labels2): # Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf r = np.random.beta(32.0, 32.0) # mixup ratio, alpha=beta=32.0 im = (im * r + im2 * (1 - r)).astype(np.uint8) labels = np.concatenate((labels, labels2), 0) return im, labels     Copy-Paste  Random scale, jittering, and random horizontal flipping are applied to two randomly selected images. A segmentation object is randomly selected from one image and pasted onto another image.  Source: Ghiasi, Golnaz, et al. \u0026ldquo;Simple copy-paste is a strong data augmentation method for instance segmentation.\u0026rdquo; Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021, Figure2\n  In YOLOv5, after creating a mosaic in the load_mosaic function, Copy-Paste is applied as shown below. Here, img4 is the mosaic image, labels4 and segments4 are the label and segmentation information in the mosaic image.\n1  img4, labels4, segments4 = copy_paste(img4, labels4, segments4, p=self.hyp[\u0026#39;copy_paste\u0026#39;])     The implementation of Copy-Paste in YOLOv5 is shown below. Note that random scale and jitter are applied elsewhere, so they’re not included here. If there is no segmentation information, Copy-Paste is not applied.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  def copy_paste(im, labels, segments, p=0.5): # Implement Copy-Paste augmentation https://arxiv.org/abs/2012.07177, labels as nx5 np.array(cls, xyxy) n = len(segments) if p and n: h, w, c = im.shape # height, width, channels im_new = np.zeros(im.shape, np.uint8) for j in random.sample(range(n), k=round(p * n)): l, s = labels[j], segments[j] box = w - l[3], l[2], w - l[1], l[4] ioa = bbox_ioa(box, labels[:, 1:5]) # intersection over area if (ioa \u0026lt; 0.30).all(): # allow 30% obscuration of existing labels labels = np.concatenate((labels, [[l[0], *box]]), 0) segments.append(np.concatenate((w - s[:, 0:1], s[:, 1:2]), 1)) cv2.drawContours(im_new, [segments[j].astype(np.int32)], -1, (1, 1, 1), cv2.FILLED) result = cv2.flip(im, 1) # augment segments (flip left-right) i = cv2.flip(im_new, 1).astype(bool) im[i] = result[i] # cv2.imwrite(\u0026#39;debug.jpg\u0026#39;, im) # debug return im, labels, segments     Random Perspective   In the load_mosaic function, Random Perspective is applied using the code below.\n1 2 3 4 5 6 7 8 9 10  img4, labels4 = random_perspective(img4, labels4, segments4, degrees=self.hyp[\u0026#39;degrees\u0026#39;], translate=self.hyp[\u0026#39;translate\u0026#39;], scale=self.hyp[\u0026#39;scale\u0026#39;], shear=self.hyp[\u0026#39;shear\u0026#39;], perspective=self.hyp[\u0026#39;perspective\u0026#39;], border=self.mosaic_border) # border to remove     Here is part of the implementation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  # Center C = np.eye(3) C[0, 2] = -im.shape[1] / 2 # x translation (pixels) C[1, 2] = -im.shape[0] / 2 # y translation (pixels) # Perspective P = np.eye(3) P[2, 0] = random.uniform(-perspective, perspective) # x perspective (about y) P[2, 1] = random.uniform(-perspective, perspective) # y perspective (about x) # Rotation and Scale R = np.eye(3) a = random.uniform(-degrees, degrees) # a += random.choice([-180, -90, 0, 90]) # add 90deg rotations to small rotations s = random.uniform(1 - scale, 1 + scale) # s = 2 ** random.uniform(-scale, scale) R[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s) # Shear S = np.eye(3) S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180) # x shear (deg) S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180) # y shear (deg) # Translation T = np.eye(3) T[0, 2] = random.uniform(0.5 - translate, 0.5 + translate) * width # x translation (pixels) T[1, 2] = random.uniform(0.5 - translate, 0.5 + translate) * height # y translation (pixels) # Combined rotation matrix M = T @ S @ R @ P @ C # order of operations (right to left) is IMPORTANT if (border[0] != 0) or (border[1] != 0) or (M != np.eye(3)).any(): # image changed if perspective: im = cv2.warpPerspective(im, M, dsize=(width, height), borderValue=(114, 114, 114)) else: # affine im = cv2.warpAffine(im, M[:2], dsize=(width, height), borderValue=(114, 114, 114))     Albumentation, HSV, Horizontal and Vertical Flip When the augment parameter is set to True, Albumentation, HSV color adjustment, and horizontal/vertical flip are applied.\n  albumentations\n albumentations is a Python library for image augmentation. When an object of the LoadImagesAndLabels class is created, it is initialized in the __init__ method as shown below.  1  self.albumentations = Albumentations(size=img_size) if augment else None    Albumentations is applied only if the library is installed. By default, albumentations is commented out in requirements.txt. By default, RandomResizedCrop, Blur, MedianBlur, ToGray, and CLAHE are applied.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  try: import albumentations as A check_version(A.__version__, \u0026#39;1.0.3\u0026#39;, hard=True) # version requirement T = [ A.RandomResizedCrop(height=size, width=size, scale=(0.8, 1.0), ratio=(0.9, 1.11), p=0.0), A.Blur(p=0.01), A.MedianBlur(p=0.01), A.ToGray(p=0.01), A.CLAHE(p=0.01), A.RandomBrightnessContrast(p=0.0), A.RandomGamma(p=0.0), A.ImageCompression(quality_lower=75, p=0.0)] # transforms self.transform = A.Compose(T, bbox_params=A.BboxParams(format=\u0026#39;yolo\u0026#39;, label_fields=[\u0026#39;class_labels\u0026#39;])) LOGGER.info(prefix + \u0026#39;, \u0026#39;.join(f\u0026#39;{x}\u0026#39;.replace(\u0026#39;always_apply=False, \u0026#39;, \u0026#39;\u0026#39;) for x in T if x.p)) except ImportError: # package not installed, skip     Hue, Saturation, Value adjustments\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  def augment_hsv(im, hgain=0.5, sgain=0.5, vgain=0.5): # HSV color-space augmentation if hgain or sgain or vgain: r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1 # random gains hue, sat, val = cv2.split(cv2.cvtColor(im, cv2.COLOR_BGR2HSV)) dtype = im.dtype # uint8 x = np.arange(0, 256, dtype=r.dtype) lut_hue = ((x * r[0]) % 180).astype(dtype) lut_sat = np.clip(x * r[1], 0, 255).astype(dtype) lut_val = np.clip(x * r[2], 0, 255).astype(dtype) im_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val))) cv2.cvtColor(im_hsv, cv2.COLOR_HSV2BGR, dst=im) # no return needed     Vertical and horizontal flip\n1 2 3 4 5 6 7 8 9 10 11  # Flip up-down if random.random() \u0026lt; hyp[\u0026#39;flipud\u0026#39;]: img = np.flipud(img) if nl: labels[:, 2] = 1 - labels[:, 2] # Flip left-right if random.random() \u0026lt; hyp[\u0026#39;fliplr\u0026#39;]: img = np.fliplr(img) if nl: labels[:, 1] = 1 - labels[:, 1]     ","date":"2023-03-16T00:20:00+09:00","permalink":"https://kimberlykang.github.io/p/yolov5_augmentation/","title":"YOLOv5 Augmentation"},{"content":"In this post, we\u0026rsquo;ll look at evolve, YOLOv5\u0026rsquo;s method for selecting hyperparameters.\nTable of Contents  Evolve Fitness Meta Parent Selection Mutation Saving Hyperparameters  Evolve YOLOv5 uses a Genetic Algorithm for hyperparameter optimization, called hyperparameter evolution. You can run it by passing the --evolve argument to train.py. Below is an example of using evolve from the YOLOv5 GitHub.\n1 2 3 4 5 6 7 8  # Single-GPU python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --evolve # Multi-GPU for i in 0 1 2 3 4 5 6 7; do sleep $(expr 30 \\* $i) \u0026amp;\u0026amp; # 30-second delay (optional) echo \u0026#39;Starting GPU \u0026#39;$i\u0026#39;...\u0026#39; \u0026amp;\u0026amp; nohup python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --device $i --evolve \u0026gt; evolve_gpu_$i.log \u0026amp; done    If you do not specify the number of generations after --evolve, it will by default generate 300 sets of hyperparameters. For these 300 hyperparameter sets, training is run for 10 epochs each, and the hyperparameter with the highest fitness is reported.  Fitness 1 2 3 4  def fitness(x): # Model fitness as a weighted combination of metrics w = [0.0, 0.0, 0.1, 0.9] # weights for [P, R, mAP@0.5, mAP@0.5:0.95] return (x[:, :4] * w).sum(1)    The above is the default fitness function. x contains values in order: precision, recall, mAP@0.5, mAP@0.5:0.95. In this fitness function, precision and recall are not used. The score uses 10% mAP@0.5 and 90% mAP@0.5:0.95. Depending on the dataset or the nature of the problem, the most important metric (precision, recall, or mAP) may differ. So, instead of using the fitness function as-is, you should adjust it as appropriate for your case.  Meta Before selecting hyperparameters using the Genetic Algorithm, meta is defined to set the mutation scale, minimum, and maximum for each hyperparameter. Below are the default meta values set in train.py.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  # Hyperparameter evolution metadata (mutation scale 0-1, lower_limit, upper_limit) meta = { \u0026#39;lr0\u0026#39;: (1, 1e-5, 1e-1), # initial learning rate (SGD=1E-2, Adam=1E-3) \u0026#39;lrf\u0026#39;: (1, 0.01, 1.0), # final OneCycleLR learning rate (lr0 * lrf) \u0026#39;momentum\u0026#39;: (0.3, 0.6, 0.98), # SGD momentum/Adam beta1 \u0026#39;weight_decay\u0026#39;: (1, 0.0, 0.001), # optimizer weight decay \u0026#39;warmup_epochs\u0026#39;: (1, 0.0, 5.0), # warmup epochs (fractions ok) \u0026#39;warmup_momentum\u0026#39;: (1, 0.0, 0.95), # warmup initial momentum \u0026#39;warmup_bias_lr\u0026#39;: (1, 0.0, 0.2), # warmup initial bias lr \u0026#39;box\u0026#39;: (1, 0.02, 0.2), # box loss gain \u0026#39;cls\u0026#39;: (1, 0.2, 4.0), # cls loss gain \u0026#39;cls_pw\u0026#39;: (1, 0.5, 2.0), # cls BCELoss positive_weight \u0026#39;obj\u0026#39;: (1, 0.2, 4.0), # obj loss gain (scale with pixels) \u0026#39;obj_pw\u0026#39;: (1, 0.5, 2.0), # obj BCELoss positive_weight \u0026#39;iou_t\u0026#39;: (0, 0.1, 0.7), # IoU training threshold \u0026#39;anchor_t\u0026#39;: (1, 2.0, 8.0), # anchor-multiple threshold \u0026#39;anchors\u0026#39;: (2, 2.0, 10.0), # anchors per output grid (0 to ignore) \u0026#39;fl_gamma\u0026#39;: (0, 0.0, 2.0), # focal loss gamma (efficientDet default gamma=1.5) \u0026#39;hsv_h\u0026#39;: (1, 0.0, 0.1), # image HSV-Hue augmentation (fraction) \u0026#39;hsv_s\u0026#39;: (1, 0.0, 0.9), # image HSV-Saturation augmentation (fraction) \u0026#39;hsv_v\u0026#39;: (1, 0.0, 0.9), # image HSV-Value augmentation (fraction) \u0026#39;degrees\u0026#39;: (1, 0.0, 45.0), # image rotation (+/- deg) \u0026#39;translate\u0026#39;: (1, 0.0, 0.9), # image translation (+/- fraction) \u0026#39;scale\u0026#39;: (1, 0.0, 0.9), # image scale (+/- gain) \u0026#39;shear\u0026#39;: (1, 0.0, 10.0), # image shear (+/- deg) \u0026#39;perspective\u0026#39;: (0, 0.0, 0.001), # image perspective (+/- fraction), range 0-0.001 \u0026#39;flipud\u0026#39;: (1, 0.0, 1.0), # image flip up-down (probability) \u0026#39;fliplr\u0026#39;: (0, 0.0, 1.0), # image flip left-right (probability) \u0026#39;mosaic\u0026#39;: (1, 0.0, 1.0), # image mosaic (probability) \u0026#39;mixup\u0026#39;: (1, 0.0, 1.0), # image mixup (probability) \u0026#39;copy_paste\u0026#39;: (1, 0.0, 1.0)} # segment copy-paste (probability)   Parent Selection 1 2 3 4 5 6 7 8 9 10 11  # Select parent(s) parent = \u0026#39;single\u0026#39; # parent selection method: \u0026#39;single\u0026#39; or \u0026#39;weighted\u0026#39; x = np.loadtxt(evolve_csv, ndmin=2, delimiter=\u0026#39;,\u0026#39;, skiprows=1) n = min(5, len(x)) # number of previous results to consider x = x[np.argsort(-fitness(x))][:n] # top n mutations w = fitness(x) - fitness(x).min() + 1E-6 # weights (sum \u0026gt; 0) if parent == \u0026#39;single\u0026#39; or len(x) == 1: # x = x[random.randint(0, n - 1)] # random selection x = x[random.choices(range(n), weights=w)[0]] # weighted selection elif parent == \u0026#39;weighted\u0026#39;: x = (x * w.reshape(n, 1)).sum(0) / w.sum() # weighted combination    Select the top 5 parents according to fitness score. The fitness score of each parent is used as its selection probability. single: Randomly selects one parent considering probabilities. weighted: Uses the average values of the 5 parents. By default, the single method is used.  Mutation 1 2 3 4 5 6 7 8 9 10 11  # Mutate mp, s = 0.8, 0.2 # mutation probability, sigma npr = np.random npr.seed(int(time.time())) g = np.array([meta[k][0] for k in hyp.keys()]) # gains 0-1 ng = len(meta) v = np.ones(ng) while all(v == 1): # mutate until a change occurs (prevent duplicates) v = (g * (npr.random(ng) \u0026lt; mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0) for i, k in enumerate(hyp.keys()): # plt.hist(v.ravel(), 300) hyp[k] = float(x[i + 7] * v[i]) # mutate    g: The mutation scale for each hyperparameter. The randn function generates a normal distribution with mean 0 and variance 1. By multiplying npr.randn(ng) with s and then adding 1, you get a normal distribution with mean 1 and variance s. (npr.random(ng) \u0026lt; mp): Decides whether to apply mutation with probability mp for each hyperparameter. The initial v array (all 1s) has some values randomly changed; these are multiplied to the parent hyperparameters. Example values for v:  [1.003, 1.0048, 1.028, 0.8988, 1, 1, 0.99587, 1.032, 0.99873, 1, 1, 0.92108, 1, 0.99281, 1, 1.0333, 0.91948, 0.9941, 1.0725, 0.95916, 1, 1, 1, 1.0628, 1, 1.0312, 0.95677, 1]    After mutation, the values are constrained within the min and max defined in the meta.\n1 2 3 4 5  # Constrain to limits for k, v in meta.items(): hyp[k] = max(hyp[k], v[1]) # lower limit hyp[k] = min(hyp[k], v[2]) # upper limit hyp[k] = round(hyp[k], 5) # significant digits   Saving Hyperparameters 1 2 3 4 5 6 7  # Train mutation results = train(hyp.copy(), opt, device, callbacks) callbacks = Callbacks() # Write mutation results keys = (\u0026#39;metrics/precision\u0026#39;, \u0026#39;metrics/recall\u0026#39;, \u0026#39;metrics/mAP_0.5\u0026#39;, \u0026#39;metrics/mAP_0.5:0.95\u0026#39;, \u0026#39;val/box_loss\u0026#39;, \u0026#39;val/obj_loss\u0026#39;, \u0026#39;val/cls_loss\u0026#39;) print_mutation(keys, results, hyp.copy(), save_dir, opt.bucket)    After training, the print_mutation function saves the training results and the best hyperparameters. Below is an example of saved hyperparameters.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  # YOLOv5 Hyperparameter Evolution Results# Best generation: 118# Last generation: 164# metrics/precision, metrics/recall, metrics/mAP_0.5, metrics/mAP_0.5:0.95, val/box_loss, val/obj_loss, val/cls_loss# 0.79541, 0.81562, 0.82932, 0.52318, 0.017451, 0.046121, 0.0056865lr0:0.01498lrf:0.01813momentum:0.89668weight_decay:0.0007warmup_epochs:2.9996warmup_momentum:0.95warmup_bias_lr:0.09796box:0.03177cls:0.72138cls_pw:1.0919obj:1.0486obj_pw:1.006iou_t:0.2anchor_t:2.201fl_gamma:0.0hsv_h:0.01734hsv_s:0.55193hsv_v:0.46602degrees:0.0translate:0.08528scale:0.38912shear:0.0perspective:0.0flipud:0.0fliplr:0.5mosaic:0.99787mixup:0.0copy_paste:0.0anchors:2.0  ","date":"2023-02-28T00:20:00+09:00","permalink":"https://kimberlykang.github.io/p/yolov5_evolve/","title":"YOLOv5 Evolve"},{"content":"In this post, we will look into how anchors are selected in YOLOv5.\nTable of Contents  Motivation Genetic Algorithm GA Used in YOLOv5 Autoanchor  Motivation YOLOv5 is a model released by Glenn Jocher on Github. Before YOLOv5, he wrote and released the Pytorch version of YOLOv3 on Github. While watching issues posted by many users training custom datasets, he realized that the aspect ratio of bounding boxes in their datasets was often quite different from those in the COCO dataset. To address this, he added a genetic algorithm to the k-means clustering commonly used for selecting anchor boxes, and introduced autoanchor in YOLOv5 to automate this whole process. You can learn more about this in the interview at the link below.\nhttps://www.youtube.com/watch?v=O4jOqVqyAo8\nGenetic Algorithm A Genetic Algorithm (GA) is a problem-solving method inspired by natural selection. Let’s look at the terms needed to understand GA:\n Population: A set of solutions to the problem. Chromosome: A single solution made up of genes. Crossover: Offspring are created using genes from parents. Mutation: Some genes change. Fitness: How suitable a chromosome/solution is for the problem.  GA proceeds in the following steps:\nGA Used in YOLOv5 In YOLO, the GA process is modified as shown below.\n The population in YOLOv5 is the width and height of the bounding boxes. GA is used to find the most suitable width and height for anchor boxes. Parents are selected using k-means. After finding k centroids (width, height), each centroid becomes a parent. No crossover is performed for the k parents; instead, only mutation is done for as many generations as desired. By default, 1000 width-height pairs are mutated per parent. The fitness of the 1000 generated (w, h) pairs is calculated, and the one with the highest fitness is selected. In the end, k (w, h) pairs are chosen, and these become the anchor boxes used in YOLOv5.  Autoanchor After the dataloader is created and before training starts, it is checked whether the current anchors are suitable for the current dataset using check_anchors. Inside this function, Autoanchor runs.\n1 2 3  if not resume: if not opt.noautoanchor: check_anchors(dataset, model=model, thr=hyp[\u0026#39;anchor_t\u0026#39;], imgsz=imgsz) # run AutoAnchor   The following metric is used to determine whether the current anchors are suitable. Here, wh is an array of all bounding box [width, height] pairs, and k is the array of current anchor [width, height] pairs.\n1 2 3 4 5 6 7  def metric(k): # compute metric r = wh[:, None] / k[None] x = torch.min(r, 1 / r).min(2)[0] # ratio metric best = x.max(1)[0] # best_x aat = (x \u0026gt; 1 / thr).float().sum(1).mean() # anchors above threshold bpr = (best \u0026gt; 1 / thr).float().mean() # best possible recall return bpr, aat    torch.min(r, 1 / r): Calculates the ratio of the bounding box width and height to each anchor box\u0026rsquo;s width and height. This results in a shape of (number of bboxes, number of anchors, 2). x = torch.min(r, 1 / r).min(2)[0]: For each bounding box, selects the smaller ratio with respect to each anchor, resulting in each bounding box having one ratio per anchor. The shape is (number of bboxes, number of anchors). best = x.max(1)[0]: For each bounding box, selects the anchor with the highest ratio. best is an array containing the highest ratio for each bounding box. The shape is (number of bboxes). aat = (x \u0026gt; 1 / thr).float().sum(1).mean() bpr = (best \u0026gt; 1 / thr).float().mean()  If bpr is less than 0.98, new anchors are generated using the kmean_anchors function.\n1 2 3 4 5 6  if bpr \u0026gt; 0.98: # threshold to recompute LOGGER.info(f\u0026#39;{s}Current anchors are a good fit to dataset ✅\u0026#39;) else: LOGGER.info(f\u0026#39;{s}Anchors are a poor fit to dataset ⚠️, attempting to improve...\u0026#39;) na = m.anchors.numel() // 2 # number of anchors anchors = kmean_anchors(dataset, n=na, img_size=imgsz, thr=thr, gen=1000, verbose=False)   In the kmean_anchors function, k-means is first used to find k centroids from the bounding boxes.\n1 2 3  from scipy.cluster.vq import kmeans s = wh.std(0) # sigmas for whitening k = kmeans(wh / s, n, iter=30)[0] * s # points   After finding the centroids, mutation is performed using those centroids and fitness is calculated with anchor_fitness. The same metric as before is used, considering the width and height ratios.\n1 2 3 4 5  def metric(k, wh): # compute metrics r = wh[:, None] / k[None] x = torch.min(r, 1 / r).min(2)[0] # ratio metric # x = wh_iou(wh, torch.tensor(k)) # iou metric return x, x.max(1)[0] # x, best_x   Mutation is performed gen times (default is 1000). After generating 1000 mutated anchors, the one with the highest fitness is used as the final anchor.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # Evolve f, sh, mp, s = anchor_fitness(k), k.shape, 0.9, 0.1 # fitness, generations, mutation prob, sigma pbar = tqdm(range(gen), bar_format=TQDM_BAR_FORMAT) # progress bar for _ in pbar: v = np.ones(sh) while (v == 1).all(): # mutate until a change occurs (prevent duplicates) v = ((npr.random(sh) \u0026lt; mp) * random.random() * npr.randn(*sh) * s + 1).clip(0.3, 3.0) kg = (k.copy() * v).clip(min=2.0) fg = anchor_fitness(kg) if fg \u0026gt; f: f, k = fg, kg.copy() pbar.desc = f\u0026#39;{PREFIX}Evolving anchors with Genetic Algorithm: fitness = {f:.4f}\u0026#39; if verbose: print_results(k, verbose) return print_results(k).astype(np.float32)   ","date":"2023-02-16T00:20:00+09:00","permalink":"https://kimberlykang.github.io/p/yolov5_anchor/","title":"YOLOv5 anchor"},{"content":"In this post, I will take an overall look at the train.py code from the YOLOv5 repository.\nTable of Contents  Data parallel Printing and Saving Training Configurations Logger torch_distributed_zero_first Dataloader  Data parallel Throughout the training code, you will come across a variable called RANK. To understand this, you need to learn about some terms related to data parallel. Let’s briefly go over these terms before examining the code.\n data parallel: This is one way of training models. Multiple copies of the model are created, and each copy is trained in parallel. Each copied model learns with different data, calculates its own local gradients/sub-gradients, and then, through synchronization, the gradients from all copies are calculated together. node: When using data parallel training, this refers to the machine or server where training occurs. Each node creates multiple processes, makes copies of the model, and trains on those copies. rank: The ID of each process that has been copied onto a node. When there are multiple nodes, rank is divided into local rank and global rank. Local rank is the ID within a single node, and global rank is the ID assigned across all processes in all nodes. world: The group that contains all processes across all nodes. The number of processes in this world is called the world size. Usually, every model copy runs on one GPU, so often the world size equals the number of GPUs. If the model is large, sometimes one model copy uses multiple GPUs.  Source: https://github.com/pytorch/examples/blob/main/distributed/ddp/README.md \nThe image above shows a data parallel application running on two nodes. In this case, each model copy trains on two GPUs, so on each node, the process with LOCAL RANK0 runs on GPU0 and GPU1, and the process with LOCAL RANK1 runs on GPU2 and GPU3. The GLOBAL RANK is assigned as 0 to 3 across all processes, regardless of the node.\nPrinting and Saving Training Configurations When the main function starts, it prints the current args and checks for what is needed for training, as shown below. At this point, the RANK variable appears as mentioned earlier; when RANK is -1, it means data parallel is not being used. To ensure only one process prints, the code below is executed only when the rank is 0.\nThe function check_git_status checks your current branch’s commit history and automatically runs git pull if there are any new commits available. For easier code management and reproducibility, I personally comment this out and run git pull manually as needed.\n1 2 3 4 5  # main function if RANK in {-1, 0}: print_args(vars(opt)) # check_git_status() check_requirements()   When training starts, the hyperparameters and training-related settings being used are saved as shown below. Each file is saved in the runs/train/exp folder, so you can check the settings used after training finishes. The exp folder is automatically created with an incremented number (such as exp1, exp2, etc.) every time you start new training.\n1 2 3 4  # train function if not evolve: yaml_save(save_dir / \u0026#39;hyp.yaml\u0026#39;, hyp) yaml_save(save_dir / \u0026#39;opt.yaml\u0026#39;, vars(opt))   Logger 1 2 3 4 5 6  if RANK in {-1, 0}: loggers = Loggers(save_dir, weights, opt, hyp, LOGGER) # loggers instance # Register actions for k in methods(loggers): callbacks.register_action(k, callback=getattr(loggers, k))   To ensure logging happens only once, the Logger object is created only when the rank is 0. Then, the methods of the loggers object are registered as callbacks with their respective function names. When calling a function, you use its name as shown below.\n1  callbacks.run(\u0026#39;on_pretrain_routine_end\u0026#39;, labels, names)   For example, the above code is executed after the training setup is complete. When this code runs, the on_pretrain_routine_end method of the Loggers class is called, and it saves label information as an image.\ntorch_distributed_zero_first If there is code that must be executed by the first rank before other processes, YOLOv5 provides the torch_distributed_zero_first function to handle this.\n1 2 3 4 5 6 7 8  @contextmanager def torch_distributed_zero_first(local_rank: int): # Decorator to make all processes in distributed training wait for each local_master to do something if local_rank not in [-1, 0]: dist.barrier(device_ids=[local_rank]) yield if local_rank == 0: dist.barrier(device_ids=[0])   The torch_distributed_zero_first function is implemented using torch\u0026rsquo;s contextmanager decorator and the barrier function.\n In a function defined with @contextmanager, the code before yield is executed before the with block, and the code after yield is executed after the with block. The barrier function blocks all processes until the whole group reaches the barrier. In the torch_distributed_zero_first function above, all processes except rank0 wait before yield for rank0. Only rank0 encounters yield and executes the with code block first. Then, when rank0 reaches the barrier after the block, the remaining processes pass yield and execute the with code block. This ensures that rank0 always executes the with code block before the other processes.  1 2  with torch_distributed_zero_first(LOCAL_RANK): data_dict = data_dict or check_dataset(data)   The above is an example of how torch_distributed_zero_first is used in the training code.\n All processes except rank0 wait until rank0 completes check_dataset and reaches the barrier. rank0 runs check_dataset and downloads the data if it does not exist. After finishing the check_dataset function, rank0 reaches the barrier. The other processes also execute check_dataset. At this point, since the data has already been downloaded by rank0, they do not download it again. By using the torch_distributed_zero_first function, the download is performed only once.  Optimizer \u0026amp; Learning rate schedule If you do not specify the optimizer argument when starting training, the default SGD is used. The values for lr0, momentum, and weight_decay are taken from the hyperparameters specified before starting the training.\n1  optimizer = smart_optimizer(model, opt.optimizer, hyp[\u0026#39;lr0\u0026#39;], hyp[\u0026#39;momentum\u0026#39;], hyp[\u0026#39;weight_decay\u0026#39;])   By default, a linear learning rate is used. This is implemented using LambdaLR as shown below.\n1 2 3 4 5  if opt.cos_lr: lf = one_cycle(1, hyp[\u0026#39;lrf\u0026#39;], epochs) # cosine 1-\u0026gt;hyp[\u0026#39;lrf\u0026#39;] else: lf = lambda x: (1 - x / epochs) * (1.0 - hyp[\u0026#39;lrf\u0026#39;]) + hyp[\u0026#39;lrf\u0026#39;] # linear scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)   Dataloader Since each rank has its own dataloader, the total batch size is divided by WORLD_SIZE to compute the new batch size for each dataloader.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  train_loader, dataset = create_dataloader(train_path, imgsz, batch_size // WORLD_SIZE, gs, single_cls, hyp=hyp, augment=True, cache=None if opt.cache == \u0026#39;val\u0026#39; else opt.cache, rect=opt.rect, rank=LOCAL_RANK, workers=workers, image_weights=opt.image_weights, quad=opt.quad, prefix=colorstr(\u0026#39;train: \u0026#39;), shuffle=True, seed=opt.seed)   Currently, YOLOv5 supports validation on a single GPU only.\n1 2 3 4 5 6 7 8 9 10 11 12 13  if RANK in {-1, 0}: val_loader = create_dataloader(val_path, imgsz, batch_size // WORLD_SIZE * 2, gs, single_cls, hyp=hyp, cache=None if noval else opt.cache, rect=True, rank=-1, workers=workers * 2, pad=0.5, prefix=colorstr(\u0026#39;val: \u0026#39;))[0]   ","date":"2023-02-06T00:20:00+09:00","permalink":"https://kimberlykang.github.io/p/yolov5_train/","title":"YOLOv5 Training"}]