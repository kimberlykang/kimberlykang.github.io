[{"content":"본 포스트에서는 2016년 CVPR에 발표된 논문 \u0026ldquo;You Only Look Once: Unified, Real-Time Object Detection\u0026quot;을 살펴보겠습니다.\nhttps://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\n목차  개념 이해 Bounding Box Class Probability Output 네트워크 구조 학습 테스트  개념 이해 YOLO에서는 이미지에서 object를 찾을 때, grid를 사용합니다. 이를 간략하게 살펴보면\n 이미지를 S×S개의 grid로 나눕니다. 각 grid cell에서 B개의 bounding box의 좌표(x, y, w, h)와 그 bounding box의 confidence score(c)를 예측합니다. 각 grid cell마다 C개의 class probability를 예측합니다. confidence score과 class probability로 최종 score와 class를 결정하고, 이 때 score가 threshold보다 높은 bounding box가 최종 object detection의 결과가 됩니다.  출처: Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp.780, Figure2\n논문에서 PASCAL VOC 데이터를 학습 실험의 예를 들어 살펴보면,\n S는 하이퍼파라미터입니다. 해당 실험에서는 S=7을 사용합니다. B 또한 하이퍼파라미터입니다. 해당 실험에서는 B=2를 사용합니다. 따라서, 각 grid cell마다 bounding box(x, y, w, h, c)를 두 세트씩 예측합니다. PASCAL VOC 데이터는 총 class 수가 20개인 데이터입니다. 따라서 C=20이 됩니다. 각 grid cell마다 20개의 class에 대한 class probability를 예측합니다.  지금까지 YOLO가 어떻게 object를 찾는지 대략적으로 살펴봤습니다. 아래에서는 각 예측값들이 어떻게 계산되고 어떤 의미를 갖는지 살펴보겠습니다.\nBounding Box bounding box는 (x, y, w, h, c)로 이루어져 있습니다.\n (x, y): bounding box 중심의 좌표.  각 grid cell마다 B개의 bounding box를 가진다는 의미는, 이 B개의 bounding box의 중심이 해당 grid cell 안에 있다는 의미입니다. box의 네 꼭지점은 bounding box 밖에 있어도 상관이 없습니다. grid cell 안에서 상대적으로 어느 위치에 있는지를 표시하며, 0~1 사이의 값을 가집니다.   (w, h): bounding box의 너비, 높이.  전체 이미지 너비, 높이에 대해 상대적인 값을 사용하며 0~1 사이의 값을 가집니다.   c: box가 object를 갖는 것에 대해 얼마나 확신하고, 그 box의 좌표가 얼마나 정확한 가에 대한 값입니다.   좌표가 얼마나 정확한지를 계산할 때에는 예측한 좌표와 Ground Truth(GT) 사이의 Intersection Over Union(IOU)를 계산합니다.    Class Probability  C: 각 grid cell이 object를 가지고 있을 때의 class probability입니다.   bounding box 갯수와 상관 없이 grid cell마다 한 세트의 class probability만 예측합니다.    Output  이미지를 S×S grid로 나누고, 각 grid마다 B개의 bounding box와 C개 class에 대한 class probability가 있다고 할 때, 최종 output tensor의 shape은 S×S×(B*5+C)가 됩니다. PASCAL VOC 실험 예를 들면, S=7, B=2, C=20입니다. 따라서 총 49개의 grid가 있고, 각 grid cell마다 30개의 예측값을 갖게 됩니다. 아래는 30개의 예측값 중 하나의 예시입니다.  [x, y, w, h, c, 배경일 확률, 비행기일 확률, \u0026hellip;, 모니터일 확률] [0.4, 0.3, 0.8, 0.7, 0.9, 0.003, 0.8, \u0026hellip; , 0.012]    네트워크 구조  TBU  학습  학습할 때는, 하나의 bounding box predictor로 각 object를 예측하기 위해, 각 GT와 가장 IOU가 높은 bounding box에 해당 GT를 할당합니다. 이를 통해 bounding box들은 서로 다른 특성을 학습하게 되고, 특정 사이즈, 특정 비율, 특정 클래스를 더 잘 예측할 수 있게 됩니다. Loss  1obji: cell i에 object가 나타났는지 여부를 의미합니다. 1objij: cell i에 있는 j번째 bounding box가 cell i에 있는 GT에 해당하는 예측값인지 여부입니다. 즉, cell i에 있는 j번째 bounding box가 GT와의 IOU가 가장 큰 지 여부입니다. 큰 box는 편차가 조금 있어도 크게 상관 없지만 작은 box에는 큰 영향을 끼칩니다. 이 문제를 해결하기 위해 bounding box 너비와 높이 값에 루트를 씌워 줬습니다.    테스트   테스트할 때는 class probability와 confidence score를 곱해서 최종 score를 계산합니다. 최종 score는 해당 박스에 class가 나타날 확률과 예측한 box가 실제 object에 얼마나 잘 맞는 지 두 가지 의미를 모두 갖고 있습니다. 아래 그림의 bounding box 선의 굵기가 confidence를 의미합니다. 이 confidence와 class probability를 곱하여 threshold 이상의 결과가 최종 결과가 됩니다. 출처: Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp.780, Figure2  ","date":"2023-02-06T00:20:00+09:00","permalink":"https://kimberlykang.github.io/p/yolov1_review/","title":"YOLOv1 리뷰"},{"content":"본 포스트에서는 YOLOv5 리파지토리의 train.py 코드를 전체적으로 살펴보도록 하겠습니다.\n목차  Data parellel 학습 조건 프린트 및 저장 Logger torch_distributed_zero_first Dataloader  Data parellel 학습 코드 중간 중간 RANK라는 variable이 나오는데, 이를 이해하기 위해선 data parellel의 몇 가지 용어들을 이해해야 합니다. 코드를 보기 전에 먼저 용어를 간단히 살펴보겠습니다.\ndata parellel는 모델 학습 방법 중 하나입니다. 모델 복사본 여러 개를 만들어, 각 복사된 모델에서 동시에 parellel하게 학습이 이루어집니다. 이 때 각 복사된 모델은 각기 다른 데이터를 학습하고, local gradients/sub-gradients를 계산한 뒤, synchronization 작업을 통해 복사본 전체의 gradients를 계산하게 됩니다.\ndata parellel 학습을 할 때, 학습이 이루어지는 머신 혹은 서버를 node라고 합니다. 각 node에서는 여러 개의 process를 만들어 모델을 복사한 뒤, 복사된 모델에서 학습이 진행되는데 이 때, 각 process의 id를 rank라고 합니다. node가 여러 개 있을 때, rank는 local rank와 global rank로 나뉘게 되는데, local rank는 한 node 안에서의 id이고, global rank는 전체 node 안에 있는 모든 process에 id를 매긴 것입니다.\n전체 node 안의 모든 process를 포함하는 그룹을 world라고 하며, world 안에 있는 process 갯수를 world size라고 합니다. 보통 모델 복사본 하나가 GPU 하나를 사용하고, 따라서 process 하나를 만들기 때문에 world size는 주로 GPU 갯수인 경우가 많습니다. 하지만 경우에 따라 모델이 큰 경우에 모델 복사본 하나가 GPU 여러 개를 사용하는 경우도 있습니다.\n출처: https://github.com/pytorch/examples/blob/main/distributed/ddp/README.md\n위 그림은 두 개 노드에서 학습을 하는 data parellel application입니다. 위 모델은 모델 복사본 하나 당 GPU 두 개를 사용하여 학습하여, 각 node안의 GPU0, GPU1에서 LOCAL RANK0을 가진 process가, GPU2, GPU3에서 LOCAL RANK1을 가진 process가 학습을 진행하게 됩니다. GLOBAL RANK는 node에 상관 없이 전체 process에 대해 0~3까지 할당됩니다.\n학습 조건 프린트 및 저장 main 함수가 시작되면 아래와 같이 현재 args를 프린트하고, 학습에 필요한 것들을 체크하는 코드가 있습니다. 이 때 위에서 살펴본 RANK가 나오는데, RANK가 -1인 경우는 data parellel을 사용하지 않는 경우입니다. 하나의 process에서만 프린트하기 위해 아래와 같이 rank가 0일 경우에만 코드를 실행합니다. check_git_status는 현재 브랜치의 커밋 내역을 확인하여 pull 받을 커밋이 있다면 자동으로 git pull 해 주는 함수입니다. 개인적으로는 코드 관리 및 재현에 용이하게 하기 위해 아래와 같이 코멘트처리 한 뒤 필요할 때마다 수동으로 git pull해서 사용하고 있습니다.\n1 2 3 4 5  # main 함수 if RANK in {-1, 0}: print_args(vars(opt)) # check_git_status() check_requirements()   학습이 시작되면 아래에서 현재 학습에 사용하고 있는 하이퍼파라미터와 학습 관련된 세팅을 저장합니다. 각 파일은 runs/train/exp 폴더에 저장되어 학습이 종료 후 학습에 사용한 설정을 확인할 수 있습니다. exp 폴더는 학습을 새로 할 때마다 exp1 exp2와 같이 자동으로 숫자가 커지며 폴더가 새로 생성됩니다.\n1 2 3 4  # train 함수 if not evolve: yaml_save(save_dir / \u0026#39;hyp.yaml\u0026#39;, hyp) yaml_save(save_dir / \u0026#39;opt.yaml\u0026#39;, vars(opt))   Logger 1 2 3 4 5 6  if RANK in {-1, 0}: loggers = Loggers(save_dir, weights, opt, hyp, LOGGER) # loggers instance # Register actions for k in methods(loggers): callbacks.register_action(k, callback=getattr(loggers, k))   로깅을 한 번만 하기 위해 rank가 0일 경우 Logger 객체를 만듭니다. 그런 다음 loggers의 함수를 각 함수의 이름으로 callback에 등록해 줍니다. 함수를 호출 할 때에는 함수 이름을 이용하여 아래와 같이 호출해줍니다.\n1  callbacks.run(\u0026#39;on_pretrain_routine_end\u0026#39;, labels, names)   예를 들면 위 코드는 학습을 위한 설정 후 실행됩니다. 위 코드가 실행되면 Loggers 클래스의 on_pretrain_routine_end 함수가 실행되어 레이블에 관한 정보를 이미지로 저장합니다.\ntorch_distributed_zero_first data 확인 등과 같이 다음 코드 진행 전에 한 번 수행할 필요가 있는 코드가 있을 때, yolov5에서는 torch_distributed_zero_first 함수를 만들어 이를 수행하고 있습니다.\n1 2 3 4 5 6 7 8  @contextmanager def torch_distributed_zero_first(local_rank: int): # Decorator to make all processes in distributed training wait for each local_master to do something if local_rank not in [-1, 0]: dist.barrier(device_ids=[local_rank]) yield if local_rank == 0: dist.barrier(device_ids=[0])   torch_distributed_zero_first 함수는 torch의 barrier 함수를 이용하여 정의되어 있습니다. barrier 함수는 전체 그룹이 barrier 함수에 도달할 때까지 process를 block합니다. 따라서 위의 torch_distributed_zero_first 에서 rank0을 제외한 나머지 process는 yield 전에서 rank0를 기다리게 되고, 후에 rank0이 barrier를 만나게 되면 동기화가 되며 함수를 빠져나가게 됩니다.\n1 2  with torch_distributed_zero_first(LOCAL_RANK): data_dict = data_dict or check_dataset(data)   위는 학습 코드에서 torch_distributed_zero_first가 사용된 예입니다. rank0을 제외한 나머지 process는 rank0가 check_dataset(data)를 수행할 때까지 기다리고, 후에 rank0가 yield 뒤 코드를 수행하며 barrier 를 만나면 이 함수를 빠져나가게 됩니다. 즉, torch_distributed_zero_first 함수를 사용함으로써 rank0에서만 특정 코드가 수행되도록 만들어줍니다.\nOptimizer \u0026amp; Learning rate schedule train을 시작할 때 optimizer argument를 주지 않는다면 default인 SGD가 사용됩니다. lr0, momentum, weight_decay는 학습 시작시 지정한 하이퍼파라미터에서 가져옵니다.\n1  optimizer = smart_optimizer(model, opt.optimizer, hyp[\u0026#39;lr0\u0026#39;], hyp[\u0026#39;momentum\u0026#39;], hyp[\u0026#39;weight_decay\u0026#39;])   Learning rate은 default로 Linear learning rate을 사용합니다. 아래와 같이 LambdaLR을 사용하여 구현되어 있습니다.\n1 2 3  else: lf = lambda x: (1 - x / epochs) * (1.0 - hyp[\u0026#39;lrf\u0026#39;]) + hyp[\u0026#39;lrf\u0026#39;] # linear scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)   Dataloader 각 rank마다 dataloader가 생기므로, 전체 batch size를 WORLD_SIZE로 나눈 것이 새로운 batch size가 되어 dataloader를 만들게 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  train_loader, dataset = create_dataloader(train_path, imgsz, batch_size // WORLD_SIZE, gs, single_cls, hyp=hyp, augment=True, cache=None if opt.cache == \u0026#39;val\u0026#39; else opt.cache, rect=opt.rect, rank=LOCAL_RANK, workers=workers, image_weights=opt.image_weights, quad=opt.quad, prefix=colorstr(\u0026#39;train: \u0026#39;), shuffle=True, seed=opt.seed)   현재 YOLOv5에서는 validation은 single-GPU만 지원하고 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13  if RANK in {-1, 0}: val_loader = create_dataloader(val_path, imgsz, batch_size // WORLD_SIZE * 2, gs, single_cls, hyp=hyp, cache=None if noval else opt.cache, rect=True, rank=-1, workers=workers * 2, pad=0.5, prefix=colorstr(\u0026#39;val: \u0026#39;))[0]   ","date":"2023-02-06T00:20:00+09:00","permalink":"https://kimberlykang.github.io/p/yolov5_train/","title":"YOLOv5 학습"}]