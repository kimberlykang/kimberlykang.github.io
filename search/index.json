[{"content":"본 포스트에서는 2016년 CVPR에 발표된 논문 \u0026ldquo;You Only Look Once: Unified, Real-Time Object Detection\u0026quot;을 살펴보겠습니다.\nhttps://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\n목차  개념 이해 Bounding Box Class Probability Output 네트워크 구조 학습 테스트  개념 이해 YOLO에서는 이미지에서 object를 찾을 때, grid를 사용합니다. 이를 간략하게 살펴보면\n 이미지를 S×S개의 grid로 나눕니다. 각 grid cell에서 B개의 bounding box의 좌표(x, y, w, h)와 그 bounding box의 confidence score(c)를 예측합니다. 각 grid cell마다 C개의 class probability를 예측합니다. confidence score과 class probability로 최종 score와 class를 결정하고, 이 때 score가 threshold보다 높은 bounding box가 최종 object detection의 결과가 됩니다.   위 이미지는 S=7, B=2일 때 하나의 grid에 대한 예측의 예시입니다. 각 grid cell마다 bounding box(x, y, w, h, c)를 두 세트씩 예측합니다. C=20일 때, 각 grid cell마다 20개의 class에 대한 class probability를 예측합니다.  지금까지 YOLO가 어떻게 object를 찾는지 대략적으로 살펴봤습니다. 아래에서는 각 예측값들이 어떻게 계산되고 어떤 의미를 갖는지 살펴보겠습니다.\nBounding Box bounding box는 (x, y, w, h, c)로 이루어져 있습니다.\n (x, y): bounding box 중심의 좌표.  각 grid cell마다 B개의 bounding box를 가진다는 의미는, 이 B개의 bounding box의 중심이 해당 grid cell 안에 있다는 의미입니다. box의 네 꼭지점은 bounding box 밖에 있어도 상관이 없습니다. grid cell 안에서 상대적으로 어느 위치에 있는지를 표시하며, 0~1 사이의 값을 가집니다.   (w, h): bounding box의 너비, 높이.  전체 이미지 너비, 높이에 대해 상대적인 값을 사용하며 0~1 사이의 값을 가집니다.   c: box가 object를 갖는 것에 대해 얼마나 확신하고, 그 box의 좌표가 얼마나 정확한 가에 대한 값입니다.  $ Pr(Object) * IOU^{truth}_{pred} $ 좌표가 얼마나 정확한지를 계산할 때에는 예측한 좌표와 Ground Truth(GT) 사이의 Intersection Over Union(IOU)를 계산합니다.    Class Probability  C: 각 grid cell이 object를 가지고 있을 때의 class probability입니다.  $ Pr(Class_i|Object) $ bounding box 갯수와 상관 없이 grid cell마다 한 세트의 class probability만 예측합니다.    Output  이미지를 S×S grid로 나누고, 각 grid마다 B개의 bounding box와 C개 class에 대한 class probability가 있다고 할 때, 최종 output tensor의 shape은 S×S×(B*5+C)가 됩니다. PASCAL VOC 실험 예를 들면, S=7, B=2, C=20입니다. 따라서 총 49개의 grid가 있고, 각 grid cell마다 30개의 예측값을 갖게 됩니다. 아래는 30개의 예측값 중 하나의 예시입니다.  [x, y, w, h, c, 배경일 확률, 비행기일 확률, \u0026hellip;, 모니터일 확률] [0.4, 0.3, 0.8, 0.7, 0.9, 0.003, 0.8, \u0026hellip; , 0.012]    네트워크 구조  GoogLeNet과 유사한 구조를 사용했습니다. 출처: Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, You Only Look Once: Unified, Real-Time Object Detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp.781, Figure 3  Loss   sum-squared error를 사용했습니다.\n  $ \\lambda_{coor} $, $ \\lambda_{noobj} $ 사용\n 대부분의 이미지는 object가 있는 grid cell보다 object가 없는 grid cell이 더 많습니다. 그래서 object가 없는 grid cell의 confidence score는 0에 가까워질 때의 gradient가 object가 있는 grid cell의 gradient보다 훨씬 커서 학습 초기에 모델이 diverge 하는 현상이 생길 수 있습니다. 이를 막기 위해, $ \\lambda_{coor}=5 $를 사용하여 bounding box 좌표 예측 loss는 증가시키고, $ \\lambda_{noobj}=0.5 $를 사용하여 object가 없는 box의 confidence 예측 loss는 감소시켰습니다.    $ \\sqrt w $, $ \\sqrt h $ 예측\n 큰 box는 편차가 조금 있어도 크게 상관 없지만 작은 box에는 큰 영향을 끼칩니다. 이 문제를 해결하기 위해 bounding box 너비와 높이 값에 루트를 씌워 줬습니다.    각 object에 가장 IOU가 높은 하나의 bounding box predictor만 할당했습니다.\n  Loss function\n$ \\lambda_{coor} \\displaystyle{\\sum_{i=0}^{s^2}} \\displaystyle{\\sum_{j=0}^{B}} \\mathbb 1_{ij}^{obj} [(x_i-\\hat x_i)^2 + (y_i-\\hat y_i)^2] $\n$ + \\lambda_{coor} \\displaystyle{\\sum_{i=0}^{s^2}} \\displaystyle{\\sum_{j=0}^{B}} \\mathbb 1_{ij}^{obj} [(\\sqrt{w_i} - \\sqrt{\\hat w_i})^2 + (\\sqrt{h_i} - \\sqrt{\\hat h_i})^2] $\n$ + \\displaystyle{\\sum_{i=0}^{s^2}} \\displaystyle{\\sum_{j=0}^{B}} \\mathbb 1_{ij}^{obj} (C_i-\\hat C_i)^2 $ $ + \\lambda_{noobj} \\displaystyle{\\sum_{i=0}^{s^2}} \\displaystyle{\\sum_{j=0}^{B}} \\mathbb 1_{ij}^{noobj} (C_i-\\hat C_i)^2 $\n$ + \\displaystyle{\\sum_{i=0}^{s^2}} \\mathbb 1_{i}^{obj} \\displaystyle{\\sum_{c∈classwa}} (p_i(c)-\\hat p_i(c))^2 $\n 1obji: cell i에 object가 나타났는지 여부를 의미합니다. 1objij: cell i에 있는 j번째 bounding box가 cell i에 있는 GT에 해당하는 예측값인지 여부입니다. 즉, cell i에 있는 j번째 bounding box가 GT와의 IOU가 가장 큰 지 여부입니다.    테스트 $$ Pr(Class_i|Object) * Pr(Object) * IOU^{truth}{pred} = Pr(Class_i) * IOU^{truth}{pred} $$\n  테스트할 때는 class probability와 confidence score를 곱해서 최종 score를 계산합니다. 최종 score는 해당 박스에 class가 나타날 확률과 예측한 box가 실제 object에 얼마나 잘 맞는 지 두 가지 의미를 모두 갖고 있습니다. 아래 그림의 bounding box 선의 굵기가 confidence를 의미합니다. 이 confidence와 class probability를 곱하여 threshold 이상의 결과가 최종 결과가 됩니다. 출처: Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, You Only Look Once: Unified, Real-Time Object Detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp.780, Figure 2  ","date":"2023-05-29T00:17:41+09:00","permalink":"https://kimberlykang.github.io/p/yolov1_review/","title":"YOLOv1 리뷰"},{"content":"본 포스트에서는 YOLOv5의 Validation에 대해 살펴보겠습니다.\n목차  Validation Non-Maximum Suppression Profile Prediction Matrix AP, mAP 계산  Validation 학습 시 한 epoch 학습이 끝날 때마다 validate를 하여 해당 epoch 학습에 대한 평가를 하게 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  if RANK in {-1, 0}: # mAP callbacks.run(\u0026#39;on_train_epoch_end\u0026#39;, epoch=epoch) ema.update_attr(model, include=[\u0026#39;yaml\u0026#39;, \u0026#39;nc\u0026#39;, \u0026#39;hyp\u0026#39;, \u0026#39;names\u0026#39;, \u0026#39;stride\u0026#39;, \u0026#39;class_weights\u0026#39;]) final_epoch = (epoch + 1 == epochs) or stopper.possible_stop if not noval or final_epoch: # Calculate mAP results, maps, _ = validate.run(data_dict, batch_size=batch_size // WORLD_SIZE * 2, imgsz=imgsz, half=amp, model=ema.ema, single_cls=single_cls, dataloader=val_loader, save_dir=save_dir, plots=False, callbacks=callbacks, compute_loss=compute_loss)    muti-gpu 학습일 경우 validate은 gpu 0에서만 하게 됩니다. 위에서 명시해 주지 않은 conf_thres, iou_thres, max_det는 default로 conf_thres=0.001, iou_thres=0.6, max_det=300를 갖게 되고, 후에 Non-Maximum Suppression(NMS)에서 사용됩니다.  Non-Maximum Suppression  각 validation data에 대해 inference를 한 뒤, prediction 결과에 Non-Maximum Suppression(NMS)를 적용합니다. NMS는 겹쳐 있는 여러 개의 bounding box에서 중복을 제거하고 하나를 선택하는 방법입니다. bounding box의 confidence score와 box 사이의 IoU를 고려하여 제거 후 남을 box를 선택하게 됩니다. 아래는 validate할 때 NMS 호출 코드입니다.  1 2 3 4 5 6 7 8 9  with dt[2]: preds = non_max_suppression(preds, conf_thres, iou_thres, labels=lb, multi_label=True, agnostic=single_cls, max_det=max_det)    parameter로 사용되는 conf_thres, iou_thres, max_det는 호출되는 위치에 따라 다른 값이 사용됩니다. val.py에서 호출  conf_thres=0.001: confidence score가 0.001보다 작은 detection 결과는 모두 삭제합니다. iou_thres=0.6: IoU가 0.6 이상이 되는 box들에 대해 confidence score 기준으로 하나의 box를 선택하게 됩니다. max_det=300: 남은 box 중 confidence score가 높은 순으로 300개를 잘라서 반환합니다.   detect.py에서 호출  conf_thres=0.25, iou_thres=0.45, max_det=1000가 사용됩니다.    Profile val.py에서 non_max_suppression를 호출 할 때 with dt[2]로 감싸서 호출을 해 주게 되는데, 이 dt는 바로 Profile 클래스 객체입니다. pre-process, inference, NMS에 걸린 시간을 기록하기 위해 validation이 시작되기 전 아래와 같이 Profile 객체를 만들어줍니다.\n1  dt = Profile(), Profile(), Profile() # profiling times   Profile 클래스는 contextlib.ContextDecorator를 상속받습니다. contextlib.ContextDecorator를 상속받은 child 클래스는 __enter에서 with에 들어올 때 수행할 동작을, __exit에서 with에서 나갈 때 수행할 동작을 정의할 수 있습니다. Profile에서는 들어올 때 시간을 체크하고 나갈 때 한 번 더 시간을 체크하여, with 안에서 수행한 전체 시간을 self.t에 저장합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  class Profile(contextlib.ContextDecorator): # YOLOv5 Profile class. Usage: @Profile() decorator or \u0026#39;with Profile():\u0026#39; context manager def __init__(self, t=0.0): self.t = t self.cuda = torch.cuda.is_available() def __enter__(self): self.start = self.time() return self def __exit__(self, type, value, traceback): self.dt = self.time() - self.start # delta-time self.t += self.dt # accumulate dt def time(self): if self.cuda: torch.cuda.synchronize() return time.time()   validation이 학습 중 호출 된 것이 아닐 경우, validation 후에 아래와 같이 평균 수행 시간을 print합니다.\n1 2 3 4 5  # Print speeds t = tuple(x.t / seen * 1E3 for x in dt) # speeds per image if not training: shape = (batch_size, 3, imgsz, imgsz) LOGGER.info(f\u0026#39;Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {shape}\u0026#39; % t)   Prediction Matrix  NMS를 마친 결과와 label, 계산하고자 하는 IoU 기준으로 process_batch를 통해 각 detection마다 물체 탐지 여부를 표시하는 Prediction Matrix를 만들어줍니다.  1  correct = process_batch(predn, labelsn, iouv)   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  def process_batch(detections, labels, iouv): \u0026#34;\u0026#34;\u0026#34; Return correct prediction matrix Arguments: detections (array[N, 6]), x1, y1, x2, y2, conf, class labels (array[M, 5]), class, x1, y1, x2, y2 Returns: correct (array[N, 10]), for 10 IoU levels \u0026#34;\u0026#34;\u0026#34; correct = np.zeros((detections.shape[0], iouv.shape[0])).astype(bool) iou = box_iou(labels[:, 1:], detections[:, :4]) correct_class = labels[:, 0:1] == detections[:, 5] for i in range(len(iouv)): x = torch.where((iou \u0026gt;= iouv[i]) \u0026amp; correct_class) # IoU \u0026gt; threshold and classes match if x[0].shape[0]: matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy() # [label, detect, iou] if x[0].shape[0] \u0026gt; 1: matches = matches[matches[:, 2].argsort()[::-1]] matches = matches[np.unique(matches[:, 1], return_index=True)[1]] # matches = matches[matches[:, 2].argsort()[::-1]] matches = matches[np.unique(matches[:, 0], return_index=True)[1]] correct[matches[:, 1].astype(int), i] = True return torch.tensor(correct, dtype=torch.bool, device=iouv.device)     iouv는 기준이 되는 IOU threshold 리스트입니다.\n 0.5~0.95 사이의 IoU threshold를 사용합니다. tensor([0.50000, 0.55000, 0.60000, 0.65000, 0.70000, 0.75000, 0.80000, 0.85000, 0.90000, 0.95000], device=\u0026lsquo;cuda:0\u0026rsquo;)    iou\n 실제 GT인 labels와 NMS 마친 결과인 detections 사이의 IoU를 구합니다. labels가 5개, detections가 300개라면 iou의 shape은 (5, 300)이 됩니다.    matches\n detection 결과 중, IoU가 threshold보다 크고, class가 match하는 결과를 뽑습니다. IoU가 큰 순으로 정렬합니다. np.unique를 통해 label, detection결과가 중복된 것을 모두 제거해줍니다. shape은 (matches갯수, 3)인데 여기서 3은 차례로 label index, detections index, IoU입니다. 예를 들면 아래는 IoU threshold와 class로 필터링한 결과입니다. 1 2 3 4  [[ 1, 0, 0.51507], [ 1, 3, 0.78271], [ 1, 11, 0.51642], [ 1, 12, 0.54807]]    아래는 중복까지 제거한 결과입니다. 1  [[ 1, 0, 0.51507]]       correct\n 각 detections에 대해 10개의 IoU threshold 기준을 적용했을 때 object를 찾았는지 여부입니다. detections가 300개일 때, shape은 (300, 10)입니다. matches에서 찾은 detections에 True를 할당하므로, True는 해당 detection 결과와 실제 label의 IoU가 threshold보다 높고, class도 일치함을 의미합니다.    전체 이미지에 대해 process_batch한 결과를 stats에 저장합니다.\n  AP, mAP 계산  ap_per_class를 통해 precision, recall, ap를 계산합니다.  1  tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats, plot=plots, save_dir=save_dir, names=names)    클래스별로 아래 식을 이용하여 10개의 IoU threshold에 대해 Precision과 Recall을 계산합니다.  $Precision = \\frac{TP}{TP+FP} = \\frac{TP}{detection 갯수}$ $Recall = \\frac{TP}{TP+FN} = \\frac{TP}{GT 갯수}$ 위의 process_batch를 계산할 때 np.unique를 통해 중복을 걸러주었기 때문에, Precision과 Recall은 모두 최대 1을 가지게 됩니다.   x 축이 Recall, y축이 Precision인 좌표평면에 각 detection에 대한 결과를 점으로 표시해 줍니다. 이 점을 모두 연결한 그래프를 Precision-Recall Curve라고 합니다. 아래 그림에서 알파벳은 각 이미지를 의미합니다. 출처: https://github.com/rafaelpadilla/Object-Detection-Metrics 아래 그림과 같이 전체 Recall 범위를 일정하게 나눈 뒤, 각 구역마다 Inpterpolated Precision을 구해줍니다. 아래 예시에서는 11개 구역으로 나누었는데 YOLOv5에서는 1000개로 나눕니다. 출처: https://github.com/rafaelpadilla/Object-Detection-Metrics Interpolated Precision을 좌표에 그려 아래와 같이 새로운 Precision-Recall Curve를 그려줍니다. 출처: https://github.com/rafaelpadilla/Object-Detection-Metrics 새로운 Precision-Recall Curve의 아래 면적이(Area Uner Curve, AUC) AP값이 됩니다. 출처: https://github.com/rafaelpadilla/Object-Detection-Metrics 위와 같은 방법으로 ap_per_class를 통해 클래스별 AP를 구한 다음, AP 값의 평균을 구해 Object Detection 성능 평가 지표로 사용하는데, 이를 Mean Average Precision(mAP)라고 합니다.  ","date":"2023-03-29T00:20:00+09:00","permalink":"https://kimberlykang.github.io/p/yolov5_val/","title":"YOLOv5 Validation"},{"content":"본 포스트에서는 YOLOv5에서 사용하는 augmentation 종류에 대해 알아보겠습니다.\n목차  Mosaic Mixup Copy-Paste Random Perspective Albumentation, HSV, 상하좌우 반전  Mosaic  hyperparameter 파일에 있는 mosaic 사용 확률로 샘플링을 하여 mosaic 사용 여부를 결정합니다. mosaic을 사용하지 않으면, 이미지가 정 가운데 들어가고 나머지가 여백으로 채워진 640×640이미지가 만들어집니다. 1 2 3 4 5 6 7 8 9 10 11 12  mosaic = self.mosaic and random.random() \u0026lt; hyp[\u0026#39;mosaic\u0026#39;] if mosaic: # Load mosaic img, labels = self.load_mosaic(index) ... else: # Load image img, (h0, w0), (h, w) = self.load_image(index) # Letterbox shape = self.batch_shapes[self.batch[index]] if self.rect else self.img_size # final letterboxed shape img, ratio, pad = letterbox(img, shape, auto=False, scaleup=self.augment)    이미지 3개를 랜덤으로 선택하여 총 4개의 이미지를 random crop 후 붙여 넣습니다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  def load_mosaic(self, index): # YOLOv5 4-mosaic loader. Loads 1 image + 3 random images into a 4-image mosaic labels4, segments4 = [], [] s = self.img_size yc, xc = (int(random.uniform(-x, 2 * s + x)) for x in self.mosaic_border) # mosaic center x, y indices = [index] + random.choices(self.indices, k=3) # 3 additional image indices random.shuffle(indices) for i, index in enumerate(indices): # Load image img, _, (h, w) = self.load_image(index) # place img in img4 if i == 0: # top left img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8) # base image with 4 tiles x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc # xmin, ymin, xmax, ymax (large image) x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h # xmin, ymin, xmax, ymax (small image) elif i == 1: # top right x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h elif i == 2: # bottom left x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h) x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h) elif i == 3: # bottom right x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h) x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h) img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b] # img4[ymin:ymax, xmin:xmax] padw = x1a - x1b padh = y1a - y1b     Mixup  Mixup은 학습 데이터에서 랜덤으로 뽑은 feature-target 벡터 $ (x_i, y_i) $, $ (x_j, y_j) $가 있을 때, 가상 feature-target 벡터 $ (\\tilde x, \\tilde y) $를 다음과 같이 구합니다. 이 때, λ는 베타 분포에서 샘플링합니다. $$ \\tilde x = \\lambda x_i + (1 - \\lambda) x_j $$ $$ \\tilde y = \\lambda y_i + (1 - \\lambda) y_j $$ mixup: BEYOND EMPIRICAL RISK MINIMIZATION 아래는 λ=0.5로 mixup한 이미지입니다.  yolov5에선 LoadImagesAndLabels 클래스의 __getitem__ 함수에서 아래와 같이 사용됩니다. 1 2 3  # MixUp augmentation if random.random() \u0026lt; hyp[\u0026#39;mixup\u0026#39;]: img, labels = mixup(img, labels, *self.load_mosaic(random.randint(0, self.n - 1)))    yolov5에서 mixup 구현한 코드입니다. 1 2 3 4 5 6  def mixup(im, labels, im2, labels2): # Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf r = np.random.beta(32.0, 32.0) # mixup ratio, alpha=beta=32.0 im = (im * r + im2 * (1 - r)).astype(np.uint8) labels = np.concatenate((labels, labels2), 0) return im, labels     Copy-Paste  랜덤하게 선택한 두 이미지에 random scale, jittering, random horizontal flipping을 적용합니다. 한 이미지에서 random하게 segmentation object를 선택한 뒤, 다른 이미지에 붙여 넣습니다.  출처: Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D. Cubuk, Quoc V. Le, and Barret Zoph. Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 2919, Figure2\n yolov5에선 load_mosaic 함수에서 mosiac을 만든 뒤 아래와 같이 사용됩니다. 여기서 img4는 mosaic 이미지, labels4, segments mosaic 이미지 안에 있는 label 정보, segmentation 정보입니다. 1  img4, labels4, segments4 = copy_paste(img4, labels4, segments4, p=self.hyp[\u0026#39;copy_paste\u0026#39;])    yolov5에서 Copy-Paste 구현한 코드입니다. random scale과 jitter는 다른 곳에서 적용하기 때문에 아래 코드에는 빠져 있습니다. 또한, segmentation 정보가 없으면 Copy-Paste를 적용하지 않습니다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  def copy_paste(im, labels, segments, p=0.5): # Implement Copy-Paste augmentation https://arxiv.org/abs/2012.07177, labels as nx5 np.array(cls, xyxy) n = len(segments) if p and n: h, w, c = im.shape # height, width, channels im_new = np.zeros(im.shape, np.uint8) for j in random.sample(range(n), k=round(p * n)): l, s = labels[j], segments[j] box = w - l[3], l[2], w - l[1], l[4] ioa = bbox_ioa(box, labels[:, 1:5]) # intersection over area if (ioa \u0026lt; 0.30).all(): # allow 30% obscuration of existing labels labels = np.concatenate((labels, [[l[0], *box]]), 0) segments.append(np.concatenate((w - s[:, 0:1], s[:, 1:2]), 1)) cv2.drawContours(im_new, [segments[j].astype(np.int32)], -1, (1, 1, 1), cv2.FILLED) result = cv2.flip(im, 1) # augment segments (flip left-right) i = cv2.flip(im_new, 1).astype(bool) im[i] = result[i] # cv2.imwrite(\u0026#39;debug.jpg\u0026#39;, im) # debug return im, labels, segments     Random Perspective  load_mosaic 함수에서 아래 코드를 통해 적용해줍니다. 1 2 3 4 5 6 7 8 9 10  img4, labels4 = random_perspective(img4, labels4, segments4, degrees=self.hyp[\u0026#39;degrees\u0026#39;], translate=self.hyp[\u0026#39;translate\u0026#39;], scale=self.hyp[\u0026#39;scale\u0026#39;], shear=self.hyp[\u0026#39;shear\u0026#39;], perspective=self.hyp[\u0026#39;perspective\u0026#39;], border=self.mosaic_border) # border to remove    아래는 구현의 일부입니다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  # Center C = np.eye(3) C[0, 2] = -im.shape[1] / 2 # x translation (pixels) C[1, 2] = -im.shape[0] / 2 # y translation (pixels) # Perspective P = np.eye(3) P[2, 0] = random.uniform(-perspective, perspective) # x perspective (about y) P[2, 1] = random.uniform(-perspective, perspective) # y perspective (about x) # Rotation and Scale R = np.eye(3) a = random.uniform(-degrees, degrees) # a += random.choice([-180, -90, 0, 90]) # add 90deg rotations to small rotations s = random.uniform(1 - scale, 1 + scale) # s = 2 ** random.uniform(-scale, scale) R[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s) # Shear S = np.eye(3) S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180) # x shear (deg) S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180) # y shear (deg) # Translation T = np.eye(3) T[0, 2] = random.uniform(0.5 - translate, 0.5 + translate) * width # x translation (pixels) T[1, 2] = random.uniform(0.5 - translate, 0.5 + translate) * height # y translation (pixels) # Combined rotation matrix M = T @ S @ R @ P @ C # order of operations (right to left) is IMPORTANT if (border[0] != 0) or (border[1] != 0) or (M != np.eye(3)).any(): # image changed if perspective: im = cv2.warpPerspective(im, M, dsize=(width, height), borderValue=(114, 114, 114)) else: # affine im = cv2.warpAffine(im, M[:2], dsize=(width, height), borderValue=(114, 114, 114))     Albumentation, HSV, 상하좌우 반전 augment parameter가 True로 설정이 되어 있으면 albumentation, HSV 색 조정, 상하좌우 반전이 적용됩니다.\n albumentations  albumentations은 이미지 augmentation을 해 주는 python 라이브러리입니다. LoadImagesAndLabels 클래스 객체가 만들어질 때 __init __ 에서 아래와 같이 선언이 된 후 사용됩니다.  1  self.albumentations = Albumentations(size=img_size) if augment else None    albumentations가 설치되어 있는 경우에만 적용됩니다. albumentations는 requirements.txt에 comment 처리 되어있습니다. 기본으로는 RandomResizedCrop, Blur, MedianBlur, ToGray, CLAHE를 적용하고 있습니다.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  try: import albumentations as A check_version(A.__version__, \u0026#39;1.0.3\u0026#39;, hard=True) # version requirement T = [ A.RandomResizedCrop(height=size, width=size, scale=(0.8, 1.0), ratio=(0.9, 1.11), p=0.0), A.Blur(p=0.01), A.MedianBlur(p=0.01), A.ToGray(p=0.01), A.CLAHE(p=0.01), A.RandomBrightnessContrast(p=0.0), A.RandomGamma(p=0.0), A.ImageCompression(quality_lower=75, p=0.0)] # transforms self.transform = A.Compose(T, bbox_params=A.BboxParams(format=\u0026#39;yolo\u0026#39;, label_fields=[\u0026#39;class_labels\u0026#39;])) LOGGER.info(prefix + \u0026#39;, \u0026#39;.join(f\u0026#39;{x}\u0026#39;.replace(\u0026#39;always_apply=False, \u0026#39;, \u0026#39;\u0026#39;) for x in T if x.p)) except ImportError: # package not installed, skip    Hue(색상), Saturation(채도), Value(명도) 조정 1 2 3 4 5 6 7 8 9 10 11 12 13 14  def augment_hsv(im, hgain=0.5, sgain=0.5, vgain=0.5): # HSV color-space augmentation if hgain or sgain or vgain: r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1 # random gains hue, sat, val = cv2.split(cv2.cvtColor(im, cv2.COLOR_BGR2HSV)) dtype = im.dtype # uint8 x = np.arange(0, 256, dtype=r.dtype) lut_hue = ((x * r[0]) % 180).astype(dtype) lut_sat = np.clip(x * r[1], 0, 255).astype(dtype) lut_val = np.clip(x * r[2], 0, 255).astype(dtype) im_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val))) cv2.cvtColor(im_hsv, cv2.COLOR_HSV2BGR, dst=im) # no return needed    상하좌우 반전 1 2 3 4 5 6 7 8 9 10 11  # Flip up-down if random.random() \u0026lt; hyp[\u0026#39;flipud\u0026#39;]: img = np.flipud(img) if nl: labels[:, 2] = 1 - labels[:, 2] # Flip left-right if random.random() \u0026lt; hyp[\u0026#39;fliplr\u0026#39;]: img = np.fliplr(img) if nl: labels[:, 1] = 1 - labels[:, 1]     ","date":"2023-03-16T00:20:00+09:00","permalink":"https://kimberlykang.github.io/p/yolov5_augmentation/","title":"YOLOv5 Augmentation"},{"content":"본 포스트에서는 YOLOv5가 hyperparameter를 선정하는 방법인 evolve에 대해 살펴보도록 하겠습니다.\n목차  Evolve Fitness Meta 부모 선정 Mutation Hyperparameter 저장  Evolve YOLOv5는 Genetic Algorithm을 사용하여 hyperparameter를 optimize하는데, 이를 hyperparameter evolution이라고 합니다. train.py에 \u0026ndash;evovle argument를 주어 수행합니다. 아래는 YOLOv5 github에 있는 evolve 사용 예시입니다.\n1 2 3 4 5 6 7 8 9  # Single-GPU python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --evolve # Multi-GPU for i in 0 1 2 3 4 5 6 7; do sleep $(expr 30 \\* $i) \u0026amp;\u0026amp; # 30-second delay (optional) echo \u0026#39;Starting GPU \u0026#39;$i\u0026#39;...\u0026#39; \u0026amp;\u0026amp; nohup python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --device $i --evolve \u0026gt; evolve_gpu_$i.log \u0026amp; done    --evolve 뒤에 generation 횟수를 명시해 주지 않는다면 default로는 300번 hyperparameter를 generation합니다. 300개의 hyperparameter에 대해 10 epoch씩 학습을 한 뒤 가장 fintness가 높은 hyperparameter를 알려줍니다.  Fitness 1 2 3 4  def fitness(x): # Model fitness as a weighted combination of metrics w = [0.0, 0.0, 0.1, 0.9] # weights for [P, R, mAP@0.5, mAP@0.5:0.95] return (x[:, :4] * w).sum(1)    위는 default로 사용되는 fitness 함수입니다. x는 차례로 precision, recall, mAP@0.5, mAP@0.5:0.95 값입니다. 위의 fitness 함수에선 precision과 recall은 사용하지 않고, mAP@0.5 10%, mAP@0.5:0.95 90%로 fitness 점수를 매깁니다. 데이터나, 풀고자 하는 문제의 종류에 따라 Precision, recall, mAP 중 높아야 하는 항목이 다를 수 있습니다. 따라서, 위의 fitness를 그대로 사용하지 않고, 상황에 맞춰 수정하여 사용하는 것이 좋습니다.  Meta Genetic Algorithm으로 hyperparameter를 선정하기 전, meta를 정의하여 mutation을 적용할 정도, 최솟값, 최댓값을 정의해 줍니다. 아래는 train.py에 기본으로 설정되어 있는 meta 값입니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  # Hyperparameter evolution metadata (mutation scale 0-1, lower_limit, upper_limit) meta = { \u0026#39;lr0\u0026#39;: (1, 1e-5, 1e-1), # initial learning rate (SGD=1E-2, Adam=1E-3) \u0026#39;lrf\u0026#39;: (1, 0.01, 1.0), # final OneCycleLR learning rate (lr0 * lrf) \u0026#39;momentum\u0026#39;: (0.3, 0.6, 0.98), # SGD momentum/Adam beta1 \u0026#39;weight_decay\u0026#39;: (1, 0.0, 0.001), # optimizer weight decay \u0026#39;warmup_epochs\u0026#39;: (1, 0.0, 5.0), # warmup epochs (fractions ok) \u0026#39;warmup_momentum\u0026#39;: (1, 0.0, 0.95), # warmup initial momentum \u0026#39;warmup_bias_lr\u0026#39;: (1, 0.0, 0.2), # warmup initial bias lr \u0026#39;box\u0026#39;: (1, 0.02, 0.2), # box loss gain \u0026#39;cls\u0026#39;: (1, 0.2, 4.0), # cls loss gain \u0026#39;cls_pw\u0026#39;: (1, 0.5, 2.0), # cls BCELoss positive_weight \u0026#39;obj\u0026#39;: (1, 0.2, 4.0), # obj loss gain (scale with pixels) \u0026#39;obj_pw\u0026#39;: (1, 0.5, 2.0), # obj BCELoss positive_weight \u0026#39;iou_t\u0026#39;: (0, 0.1, 0.7), # IoU training threshold \u0026#39;anchor_t\u0026#39;: (1, 2.0, 8.0), # anchor-multiple threshold \u0026#39;anchors\u0026#39;: (2, 2.0, 10.0), # anchors per output grid (0 to ignore) \u0026#39;fl_gamma\u0026#39;: (0, 0.0, 2.0), # focal loss gamma (efficientDet default gamma=1.5) \u0026#39;hsv_h\u0026#39;: (1, 0.0, 0.1), # image HSV-Hue augmentation (fraction) \u0026#39;hsv_s\u0026#39;: (1, 0.0, 0.9), # image HSV-Saturation augmentation (fraction) \u0026#39;hsv_v\u0026#39;: (1, 0.0, 0.9), # image HSV-Value augmentation (fraction) \u0026#39;degrees\u0026#39;: (1, 0.0, 45.0), # image rotation (+/- deg) \u0026#39;translate\u0026#39;: (1, 0.0, 0.9), # image translation (+/- fraction) \u0026#39;scale\u0026#39;: (1, 0.0, 0.9), # image scale (+/- gain) \u0026#39;shear\u0026#39;: (1, 0.0, 10.0), # image shear (+/- deg) \u0026#39;perspective\u0026#39;: (0, 0.0, 0.001), # image perspective (+/- fraction), range 0-0.001 \u0026#39;flipud\u0026#39;: (1, 0.0, 1.0), # image flip up-down (probability) \u0026#39;fliplr\u0026#39;: (0, 0.0, 1.0), # image flip left-right (probability) \u0026#39;mosaic\u0026#39;: (1, 0.0, 1.0), # image mixup (probability) \u0026#39;mixup\u0026#39;: (1, 0.0, 1.0), # image mixup (probability) \u0026#39;copy_paste\u0026#39;: (1, 0.0, 1.0)} # segment copy-paste (probability)   부모 선정 1 2 3 4 5 6 7 8 9 10 11  # Select parent(s) parent = \u0026#39;single\u0026#39; # parent selection method: \u0026#39;single\u0026#39; or \u0026#39;weighted\u0026#39; x = np.loadtxt(evolve_csv, ndmin=2, delimiter=\u0026#39;,\u0026#39;, skiprows=1) n = min(5, len(x)) # number of previous results to consider x = x[np.argsort(-fitness(x))][:n] # top n mutations w = fitness(x) - fitness(x).min() + 1E-6 # weights (sum \u0026gt; 0) if parent == \u0026#39;single\u0026#39; or len(x) == 1: # x = x[random.randint(0, n - 1)] # random selection x = x[random.choices(range(n), weights=w)[0]] # weighted selection elif parent == \u0026#39;weighted\u0026#39;: x = (x * w.reshape(n, 1)).sum(0) / w.sum() # weighted combination    fitness가 높은 순으로 5개의 부모를 선택합니다. 각 부모의 fitness를 확률로 사용합니다. single: 확률을 고려하여 부모 하나를 랜덥으로 뽑습니다. weighted: 5개 부모의 평균 값을 사용합니다. 기본으로는 single 방식을 사용합니다.  Mutation 1 2 3 4 5 6 7 8 9 10 11  # Mutate mp, s = 0.8, 0.2 # mutation probability, sigma npr = np.random npr.seed(int(time.time())) g = np.array([meta[k][0] for k in hyp.keys()]) # gains 0-1 ng = len(meta) v = np.ones(ng) while all(v == 1): # mutate until a change occurs (prevent duplicates) v = (g * (npr.random(ng) \u0026lt; mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0) for i, k in enumerate(hyp.keys()): # plt.hist(v.ravel(), 300) hyp[k] = float(x[i + 7] * v[i]) # mutate    g: mutation을 적용할 정도입니다. randn 함수는 평균 0, 분산 1인 normal distribution을 만듭니다. npr.randn(ng)에 s를 곱한 후 1을 더해주면, 평균이 1, 분산이 s인 normal distribution이 만들어집니다. (npr.random(ng) \u0026lt; mp): mutation 적용 여부를 mp의 확률로 결정합니다. 모든 값이 1이던 v array가 랜덤 샘플링을 통해 일부 값이 변하고, 이를 부모 hyperparameter에 곱해줍니다. 아래는 v값의 예시입니다.  [1.003, 1.0048, 1.028, 0.8988, 1, 1, 0.99587, 1.032, 0.99873, 1, 1, 0.92108, 1, 0.99281, 1, 1.0333, 0.91948, 0.9941, 1.0725, 0.95916, 1, 1, 1, 1.0628, 1, 1.0312, 0.95677, 1]    mutation 후 meta에서 정의한 최소, 최댓값으로 걸러줍니다.\n1 2 3 4 5  # Constrain to limits for k, v in meta.items(): hyp[k] = max(hyp[k], v[1]) # lower limit hyp[k] = min(hyp[k], v[2]) # upper limit hyp[k] = round(hyp[k], 5) # significant digits   Hyperparameter 저장 1 2 3 4 5 6 7  # Train mutation results = train(hyp.copy(), opt, device, callbacks) callbacks = Callbacks() # Write mutation results keys = (\u0026#39;metrics/precision\u0026#39;, \u0026#39;metrics/recall\u0026#39;, \u0026#39;metrics/mAP_0.5\u0026#39;, \u0026#39;metrics/mAP_0.5:0.95\u0026#39;, \u0026#39;val/box_loss\u0026#39;, \u0026#39;val/obj_loss\u0026#39;, \u0026#39;val/cls_loss\u0026#39;) print_mutation(keys, results, hyp.copy(), save_dir, opt.bucket)    학습 후, print_mutation을 통해 학습 결과 및 가장 결과가 좋은 hyperparameter를 저장합니다. 아래는 저장된 hyperparameter의 예시입니다.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  # YOLOv5 Hyperparameter Evolution Results# Best generation: 118# Last generation: 164# metrics/precision, metrics/recall, metrics/mAP_0.5, metrics/mAP_0.5:0.95, val/box_loss, val/obj_loss, val/cls_loss# 0.79541, 0.81562, 0.82932, 0.52318, 0.017451, 0.046121, 0.0056865lr0:0.01498lrf:0.01813momentum:0.89668weight_decay:0.0007warmup_epochs:2.9996warmup_momentum:0.95warmup_bias_lr:0.09796box:0.03177cls:0.72138cls_pw:1.0919obj:1.0486obj_pw:1.006iou_t:0.2anchor_t:2.201fl_gamma:0.0hsv_h:0.01734hsv_s:0.55193hsv_v:0.46602degrees:0.0translate:0.08528scale:0.38912shear:0.0perspective:0.0flipud:0.0fliplr:0.5mosaic:0.99787mixup:0.0copy_paste:0.0anchors:2.0  ","date":"2023-02-28T00:20:00+09:00","permalink":"https://kimberlykang.github.io/p/yolov5_evolve/","title":"YOLOv5 Evolve"},{"content":"본 포스트에서는 YOLOv5에서 anchor를 선정하는 방법에 대해서 알아보겠습니다.\n목차  동기 Genetic Algorithm YOLOv5에서 사용된 GA Autoanchor  동기 YOLOv5는 Glenn Jocher가 Github을 통해 발표한 모델입니다. 그는 Yolov5 이전에 Pytorch 버전의 YOLOv3 코드를 작성해서 Github에 공개 한 적이 있습니다. 이 때, 많은 사람들이 custom data set을 학습하며 올린 이슈를 보고 그는 사람들이 학습하고자 하는 dataset의 bounding box 가로 세로 비율이 COCO dataset bounding box의 가로 세로 비율과 많이 다르다는 사실을 발견하게 됩니다. 이 문제를 해결하기 위해, 그는 그 당시에 anchor box를 선정할 때 많이 쓰이던 k-means에 genetic algorithm을 추가적으로 접목시키고, 이 모든 과정을 자동으로 수행해주는 autoanchor를 YOLOv5에 소개합니다. 이에 관한 내용은 아래 링크의 인터뷰에서 더 자세히 확인하실 수 있습니다.\nhttps://www.youtube.com/watch?v=O4jOqVqyAo8\nGenetic Algorithm Genetic Algorithm(GA)는 진화 과정에서 착안한 문제 해결 방법입니다. GA를 이해하기 위해서 필요한 용어에 대해 먼저 살펴보겠습니다.\n Population: 주어진 문제에 대한 해결 방법 집합 Chromosome: 염색체. 유전자로 이루어져 있습니다. 하나의 문제 해결 방법을 의미합니다. Crossover: 부모의 유전자를 사용하여 자손이 생성됩니다. Mutation: 유전자 일부가 변화합니다. 적합도: 해당 문제에 대해 염색체에 해당하는 해결 방식이 얼마나 적합한지를 의미합니다.  GA는 아래의 순서로 수행이 됩니다.\nYOLOv5에서 사용된 GA yolo에서는 위에서 살펴본 GA 과정이 아래와 같이 변형되어 진행됩니다.\n YOLOv5의 population은 bounding box의 가로 세로가 됩니다. 그리고 GA를 통해 가장 적합한 anchor box의 가로 세로 크기를 구하려고 합니다. 부모를 선택할 때에는 k-means를 사용합니다. k개의 centroid(가로, 세로)를 구한 뒤, 각 centroid가 단독 부모가 됩니다. k개의 단독 부모에 대해 crossover는 하지 않고 generation 하고자 하는 수만큼 mutation만 합니다. default로는 각 k개 부모마다 1000개의 (가로, 세로)를 mutation합니다. 재생산한 1000개 (w, h) 사이의 적합도를 계산해서 가장 높은 하나를 선택합니다. 결국 k개의 (w, h)가 선택되는데, 이게 YOLOv5에서 사용하는 anchor box가 됩니다.  Autoanchor dataloader를 만든 후 학습에 들어가기 전, 현재 anchor가 현재 데이터에 적합한 anchor인지 check_anchors를 통해 확인합니다. 이 함수 내부에서 Autoanchor가 수행됩니다.\n1 2 3  if not resume: if not opt.noautoanchor: check_anchors(dataset, model=model, thr=hyp[\u0026#39;anchor_t\u0026#39;], imgsz=imgsz) # run AutoAnchor   현재 anchor가 적합한 anchor인지 판별한는 데에는 아래의 metric이 사용됩니다. 여기서 wh는 전체 bounding box의 [가로, 세로] array이고, k는 현재 anchor의 [가로, 세로] array입니다.\n1 2 3 4 5 6 7  def metric(k): # compute metric r = wh[:, None] / k[None] x = torch.min(r, 1 / r).min(2)[0] # ratio metric best = x.max(1)[0] # best_x aat = (x \u0026gt; 1 / thr).float().sum(1).mean() # anchors above threshold bpr = (best \u0026gt; 1 / thr).float().mean() # best possible recall return bpr, aat     torch.min(r, 1 / r): bounding box의 가로, 세로를 각 anchor box의 가로 세로로 나누어 비율을 구합니다. (bbox 갯수, anchor 갯수, 2)의 shape을 가집니다.\n  x = torch.min(r, 1 / r).min(2)[0]: bounding box의 anchor box에 대한 가로 세로 비율 중 더 작은 것을 선택합니다. 즉 bounding box마다 각 anchor에 대한 비율을 하나씩 갖게 됩니다. (bbox갯수, anchor 갯수)의 shape을 가집니다.\n  best = x.max(1)[0]: bounding box는 가장 비율이 높은 anchor 하나를 선정합니다. best는 가장 비율이 높은 anchor와의 비율 값 array입니다. (bbox 갯수)의 shape을 가집니다.\n  aat = (x \u0026gt; 1 / thr).float().sum(1).mean()\n  bpr = (best \u0026gt; 1 / thr).float().mean()\n  bpr이 0.98보다 작으면 k_mean_ahcnors 함수를 통해 새로운 anchor를 만들어줍니다.\n1 2 3 4 5 6  if bpr \u0026gt; 0.98: # threshold to recompute LOGGER.info(f\u0026#39;{s}Current anchors are a good fit to dataset ✅\u0026#39;) else: LOGGER.info(f\u0026#39;{s}Anchors are a poor fit to dataset ⚠️, attempting to improve...\u0026#39;) na = m.anchors.numel() // 2 # number of anchors anchors = kmean_anchors(dataset, n=na, img_size=imgsz, thr=thr, gen=1000, verbose=False)   kmean_anchors에서는 먼저 k-means로 bounding box에서 k개의 centroid를 구해줍니다.\n1 2 3  from scipy.cluster.vq import kmeans s = wh.std(0) # sigmas for whitening k = kmeans(wh / s, n, iter=30)[0] * s # points   centroid를 구했으면 그 centroid를 사용해서 mutatation을 한 다음 anchor_fitness를 통해 적합도를 계산해 줍니다. 위에서 사용된 것과 동일하게 가로, 세로 비율을 고려한 동일한 metric이 사용됩니다.\n1 2 3 4 5  def metric(k, wh): # compute metrics r = wh[:, None] / k[None] x = torch.min(r, 1 / r).min(2)[0] # ratio metric # x = wh_iou(wh, torch.tensor(k)) # iou metric return x, x.max(1)[0] # x, best_x   mutation은 gen갯수만큼 mutation을 해 주는데 default는 1000개입니다. 1000개의 mutation을 하고 그 중 가장 적합도가 높은 것을 anchor로 반환합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # Evolve f, sh, mp, s = anchor_fitness(k), k.shape, 0.9, 0.1 # fitness, generations, mutation prob, sigma pbar = tqdm(range(gen), bar_format=TQDM_BAR_FORMAT) # progress bar for _ in pbar: v = np.ones(sh) while (v == 1).all(): # mutate until a change occurs (prevent duplicates) v = ((npr.random(sh) \u0026lt; mp) * random.random() * npr.randn(*sh) * s + 1).clip(0.3, 3.0) kg = (k.copy() * v).clip(min=2.0) fg = anchor_fitness(kg) if fg \u0026gt; f: f, k = fg, kg.copy() pbar.desc = f\u0026#39;{PREFIX}Evolving anchors with Genetic Algorithm: fitness = {f:.4f}\u0026#39; if verbose: print_results(k, verbose) return print_results(k).astype(np.float32)   ","date":"2023-02-16T00:20:00+09:00","permalink":"https://kimberlykang.github.io/p/yolov5_anchor/","title":"YOLOv5 anchor"},{"content":"본 포스트에서는 YOLOv5 리파지토리의 train.py 코드를 전체적으로 살펴보도록 하겠습니다.\n목차  Data parellel 학습 조건 프린트 및 저장 Logger torch_distributed_zero_first Dataloader  Data parellel 학습 코드 중간 중간 RANK라는 variable이 나오는데, 이를 이해하기 위해선 data parellel의 몇 가지 용어들을 이해해야 합니다. 코드를 보기 전에 먼저 용어를 간단히 살펴보겠습니다.\n data parellel: 모델 학습 방법 중 하나입니다. 모델 복사본 여러 개를 만들어, 각 복사된 모델에서 동시에 parellel하게 학습이 이루어집니다. 이 때 각 복사된 모델은 각기 다른 데이터를 학습하고, local gradients/sub-gradients를 계산한 뒤, synchronization 작업을 통해 복사본 전체의 gradients를 계산하게 됩니다. node: data parellel 학습을 할 때, 학습이 이루어지는 머신 혹은 서버입니다 각 node에서는 여러 개의 process를 만들어 모델을 복사한 뒤, 복사된 모델에서 학습이 진행됩니다. rank: node에 복사된 각 process의 id입니다. node가 여러 개 있을 때, rank는 local rank와 global rank로 나뉘게 되는데, local rank는 한 node 안에서의 id이고, global rank는 전체 node 안에 있는 모든 process에 id를 매긴 것입니다. world: 전체 node 안의 모든 process를 포함하는 그룹입니다. world 안에 있는 process 갯수를 world size라고 합니다. 보통 모델 복사본 하나가 GPU 하나를 사용하고, 따라서 process 하나를 만들기 때문에 world size는 주로 GPU 갯수인 경우가 많습니다. 하지만 경우에 따라 모델이 큰 경우에 모델 복사본 하나가 GPU 여러 개를 사용하는 경우도 있습니다.  출처: https://github.com/pytorch/examples/blob/main/distributed/ddp/README.md\n위 그림은 두 개 노드에서 학습을 하는 data parellel application입니다. 위 모델은 모델 복사본 하나 당 GPU 두 개를 사용하여 학습하여, 각 node안의 GPU0, GPU1에서 LOCAL RANK0을 가진 process가, GPU2, GPU3에서 LOCAL RANK1을 가진 process가 학습을 진행하게 됩니다. GLOBAL RANK는 node에 상관 없이 전체 process에 대해 0~3까지 할당됩니다.\n학습 조건 프린트 및 저장 main 함수가 시작되면 아래와 같이 현재 args를 프린트하고, 학습에 필요한 것들을 체크하는 코드가 있습니다. 이 때 위에서 살펴본 RANK가 나오는데, RANK가 -1인 경우는 data parellel을 사용하지 않는 경우입니다. 하나의 process에서만 프린트하기 위해 아래와 같이 rank가 0일 경우에만 코드를 실행합니다. check_git_status는 현재 브랜치의 커밋 내역을 확인하여 pull 받을 커밋이 있다면 자동으로 git pull 해 주는 함수입니다. 개인적으로는 코드 관리 및 재현에 용이하게 하기 위해 아래와 같이 코멘트처리 한 뒤 필요할 때마다 수동으로 git pull해서 사용하고 있습니다.\n1 2 3 4 5  # main 함수 if RANK in {-1, 0}: print_args(vars(opt)) # check_git_status() check_requirements()   학습이 시작되면 아래에서 현재 학습에 사용하고 있는 하이퍼파라미터와 학습 관련된 세팅을 저장합니다. 각 파일은 runs/train/exp 폴더에 저장되어 학습이 종료 후 학습에 사용한 설정을 확인할 수 있습니다. exp 폴더는 학습을 새로 할 때마다 exp1 exp2와 같이 자동으로 숫자가 커지며 폴더가 새로 생성됩니다.\n1 2 3 4  # train 함수 if not evolve: yaml_save(save_dir / \u0026#39;hyp.yaml\u0026#39;, hyp) yaml_save(save_dir / \u0026#39;opt.yaml\u0026#39;, vars(opt))   Logger 1 2 3 4 5 6  if RANK in {-1, 0}: loggers = Loggers(save_dir, weights, opt, hyp, LOGGER) # loggers instance # Register actions for k in methods(loggers): callbacks.register_action(k, callback=getattr(loggers, k))   로깅을 한 번만 하기 위해 rank가 0일 경우 Logger 객체를 만듭니다. 그런 다음 loggers의 함수를 각 함수의 이름으로 callback에 등록해 줍니다. 함수를 호출 할 때에는 함수 이름을 이용하여 아래와 같이 호출해줍니다.\n1  callbacks.run(\u0026#39;on_pretrain_routine_end\u0026#39;, labels, names)   예를 들면 위 코드는 학습을 위한 설정 후 실행됩니다. 위 코드가 실행되면 Loggers 클래스의 on_pretrain_routine_end 함수가 실행되어 레이블에 관한 정보를 이미지로 저장합니다.\ntorch_distributed_zero_first 첫 번째 rank가 먼저 수행되어야 하는 코드가 있을 때, yolov5에서는 torch_distributed_zero_first 함수를 만들어 이를 수행하고 있습니다.\n1 2 3 4 5 6 7 8  @contextmanager def torch_distributed_zero_first(local_rank: int): # Decorator to make all processes in distributed training wait for each local_master to do something if local_rank not in [-1, 0]: dist.barrier(device_ids=[local_rank]) yield if local_rank == 0: dist.barrier(device_ids=[0])   torch_distributed_zero_first 함수는 torch의 contextmanager decorator와 barrier 함수를 이용하여 정의되어 있습니다.\n @contextmanager와 함께 정의된 함수는 yield 전에 선언된 코드는 with 코드 블록이 실행되기 전에 수행이 되고, yield 후에 선언된 코드는 with 코드 블록이 실행된 후 수행이 됩니다. barrier 함수는 전체 그룹이 barrier 함수에 도달할 때까지 process를 block합니다. 위의 torch_distributed_zero_first에서는 rank0을 제외한 나머지 process는 yield 전에서 rank0를 기다리게 되고, rank0 혼자 yield를 만나 with 코드 블록을 수행합니다. 후에 rank0이 barrier를 만나게 되면 나머지 process들도 yield를 만나 with 코드 블록을 수행하게 됩니다. 이 과정을 통해 rank0가 항상 with 코드 블록을 다른 process보다 먼저 수행하게 됩니다.  1 2  with torch_distributed_zero_first(LOCAL_RANK): data_dict = data_dict or check_dataset(data)   위는 학습 코드에서 torch_distributed_zero_first가 사용된 예입니다. * rank0을 제외한 나머지 process는 rank0가 check_dataset(data)를 수행 후 barrier를 만날 때까지 기다립니다.\n rank0는 check_dataset를 수행하여 데이터가 없으면 다운받습니다. check_dataset 함수 수행이 끝난 후 rank0는 barrier 를 만납니다. 나머지 process들도 check_dataset를 수행하게 됩니다. 이 때, 데이터는 이미 rank0에서 다운받았기 때문에 다시 다운받지 않습니다. torch_distributed_zero_first 함수를 사용함으로써 다운로드를 한 번만 받게 됩니다.  Optimizer \u0026amp; Learning rate schedule train을 시작할 때 optimizer argument를 주지 않는다면 default인 SGD가 사용됩니다. lr0, momentum, weight_decay는 학습 시작시 지정한 하이퍼파라미터에서 가져옵니다.\n1  optimizer = smart_optimizer(model, opt.optimizer, hyp[\u0026#39;lr0\u0026#39;], hyp[\u0026#39;momentum\u0026#39;], hyp[\u0026#39;weight_decay\u0026#39;])   Learning rate은 default로 Linear learning rate을 사용합니다. 아래와 같이 LambdaLR을 사용하여 구현되어 있습니다.\n1 2 3 4 5  if opt.cos_lr: lf = one_cycle(1, hyp[\u0026#39;lrf\u0026#39;], epochs) # cosine 1-\u0026gt;hyp[\u0026#39;lrf\u0026#39;] else: lf = lambda x: (1 - x / epochs) * (1.0 - hyp[\u0026#39;lrf\u0026#39;]) + hyp[\u0026#39;lrf\u0026#39;] # linear scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)   Dataloader 각 rank마다 dataloader가 생기므로, 전체 batch size를 WORLD_SIZE로 나눈 것이 새로운 batch size가 되어 dataloader를 만들게 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  train_loader, dataset = create_dataloader(train_path, imgsz, batch_size // WORLD_SIZE, gs, single_cls, hyp=hyp, augment=True, cache=None if opt.cache == \u0026#39;val\u0026#39; else opt.cache, rect=opt.rect, rank=LOCAL_RANK, workers=workers, image_weights=opt.image_weights, quad=opt.quad, prefix=colorstr(\u0026#39;train: \u0026#39;), shuffle=True, seed=opt.seed)   현재 YOLOv5에서는 validation은 single-GPU만 지원하고 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13  if RANK in {-1, 0}: val_loader = create_dataloader(val_path, imgsz, batch_size // WORLD_SIZE * 2, gs, single_cls, hyp=hyp, cache=None if noval else opt.cache, rect=True, rank=-1, workers=workers * 2, pad=0.5, prefix=colorstr(\u0026#39;val: \u0026#39;))[0]   ","date":"2023-02-06T00:20:00+09:00","permalink":"https://kimberlykang.github.io/p/yolov5_train/","title":"YOLOv5 학습"}]