<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>GPT1 on Kimberly&#39;s Blog</title>
        <link>https://kimberlykang.github.io/ko/tags/gpt1/</link>
        <description>Recent content in GPT1 on Kimberly&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ko</language>
        <lastBuildDate>Tue, 15 Aug 2023 00:17:41 +0900</lastBuildDate><atom:link href="https://kimberlykang.github.io/ko/tags/gpt1/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>GPT1, 2, 3 리뷰</title>
        <link>https://kimberlykang.github.io/ko/p/gpt_review/</link>
        <pubDate>Tue, 15 Aug 2023 00:17:41 +0900</pubDate>
        
        <guid>https://kimberlykang.github.io/ko/p/gpt_review/</guid>
        <description>&lt;p&gt;본 포스트에서는 OpenAI가 발표한 GPT1, GPT2, GPT3의 모델 구조, 학습 방법, 주요 개선점 등에 대해 살펴보겠습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT1: Improving Language Understanding by Generative Pre-Training (&lt;a class=&#34;link&#34; href=&#34;https://www.mikecaptain.com/resources/pdf/GPT-1.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.mikecaptain.com/resources/pdf/GPT-1.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;GPT2: Language Models are Unsupervised Multitask Learners (&lt;a class=&#34;link&#34; href=&#34;https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;GPT3: Language Models are Few-Shot Learners (&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2005.14165&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/2005.14165&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;목차&#34;&gt;목차
&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#gpt1&#34; &gt;GPT1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#gpt2&#34; &gt;GPT2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#gpt3&#34; &gt;GPT3&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;gpt1&#34;&gt;GPT1
&lt;/h2&gt;&lt;p&gt;GPT1은 OpenAI가 발표한 최초의 Generative Pre-trained Transformer 모델로, 대규모 unsupervised pre-training과 소규모 supervised fine-tuning을 결합한 언어 모델입니다.&lt;/p&gt;
&lt;h3 id=&#34;pre-training&#34;&gt;Pre-Training
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Unsupervised으로 학습합니다.&lt;/li&gt;
&lt;li&gt;추후 진행될 Supervised fine-tuning의 좋은 initialization point가 됩니다.&lt;/li&gt;
&lt;li&gt;아래 likelihood를 최대화를 최대화합니다.
&lt;ul&gt;
&lt;li&gt;$ L_1(\mathcal{U}) = \displaystyle{\sum_{i}} log P(u_i|u_{i-k}, &amp;hellip;, u_{i-1} ; \Theta)$
&lt;ul&gt;
&lt;li&gt;$ \mathcal U= u_1, &amp;hellip;, u_n $: unsupervised corpus의 token&lt;/li&gt;
&lt;li&gt;k: context window의 사이즈 (512)&lt;/li&gt;
&lt;li&gt;Θ: neural network parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Language model로는 multi-layer Transformer decoder을 사용합니다.
&lt;ul&gt;
&lt;li&gt;$ h_0 = U W_e + W_p \\ h_l = \mathrm{transformer \_ block}(h_{l-1}) \forall i \in [1, n] \\ P(u) = \mathrm{softmax}(h_n W_e^T)$
&lt;ul&gt;
&lt;li&gt;$ U=(u_{-k}, &amp;hellip;, u_{-1}) $&lt;/li&gt;
&lt;li&gt;$n$: layer 갯수&lt;/li&gt;
&lt;li&gt;$W_e$ : token embedding matrix&lt;/li&gt;
&lt;li&gt;$W_p$ : position embedding matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;dataset: BooksCorpus
&lt;ul&gt;
&lt;li&gt;ftfy library를 사용하여 잘못된 unicode text를 복구하는 등의 정제 작업을 했습니다.&lt;/li&gt;
&lt;li&gt;punctuation과 공백을 통일했습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;fine-tuning&#34;&gt;Fine-Tuning
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Model: target task를 수행하도록 아래 모델을 학습합니다.
&lt;ul&gt;
&lt;li&gt;$ P(y|x^1, &amp;hellip;, x^m) = \mathrm{softmax}(h_l^m W_y) $
&lt;ul&gt;
&lt;li&gt;$y$ : label&lt;/li&gt;
&lt;li&gt;$ x^1, &amp;hellip;, x^m $  : input token&lt;/li&gt;
&lt;li&gt;$ h^m_l $: 마지막 transformer block의 activation&lt;/li&gt;
&lt;li&gt;$W_y$ : 추가된 linear layer의 parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Supervised Objective: 아래 Objective를 maximize하도록 학습합니다.
&lt;ul&gt;
&lt;li&gt;$ L_2(\mathcal C) = \displaystyle{\sum_{(x, y)}} log P(y|x^1, &amp;hellip;, x^m) $
&lt;ul&gt;
&lt;li&gt;$ \mathcal C $: labeled dataset&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;전체 Objective는 아래의 Auxiliary Objective를 사용합니다.
&lt;ul&gt;
&lt;li&gt;$ L_3(\mathcal C) = L_2(\mathcal C) + \lambda * L_1(\mathcal C)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;task-specific-input-transformations&#34;&gt;Task-specific input transformations
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/gpt_review/gpt1_different_tasks.png&#34;
	width=&#34;1026&#34;
	height=&#34;421&#34;
	srcset=&#34;https://kimberlykang.github.io/p/gpt_review/gpt1_different_tasks_hu625cb35c2430c17a622fe69b8481cc62_119645_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/gpt_review/gpt1_different_tasks_hu625cb35c2430c17a622fe69b8481cc62_119645_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;gpt1_different_tasks&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;243&#34;
		data-flex-basis=&#34;584px&#34;
	
&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://www.mikecaptain.com/resources/pdf/GPT-1.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Radford, Alec, et al. &amp;ldquo;Improving language understanding by generative pre-training.&amp;rdquo; (2018), Figure 1.&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;traversal-style
&lt;ul&gt;
&lt;li&gt;(premise, hypothesis), (context, question, answer)처럼 여러 문장이나 항목이 따로따로 있는 input을 그대로 사용하면, 사전학습 형태와 일치하지 않기 때문에 모델이 이해하기가 어렵습니다.&lt;/li&gt;
&lt;li&gt;각 태스크별로 입력을 특정한 순서대로 나열된 하나의 sequence로 만들어 학습해 줍니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;대부분의 task에서 3 epoch 학습했습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;model-구조&#34;&gt;model 구조
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;decoder-only transformer&lt;/li&gt;
&lt;li&gt;masked self-attention: 768 dimension, 12개 attention head&lt;/li&gt;
&lt;li&gt;position-wise feed-forward network: 3072 dimension&lt;/li&gt;
&lt;li&gt;Adam optimization&lt;/li&gt;
&lt;li&gt;batch size: 64&lt;/li&gt;
&lt;li&gt;512개 연속된 token을 학습합니다.&lt;/li&gt;
&lt;li&gt;Layernorm, 간단한 weight initialization N(0, 0.02)&lt;/li&gt;
&lt;li&gt;BytePair Encoding(BPE)&lt;/li&gt;
&lt;li&gt;dropout: 0.01&lt;/li&gt;
&lt;li&gt;수정된 L2 regularization을 사용합니다.&lt;/li&gt;
&lt;li&gt;Activation: GELU&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gpt2&#34;&gt;GPT2
&lt;/h2&gt;&lt;p&gt;GPT2는 대규모 웹 데이터셋(WebText)과 보다 깊고 커진 모델을 도입하며 별도의 Fine-tuning 없이 Pre-training 모델로만 높은 성능을 보여주었습니다. zero-shot 성능으로 주목을 받았습니다.&lt;/p&gt;
&lt;h3 id=&#34;학습&#34;&gt;학습
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;dataset: 웹에서 수집한 데이터인 WebText로 학습했습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reddit에서 데이터를 수집할 때에는 karma 3개 이상 받은 link 수집헸습니다.&lt;/li&gt;
&lt;li&gt;8 million개 문서로 총 40GB가 됩니다.&lt;/li&gt;
&lt;li&gt;de-duplication을 거쳤습니다.&lt;/li&gt;
&lt;li&gt;WebText의 source인 경우가 많기 때문에 Wikipedia 데이터는 제외했습니다.&lt;/li&gt;
&lt;li&gt;Byte Pair Encoding(BPE)를 수정해서 사용했습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Layer normalization이 input 위치로 변경되고, 마지막 self-attention block 이후에 추가되었습니다.&lt;/li&gt;
&lt;li&gt;residual layer 갯수가 N이라고 할 때, initialization에서 $ 1/\root{N} $ 을 곱해주었습니다.&lt;/li&gt;
&lt;li&gt;수정된 byte-level BPE를 사용합니다.&lt;/li&gt;
&lt;li&gt;vocabulary 50257로 확대했습니다.&lt;/li&gt;
&lt;li&gt;context size를 512에서 1024로 확대했습니다.&lt;/li&gt;
&lt;li&gt;batch size: 512로 GPT1보다 확대되었습니다.&lt;/li&gt;
&lt;li&gt;model 별 parameter 수는 아래와 같습니다.&lt;br&gt;
&lt;img src=&#34;https://kimberlykang.github.io/p/gpt_review/gpt2_model_size.png&#34;
	width=&#34;313&#34;
	height=&#34;156&#34;
	srcset=&#34;https://kimberlykang.github.io/p/gpt_review/gpt2_model_size_hue1f62bcf9fa5634ec4b507dc49f7b36e_12339_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/gpt_review/gpt2_model_size_hue1f62bcf9fa5634ec4b507dc49f7b36e_12339_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;gpt2_model_size&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;200&#34;
		data-flex-basis=&#34;481px&#34;
	
&gt;&lt;br&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Radford, Alec, et al. &amp;ldquo;Language models are unsupervised multitask learners.&amp;rdquo; OpenAI blog 1.8 (2019): 9, Table 2.&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;results&#34;&gt;results
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;zero-shot 성능이 8개의 benchmark 중 7개에서 SOTA를 달성했습니다.
&lt;img src=&#34;https://kimberlykang.github.io/p/gpt_review/gpt2_results.png&#34;
	width=&#34;1182&#34;
	height=&#34;236&#34;
	srcset=&#34;https://kimberlykang.github.io/p/gpt_review/gpt2_results_hue2fd8463b5018ae41dacdf91cd4091d3_56442_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/gpt_review/gpt2_results_hue2fd8463b5018ae41dacdf91cd4091d3_56442_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;gpt2_results&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;500&#34;
		data-flex-basis=&#34;1202px&#34;
	
&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Radford, Alec, et al. &amp;ldquo;Language models are unsupervised multitask learners.&amp;rdquo; OpenAI blog 1.8 (2019): 9, Table 3.&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gpt3&#34;&gt;GPT3
&lt;/h2&gt;&lt;p&gt;GPT3는 트랜스포머 기반 모델 규모를 수십억에서 수천억 파라미터 수준으로 확장했습니다. 방대한 크기의 데이터와 파라미터를 바탕으로 별도의 fine-tuning 없이 프롬프트에 예제만으로 다양한 작업을 수행하는 Few-shot, Zero-shot 학습 능력을 보여주었습니다.&lt;/p&gt;
&lt;h3 id=&#34;zero-shot-one-shot-few-shot&#34;&gt;Zero-shot, One-shot, Few-shot
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;별도의 fine-tuning 없이, 프롬프트만으로 다양한 작업을 수행할 수 있습니다.
&lt;img src=&#34;https://kimberlykang.github.io/p/gpt_review/gpt3_context_learning.png&#34;
	width=&#34;1161&#34;
	height=&#34;1022&#34;
	srcset=&#34;https://kimberlykang.github.io/p/gpt_review/gpt3_context_learning_hu677e469942d8b73027e071bed8b2e023_212580_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/gpt_review/gpt3_context_learning_hu677e469942d8b73027e071bed8b2e023_212580_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;gpt3_context_learning&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;113&#34;
		data-flex-basis=&#34;272px&#34;
	
&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2005.14165&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Brown, Tom, et al. &amp;ldquo;Language models are few-shot learners.&amp;rdquo; arXiv preprint arXiv:2005.14165 (2020): 4, Figure 2.1.&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;학습-1&#34;&gt;학습
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Dataset: CommonCrawl&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;high-quality corpora와 similarity 계산하여 필터링하여 사용합니다.&lt;/li&gt;
&lt;li&gt;필터링 전 45TB 데이터가, 필터링 후 570GB가 남았습니다.&lt;/li&gt;
&lt;li&gt;document level에서 fuzzy deduplication을 수행했습니다.&lt;/li&gt;
&lt;li&gt;diversity 주기 위해 잘 알려진 고품질 데이터(확장된 WebText, books corpora Books1, Books2, 영어 wwikipedia)를 추가하고, 학습 때, 고품질 데이터가 더 자주 smapling 되도록 설정했습니다.
&lt;img src=&#34;https://kimberlykang.github.io/p/gpt_review/gpt3_datasets.png&#34;
	width=&#34;1048&#34;
	height=&#34;277&#34;
	srcset=&#34;https://kimberlykang.github.io/p/gpt_review/gpt3_datasets_hu440b2680c6f21d65f6017e90b1263950_74530_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/gpt_review/gpt3_datasets_hu440b2680c6f21d65f6017e90b1263950_74530_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;gpt3_datasets&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;378&#34;
		data-flex-basis=&#34;908px&#34;
	
&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2005.14165&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Brown, Tom, et al. &amp;ldquo;Language models are few-shot learners.&amp;rdquo; arXiv preprint arXiv:2005.14165 (2020): 4, Table 2.2.&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;model 별 parameter 수는 아래와 같습니다.&lt;br&gt;
&lt;img src=&#34;https://kimberlykang.github.io/p/gpt_review/gpt3_model_size.png&#34;
	width=&#34;1310&#34;
	height=&#34;355&#34;
	srcset=&#34;https://kimberlykang.github.io/p/gpt_review/gpt3_model_size_hu7d35ca2d3c1946aeb193a736972c2df0_130862_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/gpt_review/gpt3_model_size_hu7d35ca2d3c1946aeb193a736972c2df0_130862_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;gpt3_model_size&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;369&#34;
		data-flex-basis=&#34;885px&#34;
	
&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2005.14165&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Brown, Tom, et al. &amp;ldquo;Language models are few-shot learners.&amp;rdquo; arXiv preprint arXiv:2005.14165 (2020): 4, Table 2.1.&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Model 구조는 GPT2와 유사하고, Sparse Transformer를 사용했습니다.&lt;/li&gt;
&lt;li&gt;Adam optimization.&lt;/li&gt;
&lt;li&gt;cosine decay.&lt;/li&gt;
&lt;li&gt;linear LR warmup.&lt;/li&gt;
&lt;li&gt;weight decay.&lt;/li&gt;
&lt;li&gt;batch size를 32k token에서 4-12 billion toekn으로 늘렸습니다.&lt;/li&gt;
&lt;li&gt;replacement 없이 데이터 sampling하고, 다른 문서로 이어질 때에는 end of text token를 사용했습니다.&lt;/li&gt;
&lt;li&gt;context window $n_{ctx}$=2048로 GPT2보다 확대되었습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;학습 데이터에서  k개 예시를 샘플링해서 \n로 연결했습니다. k: 10~100&lt;/li&gt;
&lt;li&gt;dev set에서 가장 결과 좋은 K를 선정하여 test set에 적용하여 최종 결과를 확인했습니다.
&lt;img src=&#34;https://kimberlykang.github.io/p/gpt_review/gpt3_results_translation.png&#34;
	width=&#34;1088&#34;
	height=&#34;717&#34;
	srcset=&#34;https://kimberlykang.github.io/p/gpt_review/gpt3_results_translation_huca06fc24c75695979961eca8e55aa3a0_194526_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/gpt_review/gpt3_results_translation_huca06fc24c75695979961eca8e55aa3a0_194526_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;gpt3_results_translation&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;151&#34;
		data-flex-basis=&#34;364px&#34;
	
&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2005.14165&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Brown, Tom, et al. &amp;ldquo;Language models are few-shot learners.&amp;rdquo; arXiv preprint arXiv:2005.14165 (2020): 4, Figure 3.4.&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;
&lt;img src=&#34;https://kimberlykang.github.io/p/gpt_review/gpt3_results_winogrande.png&#34;
	width=&#34;1070&#34;
	height=&#34;691&#34;
	srcset=&#34;https://kimberlykang.github.io/p/gpt_review/gpt3_results_winogrande_hu8e439faf224b54397cefc3ffb7907cd6_152485_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/gpt_review/gpt3_results_winogrande_hu8e439faf224b54397cefc3ffb7907cd6_152485_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;gpt3_results_winogrande&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;154&#34;
		data-flex-basis=&#34;371px&#34;
	
&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2005.14165&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Brown, Tom, et al. &amp;ldquo;Language models are few-shot learners.&amp;rdquo; arXiv preprint arXiv:2005.14165 (2020): 4, Figure 3.5.&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;
&lt;img src=&#34;https://kimberlykang.github.io/p/gpt_review/gpt3_results_anli.png&#34;
	width=&#34;1064&#34;
	height=&#34;685&#34;
	srcset=&#34;https://kimberlykang.github.io/p/gpt_review/gpt3_results_anli_hu58f4c8db94617a0e14640ed52d754665_156893_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/gpt_review/gpt3_results_anli_hu58f4c8db94617a0e14640ed52d754665_156893_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;gpt3_results_anli&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;155&#34;
		data-flex-basis=&#34;372px&#34;
	
&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2005.14165&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Brown, Tom, et al. &amp;ldquo;Language models are few-shot learners.&amp;rdquo; arXiv preprint arXiv:2005.14165 (2020): 4, Figure 3.9.&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;
&lt;img src=&#34;https://kimberlykang.github.io/p/gpt_review/gpt3_result_examples_parameters.png&#34;
	width=&#34;1548&#34;
	height=&#34;853&#34;
	srcset=&#34;https://kimberlykang.github.io/p/gpt_review/gpt3_result_examples_parameters_hu424a2342ceff2d88e30d3cc0c9b5f282_172453_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/gpt_review/gpt3_result_examples_parameters_hu424a2342ceff2d88e30d3cc0c9b5f282_172453_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;gpt3_result_examples_parameters&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;181&#34;
		data-flex-basis=&#34;435px&#34;
	
&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2005.14165&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Brown, Tom, et al. &amp;ldquo;Language models are few-shot learners.&amp;rdquo; arXiv preprint arXiv:2005.14165 (2020): 4, Figure 1.2.&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;
&lt;img src=&#34;https://kimberlykang.github.io/p/gpt_review/gpt3_aggregate_performance.png&#34;
	width=&#34;1350&#34;
	height=&#34;844&#34;
	srcset=&#34;https://kimberlykang.github.io/p/gpt_review/gpt3_aggregate_performance_hudc5a382bc7e995479e82dc2ff38c451e_662714_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/gpt_review/gpt3_aggregate_performance_hudc5a382bc7e995479e82dc2ff38c451e_662714_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;gpt3_aggregate_performance&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;159&#34;
		data-flex-basis=&#34;383px&#34;
	
&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2005.14165&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Brown, Tom, et al. &amp;ldquo;Language models are few-shot learners.&amp;rdquo; arXiv preprint arXiv:2005.14165 (2020): 4, Figure 1.3.&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
