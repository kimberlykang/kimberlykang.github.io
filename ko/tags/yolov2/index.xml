<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>YOLOv2 on Kimberly&#39;s Blog</title>
        <link>https://kimberlykang.github.io/ko/tags/yolov2/</link>
        <description>Recent content in YOLOv2 on Kimberly&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ko</language>
        <lastBuildDate>Sun, 18 Jun 2023 00:09:40 +0900</lastBuildDate><atom:link href="https://kimberlykang.github.io/ko/tags/yolov2/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>YOLOv2 리뷰</title>
        <link>https://kimberlykang.github.io/ko/p/yolov2_review/</link>
        <pubDate>Sun, 18 Jun 2023 00:09:40 +0900</pubDate>
        
        <guid>https://kimberlykang.github.io/ko/p/yolov2_review/</guid>
        <description>&lt;p&gt;본 포스트에서는 2017년 CVPR에 발표된 논문 &amp;ldquo;YOLO9000: Better, Faster, Stronger&amp;quot;을 살펴보겠습니다.&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;YOLO9000: Better, Faster, Stronger&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;목차&#34;&gt;목차
&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%eb%8f%99%ea%b8%b0&#34; &gt;동기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#batch-normalization&#34; &gt;Batch Normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%eb%86%92%ec%9d%80-resolution%ec%97%90%ec%84%9c-classifier-%ed%95%99%ec%8a%b5&#34; &gt;높은 Resolution에서 classifier 학습&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#anchor-box-%ec%82%ac%ec%9a%a9&#34; &gt;Anchor Box 사용&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#dimension-clusters&#34; &gt;Dimension Clusters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%ec%98%88%ec%b8%a1%ea%b0%92&#34; &gt;예측값&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#fine-grained-features&#34; &gt;Fine-Grained Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%eb%84%a4%ed%8a%b8%ec%9b%8c%ed%81%ac&#34; &gt;네트워크&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%ea%b2%b0%ea%b3%bc&#34; &gt;결과&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;동기&#34;&gt;동기
&lt;/h2&gt;&lt;p&gt;YOLOv1는 그 당시의 State-of-the-art(SOTA)였던 Fast-RCNN과 비교하여 localization error가 상당히 컸습니다. 또한, recall도 상대적으로 굉장히 낮았습니다. 즉, object를 빼먹고 찾은 경우가 많았습니다. YOLOv2에서는 네트워크 크기를 키우지 않고 빠른 속도를 유지하면서 localization error와 낮은 recall 문제를 해결한 방법들을 아래와 같이 고안하게 됩니다.&lt;/p&gt;
&lt;h2 id=&#34;batch-normalization&#34;&gt;Batch Normalization
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;http://proceedings.mlr.press/v37/ioffe15.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;covariate shift를 해결하기 위해 고안된 방법입니다. 이를 위해 한 layer에서 다음 layer로 넘어가기 전에, mini-batch 안의 데이터를 그 mini-batch의 평균과 분산으로 normalize 해 줍니다.&lt;/li&gt;
&lt;li&gt;2015년 Proceedings of Machine Learning Research(PMLR)에 발표된 이후로 지금까지 활발하게 사용되고 있습니다.&lt;/li&gt;
&lt;li&gt;YOLOv2에서는 Batch Normalization을 사용 하여 mAP가 2% 상승했습니다.&lt;/li&gt;
&lt;li&gt;또한 Batch Normalization이 regularization 역할을 해 주어서 네트워크에서 dropout 을 제거하여도 overfitting이 발생하지 않았습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;높은-resolution에서-classifier-학습&#34;&gt;높은 Resolution에서 classifier 학습
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;object detection 모델들은 보통 ImageNet 데이터셋으로 classifier를 학습한 뒤, 학습한 네트워크를 object detection을 하도록 transfer learning을 해서 사용합니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;YOLOv1도 이 방법을 사용했는데, classification 학습에는 224×224 크기의 이미지를 사용했고, detection 학습에는 448×448 크기의 이미지를 사용했습니다. 그럼 네트워크는 object detection을 학습 할 때 object detectino 뿐만 아니라 달라진 input 해상도도 학습을 해야 하게 됩니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;위와 같은 현상을 방지하기 위해 YOLOv2는 object detection 학습 전 &lt;strong&gt;448×448&lt;/strong&gt; 사이즈의 ImageNet 데이터에 대해 &lt;strong&gt;classification&lt;/strong&gt; 학습을 &lt;strong&gt;10 epoch&lt;/strong&gt; 수행했습니다. 그 결과 mAP가 4%가 증가했습니다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;anchor-box-사용&#34;&gt;Anchor Box 사용
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;YOLOv1은 feature extractor에 Fully connected layer를 붙여서 bbox 좌표를 바로 예측했습니다.&lt;/li&gt;
&lt;li&gt;YOLOv2는 Fully connected layer를 제거하고, anchor box를 사용하여 offset을 학습했습니다. 그리고 각 anchor마다 class와 objectness를 예측했습니다.&lt;/li&gt;
&lt;li&gt;anchor box 사용 결과 mAP는 69.5에서 69.2로 살짝 떨어졌지만, recall은 81%에서 88%로 상승했습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;dimension-clusters&#34;&gt;Dimension Clusters
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;YOLOv2에서는 anchor box를 직접 고르는 것 대신에 training set에 있는 bounding box에 k-means clustering을 수행해 anchor box를 골랐습니다. 거리를 구하는 metric으로 Euclidian distance를 사용하면 anchor box가 클 수록 error가 더 커지기 때문에, 이를 방지하기 위해 거리 metric으로 아래를 사용했습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ d(box,centroid)=1-IOU(box, centroid) $&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;아래는 다양한 k값에 대하여 학습 데이터에 있는 bounding box와 가장 가까운 centroid와의 IOU 값의 평균을 그린 그래프와, k=5일 때의 anchor box입니다.&lt;br&gt;
&lt;img src=&#34;https://kimberlykang.github.io/p/yolov2_review/figure2.png&#34;
	width=&#34;405&#34;
	height=&#34;205&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov2_review/figure2_hu84c5c7f5411da2afaeabbc60e0c8b938_14216_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov2_review/figure2_hu84c5c7f5411da2afaeabbc60e0c8b938_14216_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;197&#34;
		data-flex-basis=&#34;474px&#34;
	
&gt;&lt;br&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Redmon, Joseph, and Ali Farhadi. &amp;ldquo;YOLO9000: better, faster, stronger.&amp;rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, Figure 2&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;k가 커질수록 모델은 더 복잡해지지만 성능은 더 좋아지는데, 두 가지를 고려해 논문에서는 k=5를 선택했습니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;k=5일 때 k-means clustering이 찾은 VOC 2007 데이터의 anchor box가 위 그림의 흰 색 박스, COCO 데이터의 anchor box가 위 그림의 파란색 박스입니다. 데이터에 따라 다른 모양의 anchor box가 선택되는 걸 볼 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/yolov2_review/table1.png&#34;
	width=&#34;272&#34;
	height=&#34;102&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov2_review/table1_hu9f6c5b9270229f8ac689cde3b91a7139_11755_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov2_review/table1_hu9f6c5b9270229f8ac689cde3b91a7139_11755_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;266&#34;
		data-flex-basis=&#34;640px&#34;
	
&gt;&lt;br&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Redmon, Joseph, and Ali Farhadi. &amp;ldquo;YOLO9000: better, faster, stronger.&amp;rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, Table 1&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;위 표는 다양한 방법으로 anchor box를 생성했을 때 VOX 2007 데이터의 bounding box와 가장 가까운 anchor box와의 평균 IOU입니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cluster SSE는 거리 metric으로 Sum of Squared Error(Euclidean distance)를 사용한 k-means, Cluster IOU는 거리 metric으로 위에 나온 IOU를 이용한 식을 사용한 k-means, Anchor Boxes는 Anchor가 처음 소개된 논문 &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1506.01497.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[15]&lt;/a&gt;에서 사용한 anchor box입니다. [15]에서는 직접 뽑은 9개의 anchor를 사용했습니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;위 표에서 평균 IOU 값을 살펴보면, k-means를 사용 시 거리 metric으로 SSE를 사용했을 때보다 IOU를 사용했을 때 IOU 값이 높았습니다. 또한 직접 뽑은 Achorbox를 사용했을 때보다 IOU 사용 k-means를 사용했을 때가 평균 IOU 값이 높았습니다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;예측값&#34;&gt;예측값
&lt;/h2&gt;&lt;p&gt;YOLOv2가 예측 값을 알아보기 전에 먼저 예측에 사용된 용어를 살펴보겠습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ (c_x, c_y) $: 셀의 좌 상단 꼭지점 좌표입니다.&lt;/li&gt;
&lt;li&gt;$ (b_x, b_y) $: bounding box 중심의 x, y 좌표입니다.&lt;/li&gt;
&lt;li&gt;$ (b_w, b_h) $: bounding box의 가로, 세로입니다.&lt;/li&gt;
&lt;li&gt;$ (p_w, p_h) $: box prior(anchor)의 가로, 세로입니다.&lt;/li&gt;
&lt;li&gt;$ (t_x, t_y, t_w, t_h, t_o) $: YOLOv2의 네트워크의 output 값입니다. target의 t를 사용합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;YOLOv2가 예측하는 bounding box의 중심 x, y좌표, bounding box의 가로, 세로, confindence score는 아래와 같습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ b_x=\sigma(t_x) + c_x $&lt;/li&gt;
&lt;li&gt;$ b_y=\sigma(t_y) + c_y $&lt;/li&gt;
&lt;li&gt;$ b_w=p_w e^{t_w}$&lt;/li&gt;
&lt;li&gt;$ b_h=p_h e^{t_h}$&lt;/li&gt;
&lt;li&gt;$ Pr(object) * IOU(b, object)=\sigma(t_o)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/yolov2_review/figure3.png&#34;
	width=&#34;402&#34;
	height=&#34;301&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov2_review/figure3_hu0e3241b5964dc481420d4fecde7e67db_16677_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov2_review/figure3_hu0e3241b5964dc481420d4fecde7e67db_16677_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 3&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;br&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Redmon, Joseph, and Ali Farhadi. &amp;ldquo;YOLO9000: better, faster, stronger.&amp;rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, Figure 3&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bounding box의 center 좌표를 예측할 때, logistic activation($\sigma$)를 사용하여 예측값이 0과 1 사이 값이 되어 셀 안에서의 상대 좌표를 나타내게 됩니다. bounding box의 가로, 세로를 예측할 때는 prior로 사용되는 anchor box의 가로, 세로에 예측값을 곱합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;fine-grained-features&#34;&gt;Fine-Grained Features
&lt;/h2&gt;&lt;p&gt;passthrough layer를 추가하여, 크기가 다른 feature map을 detection에 사용할 수 있도록 하였습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;detection에 사용되는 13×13 feature map과 바로 전 26×26 feature map을 passthrough layer를 통해 연결했습니다.&lt;/li&gt;
&lt;li&gt;(26×26×512) 크기의 feature map을 (13×13×2048)로 reshape한 뒤, 13×13 feature map에 concatenate해서 사용했습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;네트워크&#34;&gt;네트워크
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;VGG-16을 base feature extractor로 사용했습니다. VGG-16은 한 이미지에 대한 결과를 계산할 때 30.69 billion floating point operation을 합니다.&lt;/li&gt;
&lt;li&gt;본 논문에서는 19개의 convolutional layer를 가진 Darknet 19를 사용했습니다. Darknet 19는 5.58 billion operation을 합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;결과&#34;&gt;결과
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/yolov2_review/table2.png&#34;
	width=&#34;592&#34;
	height=&#34;236&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov2_review/table2_hu863f8ddaabe1d225215e816998023d61_32710_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov2_review/table2_hu863f8ddaabe1d225215e816998023d61_32710_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 4&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;250&#34;
		data-flex-basis=&#34;602px&#34;
	
&gt;&lt;br&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Redmon, Joseph, and Ali Farhadi. &amp;ldquo;YOLO9000: better, faster, stronger.&amp;rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, Table 2&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;위 표는 &lt;strong&gt;VOC 2007&lt;/strong&gt; 데이터에 대해 모델 종류별로 실험하여 얻은 mAP를 나타낸 표입니다.&lt;/li&gt;
&lt;li&gt;YOLOv1보다 mAP가 상당히 향상된 것을 볼 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/yolov2_review/table3.png&#34;
	width=&#34;410&#34;
	height=&#34;240&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov2_review/table3_huf4e0ec058b81074a4932a215457ea15d_32511_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov2_review/table3_huf4e0ec058b81074a4932a215457ea15d_32511_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 5&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;170&#34;
		data-flex-basis=&#34;410px&#34;
	
&gt;&lt;br&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Redmon, Joseph, and Ali Farhadi. &amp;ldquo;YOLO9000: better, faster, stronger.&amp;rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, Table 3&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/yolov2_review/figure4.png&#34;
	width=&#34;389&#34;
	height=&#34;271&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov2_review/figure4_hu4d13b03f74c29d219d016509dad2c9ec_17592_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov2_review/figure4_hu4d13b03f74c29d219d016509dad2c9ec_17592_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 6&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;143&#34;
		data-flex-basis=&#34;344px&#34;
	
&gt;&lt;br&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Redmon, Joseph, and Ali Farhadi. &amp;ldquo;YOLO9000: better, faster, stronger.&amp;rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, Figure 4&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;위 표와 그림은 &lt;strong&gt;VOC 2007&lt;/strong&gt; 데이터에 대해 여러가지 모델로 실험한 mAP와 FPS입니다.&lt;/li&gt;
&lt;li&gt;각 YOLOv2 228×228, 352×352, 416×416, 480×480, 544×544는 동일한 weight를 가진 동일 모델을 이미지 사이즈만 다르게 해서 실험한 결과입니다.&lt;/li&gt;
&lt;li&gt;YOLOv2가 다른 모델과 mAP는 비슷하지만, FPS는 훨씬 높은 것을 볼 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/yolov2_review/table4.png&#34;
	width=&#34;722&#34;
	height=&#34;106&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov2_review/table4_hu7de32c94a58053b665da3ff9f3211014_41572_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov2_review/table4_hu7de32c94a58053b665da3ff9f3211014_41572_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 7&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;681&#34;
		data-flex-basis=&#34;1634px&#34;
	
&gt;&lt;br&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Redmon, Joseph, and Ali Farhadi. &amp;ldquo;YOLO9000: better, faster, stronger.&amp;rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, Table 4&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;위 표는 &lt;strong&gt;PASCAL VOC 2012&lt;/strong&gt; 데이터에 대한 각 모델의 테스트 결과입니다. YOLOv2는 mAP가 YOLO보다 훨씬 높고, Faster R-CNN(ResNet), SSD512 등과 비슷한 mAP를 가집니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/yolov2_review/table5.png&#34;
	width=&#34;628&#34;
	height=&#34;176&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov2_review/table5_hu3f402c3326b35ae85a6bf9621784c9c8_49763_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov2_review/table5_hu3f402c3326b35ae85a6bf9621784c9c8_49763_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 8&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;356&#34;
		data-flex-basis=&#34;856px&#34;
	
&gt;&lt;br&gt;
&lt;span style=&#34;font-size:70%&#34;&gt; 출처: &lt;a class=&#34;link&#34; href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Redmon, Joseph, and Ali Farhadi. &amp;ldquo;YOLO9000: better, faster, stronger.&amp;rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, Table 5&lt;/em&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;위 표는 &lt;strong&gt;COCO test-dev2015&lt;/strong&gt; 데이터에 대한 각 모델의 테스트 결과입니다.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
