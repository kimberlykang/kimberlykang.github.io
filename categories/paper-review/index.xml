<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Paper review on Kimberly&#39;s Blog</title>
        <link>https://kimberlykang.github.io/categories/paper-review/</link>
        <description>Recent content in Paper review on Kimberly&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ko-kr</language>
        <lastBuildDate>Tue, 04 Jul 2023 00:17:41 +0900</lastBuildDate><atom:link href="https://kimberlykang.github.io/categories/paper-review/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>XLNet 리뷰</title>
        <link>https://kimberlykang.github.io/p/xlnet_review/</link>
        <pubDate>Tue, 04 Jul 2023 00:17:41 +0900</pubDate>
        
        <guid>https://kimberlykang.github.io/p/xlnet_review/</guid>
        <description>&lt;p&gt;본 포스트에서는 2019년 Google이 발표한 &amp;ldquo;XLNet: Generalized Autoregressive Pretraining for Language Understanding&amp;quot;을 살펴보겠습니다.&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1906.08237.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/1906.08237.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;목차&#34;&gt;목차&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#autoregressive-vs-auto-encoding&#34; &gt;AutoRegressive vs. Auto Encoding &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#permutation-language-modeling&#34; &gt;Permutation Language Modeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#two-stream-self-attention&#34; &gt;Two-Stream Self-Attention&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;autoregressive-vs-auto-encoding&#34;&gt;AutoRegressive vs. Auto Encoding&lt;/h2&gt;
&lt;h3 id=&#34;autoregressive-ar-lauguage-modeling&#34;&gt;AutoRegressive (AR) Lauguage Modeling&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;input token sequence가 주어졌을 때, 다음 token을 예측합니다.&lt;/li&gt;
&lt;li&gt;아래 likelihood를 최대화하도록 pre-training합니다.
&lt;ul&gt;
&lt;li&gt;$ \underset{θ}{max} \log p_θ ({\bf x} )
= \displaystyle{\sum_{t=1}^{T}} \log p_θ (x_t | {\bf x_{\rm &amp;lt; t}})
= \displaystyle{\sum_{t=1}^{T}} \log \frac{exp(h_θ({\bf x_{\rm 1:t-1}})_t^{\top} e(x_t))}{ \sum _{x&#39;} exp(h_θ({\bf x _{\rm 1:t-1}})^{\top} e(x&#39;))}
$
&lt;ul&gt;
&lt;li&gt;$ {\bf x} = [x_1, &amp;hellip;, x_T]$: text sequence&lt;/li&gt;
&lt;li&gt;$ h_θ({\bf x _{\rm 1:t-1}}) $: RNNs, Transformer와 같은 neural model을 통해서 얻은 context representation&lt;/li&gt;
&lt;li&gt;$ e(x) $: x의 embedding&lt;/li&gt;
&lt;li&gt;exp는 softmax를 위해 사용되었습니다.&lt;/li&gt;
&lt;li&gt;각 단어는 그 단어 이전에 나온 단어들을 보고 예측이 됩니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;auto-encodingae&#34;&gt;Auto Encoding(AE)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ex) BERT&lt;/li&gt;
&lt;li&gt;input sequence의 일부 token을 mask한 뒤, mask된 token의 원래 token을 예측합니다.&lt;/li&gt;
&lt;li&gt;아래 likelihood를 최대화하도록 pre-training합니다.
&lt;ul&gt;
&lt;li&gt;$ \underset{θ}{max} \log p_θ ({\bf \bar x} | {\bf \hat x})
≈ \displaystyle{\sum_{t=1}^{T}}m_t \log p_θ (x_t | {\bf \hat x})
= \displaystyle{\sum_{t=1}^{T}}m_t \log \frac{exp(H_θ({\bf \hat x})_t^{\top} e(x_t))}{ \sum _{x&#39;} exp(H_θ({\bf \hat x})_t^{\top} e(x&#39;))}
$
&lt;ul&gt;
&lt;li&gt;$ {\bf x} = [x_1, &amp;hellip;, x_T]$: text sequence&lt;/li&gt;
&lt;li&gt;$ {\bf \hat x} $: random하게 $ {\bf x}$의 token을 [MASK]으로 바꾼 corrupted version&lt;/li&gt;
&lt;li&gt;$ {\bf \bar x} $: masked token&lt;/li&gt;
&lt;li&gt;$ m_t=1 $: $x_t$가 masked&lt;/li&gt;
&lt;li&gt;$ H_θ $: text sequence를 hidden vector로 매핑하는 Transformer&lt;/li&gt;
&lt;li&gt;$ e(x) $: x의 embedding&lt;/li&gt;
&lt;li&gt;exp는 softmax를 위해 사용되었습니다.&lt;/li&gt;
&lt;li&gt;approximate factorization입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;학습 목표는 $ {\bf \hat x} $로부터 $ {\bf \bar x} $를 복원하는 것입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;permutation-language-modeling&#34;&gt;Permutation Language Modeling&lt;/h2&gt;
&lt;h3 id=&#34;independence-assumption&#34;&gt;Independence Assumption&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BERT에서는 모든 masked token들은 독립적이라고 가정하고, 따로 예측을 합니다.&lt;/li&gt;
&lt;li&gt;ex) [&lt;code&gt;New&lt;/code&gt;, &lt;code&gt;York&lt;/code&gt;, &lt;code&gt;is&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;city&lt;/code&gt;]에서 [&lt;code&gt;New&lt;/code&gt;, &lt;code&gt;York&lt;/code&gt;]를 prediction target이라고 할 때 BERT와 XLNet의 objectives는 아래와 같습니다.
&lt;ul&gt;
&lt;li&gt;$ {\cal J}_{BERT} = \log p(\text{New} | \text{is a city}) + \log p(\text{York} | \text{is a city})$&lt;/li&gt;
&lt;li&gt;$ {\cal J}_{XLNet} = \log p(\text{New} | \text{is a city}) + \log p(\text{York} | \textcolor{red}{\text{New}} \text{, is a city})$&lt;/li&gt;
&lt;li&gt;BERT에서 &lt;code&gt;York&lt;/code&gt;의 예측은 &lt;code&gt;New&lt;/code&gt;의 예측에 independent합니다.&lt;/li&gt;
&lt;li&gt;BERT에서는 (&lt;code&gt;New&lt;/code&gt;, &lt;code&gt;York&lt;/code&gt;) 사이의 dependency를 찾지 못하지만, XLNet은 이를 고려하여 학습합니다.&lt;/li&gt;
&lt;li&gt;AutoRegressive를 사용하면 [&lt;code&gt;New&lt;/code&gt;, &lt;code&gt;York&lt;/code&gt;, &lt;code&gt;is&lt;/code&gt;]를 보고 &lt;code&gt;a&lt;/code&gt;를 예측해야 합니다. XLNet은 이러한 한계를 순열을 사용하여 극복합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;permutation-language-modeling-objective&#34;&gt;Permutation Language Modeling Objective&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;길이가 T인 sequence x에 대해 T!개의 각기 다른 순서가 존재합니다.&lt;/li&gt;
&lt;li&gt;모든 factorization 순서에 대해 parameter를 공유함으로써 모델은 모든 위치에서 양 방향의 정보를 학습하게 됩니다.&lt;/li&gt;
&lt;li&gt;independence assumption을 하지 않고, &lt;code&gt;[MASK]&lt;/code&gt;를 사용하지 않기 때문에 pretrain-finetune discrepancy도 없게 됩니다.&lt;/li&gt;
&lt;li&gt;factorization order에만 순열을 사용하고 sequence order는 기존 순서를 사용합니다. 즉, positional encoding는 기존 sequence order에 따라 이루어집니다.&lt;/li&gt;
&lt;li&gt;objective는 아래와 같습니다.
&lt;ul&gt;
&lt;li&gt;$ \underset{θ}{max} \Bbb E_{{\bf z} \sim \cal Z_t} [\displaystyle{\sum_{t=1}^{T}} \log p_θ (x_{z_t} | {\bf x_{z \rm &amp;lt; t}})] $&lt;/li&gt;
&lt;li&gt;$ \cal Z_t $: 가능한 모든 순열입니다.&lt;/li&gt;
&lt;li&gt;$ z_t $: 순열의 t번째 값입니다.&lt;/li&gt;
&lt;li&gt;factorization order $ {\bf z} $를 샘플링한 후 factorization order에 따라 likelihood $ \log p_θ $를 계산합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ex) New York is a city
&lt;ul&gt;
&lt;li&gt;순열 1: [&lt;code&gt;New&lt;/code&gt;, &lt;code&gt;York&lt;/code&gt;, &lt;code&gt;is&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;city&lt;/code&gt;]. &lt;code&gt;New&lt;/code&gt; -&amp;gt; &lt;code&gt;York&lt;/code&gt; -&amp;gt;&lt;code&gt;is&lt;/code&gt; 를 보고 &lt;code&gt;a&lt;/code&gt;를 예측합니다.&lt;/li&gt;
&lt;li&gt;순열 2: [&lt;code&gt;city&lt;/code&gt;, &lt;code&gt;York&lt;/code&gt;, &lt;code&gt;New&lt;/code&gt;, &lt;code&gt;is&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt;]. &lt;code&gt;city&lt;/code&gt; -&amp;gt; &lt;code&gt;York&lt;/code&gt; -&amp;gt; &lt;code&gt;New&lt;/code&gt; -&amp;gt; &lt;code&gt;is&lt;/code&gt;를 보고 &lt;code&gt;a&lt;/code&gt;를 예측합니다.&lt;/li&gt;
&lt;li&gt;위치상으로 &lt;code&gt;a&lt;/code&gt; 뒤에 있는 &lt;code&gt;city&lt;/code&gt;를 보고 &lt;code&gt;a&lt;/code&gt;를 예측하게 됩니다.&lt;/li&gt;
&lt;li&gt;순열을 사용함으로써 bidirectional context를 학습하게 됩니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/xlnet_review/permutation.png&#34;
	width=&#34;962&#34;
	height=&#34;729&#34;
	srcset=&#34;https://kimberlykang.github.io/p/xlnet_review/permutation_hu2a3025eb74dc7a675c4490d541f9fe32_91998_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/xlnet_review/permutation_hu2a3025eb74dc7a675c4490d541f9fe32_91998_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Permutation&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;131&#34;
		data-flex-basis=&#34;316px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1906.08237.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;XLNet: Generalized Autoregressive Pretraining for Language Understanding, Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le, arXiv:1906.08237, 2019, Figure 4&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;위의 순열로 뽑은 factorization order에 따른 $ \log p_θ $를 구한 뒤 expectation을 구합니다.&lt;/li&gt;
&lt;li&gt;mem는 transformer XL의 특징으로 긴 sequence 학습이 가능하게 해 주는 것으로 gradient가 적용이 안 됩니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;two-stream-self-attention&#34;&gt;Two-Stream Self-Attention&lt;/h2&gt;
&lt;h3 id=&#34;target-position을-고려한-re-parameterization&#34;&gt;Target Position을 고려한 Re-parameterization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;기존 transformer 사용시 t까지 순서가 동일하고 t에서의 순서는 다른 순열 $ {\bf z}^{(1)} $, $ {\bf z}^{(2)} $가 있을 때, 기존의 방법을 사용하면 아래와 같은 문제가 발생합니다.
&lt;img src=&#34;https://kimberlykang.github.io/p/xlnet_review/problems_original_transformer.png&#34;
	width=&#34;1188&#34;
	height=&#34;166&#34;
	srcset=&#34;https://kimberlykang.github.io/p/xlnet_review/problems_original_transformer_hu12f964d52a9bfc2531679f083dd729c6_34637_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/xlnet_review/problems_original_transformer_hu12f964d52a9bfc2531679f083dd729c6_34637_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Problems with Original Transformer&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;715&#34;
		data-flex-basis=&#34;1717px&#34;
	
&gt;
&lt;ul&gt;
&lt;li&gt;$ {\bf z}^{(1)}_t $, $ {\bf z}^{(2)}_t $ 값이 달라 서로 다른 target position을 갖고 따라서 다른 ground-truth를 갖지만, 동일한 model prediction을 하게 됩니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$ {\bf x_{z \rm &amp;lt; t}} $와 함께 target position $ z_t $를 추가로 input으로 받는 새로운 representation $ g_θ({\bf x_{z \rm &amp;lt; t}}, z_t) $를 사용하여 아래와 같이 prediction을 합니다.
&lt;ul&gt;
&lt;li&gt;$ p_θ (X_{x_{z_t}}=x | x_{z &amp;lt; t}) = \frac{exp(e(x)^{\top} g_θ({\bf x_{z \rm &amp;lt; t}}, z_t))}{ \sum _{x&#39;} exp(e(x&#39;)^{\top} g_θ({\bf x _{z \rm &amp;lt; t}}, z_t) )}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;두-개의-hidden-representation을-사용&#34;&gt;두 개의 hidden representation을 사용&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;$ g_θ({\bf x _{z \rm &amp;lt; t}}, z_t) $의 조건
&lt;ul&gt;
&lt;li&gt;content $ x_{z_t} $를 사용하면 학습이 일어나지 않기 때문에, content $ x_{z_t} $는 사용하지 않고 position $ z_t $만 사용해야 합니다.&lt;/li&gt;
&lt;li&gt;j&amp;gt;t인 $ x_{z_j} $를 예측하기 위해서는 content $ x_{z_t}$는 사용하지 않지만 contextual information을 갖고 있긴 해야 합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;hidden representation
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;content representation&lt;/strong&gt; $ h_θ({\bf x_{z \rm ≤ t}}) $: 기존 Transformer의 hidden state와 비슷한 역할을 합니다. context와 $ x_{z_t} $를 모두 encode합니다.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;query representation&lt;/strong&gt; $ g_θ({\bf x_{z \rm &amp;lt; t}}, z_t) $ t 이전의 contextual information과 position $ z_t $를 encode합니다.&lt;/li&gt;
&lt;li&gt;$ h_i^{(0)}=e(x_i) $. content stream은 첫 layer는 word embedding입니다.&lt;/li&gt;
&lt;li&gt;$ g_i^{(0)}=w $. query stream은 첫 layer는 학습이 되는 vector로 initialize 됩니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;update
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;query stream&lt;/strong&gt;: $ g_{z_t}^{(m)} ← Attention(Q=g_{z_t}^{(m-1)}, KV=h_\textcolor{red}{{\bf z} &amp;lt; t}^{(m-1)}; θ)$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;content stream&lt;/strong&gt;: $ h_{z_t}^{(m)} ← Attention(Q=h_{z_t}^{(m-1)}, KV=h_\textcolor{red}{{\bf z} ≤ t}^{(m-1)}; θ)$&lt;/li&gt;
&lt;li&gt;query stream에서는  &lt;code&gt;$z_t$&lt;/code&gt;는 사용하지만, &lt;code&gt;$ x_{z_t} $&lt;/code&gt;는 사용하지 않습니다. 반면에 content stream에서는 &lt;code&gt;$z_t$&lt;/code&gt;, &lt;code&gt;$ x_{z_t} $&lt;/code&gt; 둘 다 사용합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/xlnet_review/two_stream_attention.png&#34;
	width=&#34;1559&#34;
	height=&#34;718&#34;
	srcset=&#34;https://kimberlykang.github.io/p/xlnet_review/two_stream_attention_hudafbccca2a7cd50030c8f978b100117b_172802_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/xlnet_review/two_stream_attention_hudafbccca2a7cd50030c8f978b100117b_172802_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Two-Stream Self-Attention&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;217&#34;
		data-flex-basis=&#34;521px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1906.08237.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;XLNet: Generalized Autoregressive Pretraining for Language Understanding, Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le, arXiv:1906.08237, 2019, Figure 4&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;factorization order이 3 -&amp;gt; 2 -&amp;gt; 4 -&amp;gt; 1일 때,&lt;/li&gt;
&lt;li&gt;다음 layer의 $h_3^{(1)}$ 자기 자신을 포함한 이전 시점의 이전 layer를 attend합니다. 즉, $h_1^{(0)}, h_2^{(0)}, h_3^{(0)}$을 attend 합니다.&lt;/li&gt;
&lt;li&gt;다음 layer의 $g_3^{(1)}$ 이전 시점의 이전 layer만을 attend합니다. 즉, $h_1^{(0)}, h_2^{(0)}$을 attend 합니다.&lt;/li&gt;
&lt;li&gt;마지막 layer에서는 g에서 token을 예측합니다.&lt;/li&gt;
&lt;li&gt;위와 같은 attention 방식으로 인해 prediction layer에서는 현재 word embedding의 정보를 직접적으로 얻지 못하기 때문에 학습이 잘 됩니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/xlnet_review/content_stream.png&#34;
	width=&#34;755&#34;
	height=&#34;934&#34;
	srcset=&#34;https://kimberlykang.github.io/p/xlnet_review/content_stream_hu03bbe986eea1aeffac54966a37d2c831_134712_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/xlnet_review/content_stream_hu03bbe986eea1aeffac54966a37d2c831_134712_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Content Stream&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;80&#34;
		data-flex-basis=&#34;194px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1906.08237.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;XLNet: Generalized Autoregressive Pretraining for Language Understanding, Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le, arXiv:1906.08237, 2019, Figure 5&lt;/em&gt;&lt;/a&gt;&lt;br&gt;
&lt;img src=&#34;https://kimberlykang.github.io/p/xlnet_review/query_stream.png&#34;
	width=&#34;756&#34;
	height=&#34;931&#34;
	srcset=&#34;https://kimberlykang.github.io/p/xlnet_review/query_stream_hu31ba4eacdc469d4a671273c750ea7e9b_139173_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/xlnet_review/query_stream_hu31ba4eacdc469d4a671273c750ea7e9b_139173_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Query Stream&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;81&#34;
		data-flex-basis=&#34;194px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1906.08237.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;XLNet: Generalized Autoregressive Pretraining for Language Understanding, Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le, arXiv:1906.08237, 2019, Figure 6&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>BERT 리뷰</title>
        <link>https://kimberlykang.github.io/p/bert_review/</link>
        <pubDate>Sun, 18 Jun 2023 00:17:41 +0900</pubDate>
        
        <guid>https://kimberlykang.github.io/p/bert_review/</guid>
        <description>&lt;p&gt;본 포스트에서는 2018년 Google이 발표한 &amp;ldquo;BERT : Pre-training of Deep Bidirectional Trnasformers for Language Understanding&amp;quot;을 살펴보겠습니다.&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/pdf/1810.04805.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;목차&#34;&gt;목차&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%eb%aa%a8%eb%8d%b8-%ea%b5%ac%ec%a1%b0&#34; &gt;모델 구조&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#input&#34; &gt;Input&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#pre-training&#34; &gt;Pre-training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#fine-tuning&#34; &gt;Fine-tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%eb%84%a4%ed%8a%b8%ec%9b%8c%ed%81%ac-%ea%b5%ac%ec%a1%b0&#34; &gt;네트워크 구조&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%ed%95%99%ec%8a%b5&#34; &gt;학습&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%ed%85%8c%ec%8a%a4%ed%8a%b8&#34; &gt;테스트&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;모델-구조&#34;&gt;모델 구조&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/bert_review/bert_gpt_elmo.png&#34;
	width=&#34;1495&#34;
	height=&#34;346&#34;
	srcset=&#34;https://kimberlykang.github.io/p/bert_review/bert_gpt_elmo_hu627b81f4322d5b928ebc700608cfc37d_114762_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/bert_review/bert_gpt_elmo_hu627b81f4322d5b928ebc700608cfc37d_114762_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;BERT, GPT, ELMO 비교&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;432&#34;
		data-flex-basis=&#34;1036px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805, 2018, Figure 3&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GPT&lt;/strong&gt;: left-to-right Transformer를 사용합니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ELMo&lt;/strong&gt;: left-to-right LSTM, right-to-left LSTM을 따로 학습 후, concatenate해서 사용합니다. feature-based approach입니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;BERT&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bi-directional transformer를 사용합니다.&lt;/li&gt;
&lt;li&gt;예측을 하지 않고 자기 자신의 token을 참조할 수 있는 bi-directional 모델의 문제점을 두 가지 pre-training task로 극복합니다.&lt;/li&gt;
&lt;li&gt;parameter 갯수는 BERT base가 110M, BERT large가 340M입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GPT&lt;/strong&gt;와 &lt;strong&gt;BERT&lt;/strong&gt;의 차이점&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT는 BooksCorpus(800M 단어)로만 학습하고, BERT는 BooksCorpus와 Wikipedia(2,500M 단어)로 학습했습니다.&lt;/li&gt;
&lt;li&gt;GPT는 &lt;code&gt;[SEP]&lt;/code&gt;와 &lt;code&gt;[CLS]&lt;/code&gt;를 fine-tuning에서만 사용했고, BERT는 &lt;code&gt;[SEP]&lt;/code&gt;, &lt;code&gt;[CLS]&lt;/code&gt;, sentence A/B embedding을 pre-training에도 사용했습니다.&lt;/li&gt;
&lt;li&gt;GPT는 32,000 단어의 batch size로 1M step을 학습했고, BERT는 128,000 단어의 batch size로 1M step을 학습했습니다.&lt;/li&gt;
&lt;li&gt;GPT는 전체 fine-tuning 실험에서 동일한 learning rate를 사용했고, BERT는 task-specific한 learning rate을 사용했습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;input&#34;&gt;Input&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/bert_review/input.png&#34;
	width=&#34;863&#34;
	height=&#34;263&#34;
	srcset=&#34;https://kimberlykang.github.io/p/bert_review/input_hu06eac06465f811180df7aaa2dce1d28d_34819_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/bert_review/input_hu06eac06465f811180df7aaa2dce1d28d_34819_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Input 구성 요소&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;328&#34;
		data-flex-basis=&#34;787px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805, 2018, Figure 2&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Token embeddings
&lt;ul&gt;
&lt;li&gt;WordPiece embedding을 사용합니다.(We use WordPiece emgeddings with a 30,000 token vocabulary.)&lt;/li&gt;
&lt;li&gt;vocab_size=30522로 522개는 빈 토큰인 것으로 보입니다.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[CLS]&lt;/code&gt;: 모든 sequence는 &lt;code&gt;[CLS]&lt;/code&gt; token으로 시작하며, classification task에 사용됩니다.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[SEP]&lt;/code&gt;: Sequence 안의 sentence를 나누기 위해서 사용됩니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Segment Embeddings: 첫 번째 sentence인지 두 번째 sentence인지 구분하기 위해 사용됩니다.&lt;/li&gt;
&lt;li&gt;Position Embeddings: 각 토큰이 얼마나 떨어져 있는지 나타내기 위해 사용됩니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;pre-training&#34;&gt;Pre-training&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;unlabled data를 사용하여 두 가지 unsupervised task를 학습합니다.&lt;/p&gt;
&lt;h3 id=&#34;task1-masked-lmmlm&#34;&gt;Task1: Masked LM(MLM)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;전체 input token의 15%를 random하게 mask한 뒤, 이에 해당하는 마지막 hidden vector가 전체 vocabulary에 대한 softmax를 수행합니다.&lt;/li&gt;
&lt;li&gt;fine-tuning에서는 &lt;code&gt;[MASK]&lt;/code&gt; token이 없기 때문에, mask하도록 선택된 15% 중, 80%는 &lt;code&gt;[MASK]&lt;/code&gt; token으로 바꾸고, 10%는 random한 다른 token으로 바꾸며 나머지 10%는 바꾸지 않은 채 예측을 합니다.&lt;/li&gt;
&lt;li&gt;아래 그림은 sequence &amp;quot;my dog is cutep [SEP] he likes play ###ing[SEP]&amp;quot;에 mask 적용 후 예측을 하는 그림입니다.&lt;br&gt;
&lt;img src=&#34;https://kimberlykang.github.io/p/bert_review/mlm.png&#34;
	width=&#34;457&#34;
	height=&#34;230&#34;
	srcset=&#34;https://kimberlykang.github.io/p/bert_review/mlm_hue48afc9cd1375f9851ac94854c641983_9589_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/bert_review/mlm_hue48afc9cd1375f9851ac94854c641983_9589_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Masked LM 예시&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;198&#34;
		data-flex-basis=&#34;476px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://wikidocs.net/115055&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://wikidocs.net/115055&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&amp;lsquo;dog&amp;rsquo;은 [MASK]로 변경되었습니다.&lt;/li&gt;
&lt;li&gt;&amp;lsquo;he&amp;rsquo;는 &amp;lsquo;king으로 변경되었습니다.&lt;/li&gt;
&lt;li&gt;&amp;lsquo;play&amp;rsquo;는 변경되지 않았습니다.&lt;/li&gt;
&lt;li&gt;&amp;lsquo;dog&amp;rsquo;, &amp;lsquo;he&amp;rsquo;, &amp;lsquo;play&amp;rsquo;에 해당되는 토큰의 마지막 hidden vector는 Fully Connected Layer를 거쳐 softmax를 하여 전체 vocabulary에 대한 확률을 구합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;task2-next-sentence-predictionnsp&#34;&gt;Task2: Next Sentence Prediction(NSP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;sentence A, B를 선택할 때, 50%의 경우에는 B로 A 다음에 이어지는 sentence를, 나머지 50%의 경우에는 B로 corpus에서 random한 sentence를 선택합니다.&lt;/li&gt;
&lt;li&gt;아래는 sentence 예시입니다.
&lt;blockquote&gt;
&lt;p&gt;Input = [CLS] the man went to [MASK] store [SEP] &lt;br/&gt;
he bought a gallon [MASK] milk [SEP] &lt;br/&gt;
Label = IsNext &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Input = [CLS] the man [MASK] to the store [SEP] &lt;br/&gt;
penguin [MASK] are flight ##less birds [SEP] &lt;br/&gt;
Label = NotNext&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/bert_review/nsp.png&#34;
	width=&#34;486&#34;
	height=&#34;237&#34;
	srcset=&#34;https://kimberlykang.github.io/p/bert_review/nsp_hu2312c72a16e1af0a0fe9440fe0f26496_10762_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/bert_review/nsp_hu2312c72a16e1af0a0fe9440fe0f26496_10762_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Next Sentence Prediction 예시&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;205&#34;
		data-flex-basis=&#34;492px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://wikidocs.net/115055&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://wikidocs.net/115055&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;[CLS]&lt;/code&gt; toekn의 마지막 hidden vector가 &lt;code&gt;IsNext&lt;/code&gt;/&lt;code&gt;NotNext&lt;/code&gt; classification을 하는 데 사용됩니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;fine-tuning&#34;&gt;Fine-tuning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;pre-train 한 parameter로 initialize한 뒤, downstream task를 위한 labeled data로 fine-tune 합니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;output layer 하나만 추가되므로, scratch부터 학습되는 parameter 갯수는 매우 적습니다.&lt;/p&gt;
&lt;h3 id=&#34;sentence-pair-classification-tasks&#34;&gt;Sentence Pair Classification Tasks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dataset
&lt;ul&gt;
&lt;li&gt;MNLI: 첫 번째 sentence를 고려했을 때, 두 번째 sentence가 참, 거짓, 판단불가인지 선택합니다.&lt;/li&gt;
&lt;li&gt;QQP: 질문 두 개가 의미상으로 동일한지 판단합니다.&lt;/li&gt;
&lt;li&gt;QNLI: 질문에 대한 정확한 대답이 있는지 판단합니다.&lt;/li&gt;
&lt;li&gt;STS-B: 두 문장이 얼마나 비슷한지 1~5점 중 선택합니다.&lt;/li&gt;
&lt;li&gt;MRPC: 두 문장이 의미상으로 동일한지 판단합니다.&lt;/li&gt;
&lt;li&gt;RTE: 첫 번째 sentence를 고려했을 때, 두 번째 sentence가 참, 거짓, 판단불가인지 선택합니다.&lt;/li&gt;
&lt;li&gt;SWAG: sentence가 주어졌을 때 네 개의 선택지 중 가장 다음 문장으로 적합한 문장을 고르는 문제를 네 개로 나누어 적합한지 판단하는 문제로 변경합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;C는 &lt;code&gt;[CLS]&lt;/code&gt; token의 마지막 hidden vector로 $C∈R^H$입니다. H는 hidden size로 $BERT_{BASE}$는 768, $BERT_{LARGE}$는 1024입니다.&lt;/li&gt;
&lt;li&gt;새로 학습해야 하는 parameter는 classification layer의 weight $W∈\mathbb R^{K \times H}$입니다. K는 클래스 갯수입니다.
&lt;img src=&#34;https://kimberlykang.github.io/p/bert_review/sentence_pair_classification.png&#34;
	width=&#34;564&#34;
	height=&#34;503&#34;
	srcset=&#34;https://kimberlykang.github.io/p/bert_review/sentence_pair_classification_huc922a87801af2b03b3e761b918c5893f_59915_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/bert_review/sentence_pair_classification_huc922a87801af2b03b3e761b918c5893f_59915_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Sentence Pair Classification Tasks&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;112&#34;
		data-flex-basis=&#34;269px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805, 2018, Figure 4&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;결과
&lt;img src=&#34;https://kimberlykang.github.io/p/bert_review/result_glue.png&#34;
	width=&#34;766&#34;
	height=&#34;162&#34;
	srcset=&#34;https://kimberlykang.github.io/p/bert_review/result_glue_hudb56a7fdb2eb4cad4ac7338936233677_38509_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/bert_review/result_glue_hudb56a7fdb2eb4cad4ac7338936233677_38509_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;GLUE result&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;472&#34;
		data-flex-basis=&#34;1134px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805, 2018, Table 1&lt;/em&gt;&lt;/a&gt;&lt;br&gt;
&lt;img src=&#34;https://kimberlykang.github.io/p/bert_review/result_swag.png&#34;
	width=&#34;248&#34;
	height=&#34;184&#34;
	srcset=&#34;https://kimberlykang.github.io/p/bert_review/result_swag_hu741fe5f854be73947a1e26ebf777395c_13805_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/bert_review/result_swag_hu741fe5f854be73947a1e26ebf777395c_13805_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;SWAG result&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;134&#34;
		data-flex-basis=&#34;323px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805, 2018, Table 4&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;single-sentence-classification-tasks&#34;&gt;Single Sentence Classification Tasks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dataset
&lt;ul&gt;
&lt;li&gt;SST-2: sentiment를 판단합니다.&lt;/li&gt;
&lt;li&gt;CoLA: 문장이 언어학적으로 &amp;ldquo;acceptable&amp;rdquo; 한 지 판단합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sentence Pair Classification Tasks에서와 같이 fine-tuning과 더불어 classification layer의 weight $W∈\mathbb R^{K \times H}$를 학습합니다.&lt;br&gt;
&lt;img src=&#34;https://kimberlykang.github.io/p/bert_review/single_sentence_classification.png&#34;
	width=&#34;564&#34;
	height=&#34;503&#34;
	srcset=&#34;https://kimberlykang.github.io/p/bert_review/single_sentence_classification_huc27f4a6a57c6ad8034f0e0ab6460b268_58378_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/bert_review/single_sentence_classification_huc27f4a6a57c6ad8034f0e0ab6460b268_58378_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Single Sentence Classification Tasks&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;112&#34;
		data-flex-basis=&#34;269px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805, 2018, Figure 4&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;question-answering-tasks&#34;&gt;Question Answering Tasks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dataset
&lt;ul&gt;
&lt;li&gt;SQuAD v1.1: Question에 대한 answer을 passage 안에서 찾습니다.&lt;/li&gt;
&lt;li&gt;SQuAD v2.0&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Question을 &lt;code&gt;A embedding&lt;/code&gt;으로, passage를 &lt;code&gt;B embedding&lt;/code&gt;으로 나타냅니다.&lt;/li&gt;
&lt;li&gt;fine-tuning에서 start vector $S∈R^H$와 end vector $E∈R^H$를 학습합니다.&lt;/li&gt;
&lt;li&gt;각 token의 마지막 hidden vector T와 S와 E와 사이의 dot product와 softmax를 구해 start와 end 를 결정합니다.&lt;br&gt;
&lt;img src=&#34;https://kimberlykang.github.io/p/bert_review/qa.png&#34;
	width=&#34;565&#34;
	height=&#34;487&#34;
	srcset=&#34;https://kimberlykang.github.io/p/bert_review/qa_hud93a7b2d11ea663f2f90914a19fb3bea_63643_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/bert_review/qa_hud93a7b2d11ea663f2f90914a19fb3bea_63643_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Single Sentence Classification Tasks&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;116&#34;
		data-flex-basis=&#34;278px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805, 2018, Figure 4&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;single-sentence-tagging-tasks&#34;&gt;Single Sentence Tagging Tasks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dataset
&lt;ul&gt;
&lt;li&gt;CoNLL-2003 NER&lt;br&gt;
&lt;img src=&#34;https://kimberlykang.github.io/p/bert_review/single_sentence_tagging.png&#34;
	width=&#34;565&#34;
	height=&#34;487&#34;
	srcset=&#34;https://kimberlykang.github.io/p/bert_review/single_sentence_tagging_hued301fd6ac715def572f51aeadf0bbe5_59367_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/bert_review/single_sentence_tagging_hued301fd6ac715def572f51aeadf0bbe5_59367_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Single Sentence Tagging Tasks&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;116&#34;
		data-flex-basis=&#34;278px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805, 2018, Figure 4&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;결과
&lt;img src=&#34;https://kimberlykang.github.io/p/bert_review/conll_2003.png&#34;
	width=&#34;694&#34;
	height=&#34;563&#34;
	srcset=&#34;https://kimberlykang.github.io/p/bert_review/conll_2003_hu968eea676d56d06edf3db2745542ef98_87486_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/bert_review/conll_2003_hu968eea676d56d06edf3db2745542ef98_87486_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;CoNLL-2003 Named Entity Recognition Tasks&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;123&#34;
		data-flex-basis=&#34;295px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805, 2018, Table 7&lt;/em&gt;&lt;/a&gt;&lt;br&gt;
file:///run/user/1000/gvfs/sftp:host=4gpu,user=embian/home/embian/projects/yolov8/nohup.out&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>YOLOv1 리뷰</title>
        <link>https://kimberlykang.github.io/p/yolov1_review/</link>
        <pubDate>Mon, 29 May 2023 00:17:41 +0900</pubDate>
        
        <guid>https://kimberlykang.github.io/p/yolov1_review/</guid>
        <description>&lt;p&gt;본 포스트에서는 2016년 CVPR에 발표된 논문 &amp;ldquo;You Only Look Once: Unified, Real-Time Object Detection&amp;quot;을 살펴보겠습니다.&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;목차&#34;&gt;목차&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%ea%b0%9c%eb%85%90-%ec%9d%b4%ed%95%b4&#34; &gt;개념 이해&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#bounding-box&#34; &gt;Bounding Box&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#class-probability&#34; &gt;Class Probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#output&#34; &gt;Output&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%eb%84%a4%ed%8a%b8%ec%9b%8c%ed%81%ac-%ea%b5%ac%ec%a1%b0&#34; &gt;네트워크 구조&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%ed%95%99%ec%8a%b5&#34; &gt;학습&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%ed%85%8c%ec%8a%a4%ed%8a%b8&#34; &gt;테스트&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;개념-이해&#34;&gt;개념 이해&lt;/h2&gt;
&lt;p&gt;YOLO에서는 이미지에서 object를 찾을 때, grid를 사용합니다. 이를 간략하게 살펴보면&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;이미지를 &lt;strong&gt;S×S&lt;/strong&gt;개의 grid로 나눕니다.&lt;/li&gt;
&lt;li&gt;각 grid cell에서 &lt;strong&gt;B&lt;/strong&gt;개의 bounding box의 좌표(x, y, w, h)와 그 bounding box의 confidence score(c)를 예측합니다.&lt;/li&gt;
&lt;li&gt;각 grid cell마다 &lt;strong&gt;C&lt;/strong&gt;개의 class probability를 예측합니다.&lt;/li&gt;
&lt;li&gt;confidence score과 class probability로 최종 score와 class를 결정하고, 이 때 score가 threshold보다 높은 bounding box가 최종 object detection의 결과가 됩니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/yolov1_review/bboxes.png&#34;
	width=&#34;1186&#34;
	height=&#34;822&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov1_review/bboxes_hud2dccab5ff2d7257884dfebad1d3f231_1100556_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov1_review/bboxes_hud2dccab5ff2d7257884dfebad1d3f231_1100556_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Bounding Box 예측 예시&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;346px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;위 이미지는 &lt;strong&gt;S=7&lt;/strong&gt;, &lt;strong&gt;B=2&lt;/strong&gt;일 때 하나의 grid에 대한 예측의 예시입니다.&lt;/li&gt;
&lt;li&gt;각 grid cell마다 bounding box(x, y, w, h, c)를 두 세트씩 예측합니다.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;C=20&lt;/strong&gt;일 때, 각 grid cell마다 20개의 class에 대한 class probability를 예측합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;지금까지 YOLO가 어떻게 object를 찾는지 대략적으로 살펴봤습니다. 아래에서는 각 예측값들이 어떻게 계산되고 어떤 의미를 갖는지 살펴보겠습니다.&lt;/p&gt;
&lt;h2 id=&#34;bounding-box&#34;&gt;Bounding Box&lt;/h2&gt;
&lt;p&gt;bounding box는 (x, y, w, h, c)로 이루어져 있습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(x, y): bounding box &lt;strong&gt;중심의 좌표&lt;/strong&gt;.
&lt;ul&gt;
&lt;li&gt;각 grid cell마다 B개의 bounding box를 가진다는 의미는, 이 B개의 bounding box의 중심이 해당 grid cell 안에 있다는 의미입니다.&lt;/li&gt;
&lt;li&gt;box의 네 꼭지점은 bounding box 밖에 있어도 상관이 없습니다.&lt;/li&gt;
&lt;li&gt;grid cell 안에서 상대적으로 어느 위치에 있는지를 표시하며, 0~1 사이의 값을 가집니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(w, h): bounding box의 &lt;strong&gt;너비, 높이&lt;/strong&gt;.
&lt;ul&gt;
&lt;li&gt;전체 이미지 너비, 높이에 대해 상대적인 값을 사용하며 0~1 사이의 값을 가집니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;c: box가 &lt;strong&gt;object를 갖는 것&lt;/strong&gt;에 대해 얼마나 확신하고, 그 box의 &lt;strong&gt;좌표가 얼마나 정확한&lt;/strong&gt; 가에 대한 값입니다.
&lt;ul&gt;
&lt;li&gt;$ Pr(Object) * IOU^{truth}_{pred} $&lt;/li&gt;
&lt;li&gt;좌표가 얼마나 정확한지를 계산할 때에는 &lt;strong&gt;예측한 좌표와 Ground Truth&lt;/strong&gt;(&lt;strong&gt;GT&lt;/strong&gt;) &lt;strong&gt;사이의 Intersection Over Union&lt;/strong&gt;(&lt;strong&gt;IOU&lt;/strong&gt;)를 계산합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;class-probability&#34;&gt;Class Probability&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;C: 각 grid cell이 object를 가지고 있을 때의 class probability입니다.
&lt;ul&gt;
&lt;li&gt;$ Pr(Class_i|Object) $&lt;/li&gt;
&lt;li&gt;bounding box 갯수와 상관 없이 grid cell마다 한 세트의 class probability만 예측합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;output&#34;&gt;Output&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;이미지를 S×S grid로 나누고, 각 grid마다 B개의 bounding box와 C개 class에 대한 class probability가 있다고 할 때, 최종 output tensor의 shape은 &lt;strong&gt;S×S×&lt;/strong&gt;(&lt;strong&gt;B*5+C&lt;/strong&gt;)가 됩니다.&lt;/li&gt;
&lt;li&gt;PASCAL VOC 실험 예를 들면, S=7, B=2, C=20입니다. 따라서 총 49개의 grid가 있고, 각 grid cell마다 30개의 예측값을 갖게 됩니다. 아래는 30개의 예측값 중 하나의 예시입니다.
&lt;ul&gt;
&lt;li&gt;[x, y, w, h, c, 배경일 확률, 비행기일 확률, &amp;hellip;, 모니터일 확률]&lt;/li&gt;
&lt;li&gt;[0.4, 0.3, 0.8, 0.7, 0.9, 0.003, 0.8, &amp;hellip; , 0.012]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;네트워크-구조&#34;&gt;네트워크 구조&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GoogLeNet과 유사한 구조를 사용했습니다.
&lt;img src=&#34;https://kimberlykang.github.io/p/yolov1_review/architecture.png&#34;
	width=&#34;670&#34;
	height=&#34;272&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov1_review/architecture_hu519f237000a77904bf7b34099756d7b5_30076_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov1_review/architecture_hu519f237000a77904bf7b34099756d7b5_30076_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Architecture&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;246&#34;
		data-flex-basis=&#34;591px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, You Only Look Once: Unified, Real-Time Object Detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp.781, Figure 3&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;loss&#34;&gt;Loss&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;sum-squared error를 사용했습니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$ \lambda_{coor} $, $ \lambda_{noobj} $ 사용&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;대부분의 이미지는 object가 있는 grid cell보다 object가 없는 grid cell이 더 많습니다. 그래서 object가 없는 grid cell의 confidence score는 0에 가까워질 때의 gradient가 object가 있는 grid cell의 gradient보다 훨씬 커서 학습 초기에 모델이 diverge 하는 현상이 생길 수 있습니다. 이를 막기 위해, $ \lambda_{coor}=5 $를 사용하여 bounding box 좌표 예측 loss는 증가시키고, $ \lambda_{noobj}=0.5 $를 사용하여 object가 없는 box의 confidence 예측 loss는 감소시켰습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$ \sqrt w $, $ \sqrt h $ 예측&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;큰 box는 편차가 조금 있어도 크게 상관 없지만 작은 box에는 큰 영향을 끼칩니다. 이 문제를 해결하기 위해 bounding box 너비와 높이 값에 루트를 씌워 줬습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;각 object에 가장 IOU가 높은 하나의 bounding box predictor만 할당했습니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Loss function&lt;br&gt;
$ \lambda_{coor} \displaystyle{\sum_{i=0}^{s^2}} \displaystyle{\sum_{j=0}^{B}} \mathbb 1_{ij}^{obj}  [(x_i-\hat x_i)^2 + (y_i-\hat y_i)^2] $&lt;br&gt;
$ + \lambda_{coor} \displaystyle{\sum_{i=0}^{s^2}} \displaystyle{\sum_{j=0}^{B}} \mathbb 1_{ij}^{obj}  [(\sqrt{w_i} - \sqrt{\hat w_i})^2 + (\sqrt{h_i} - \sqrt{\hat h_i})^2] $&lt;br&gt;
$ + \displaystyle{\sum_{i=0}^{s^2}} \displaystyle{\sum_{j=0}^{B}} \mathbb 1_{ij}^{obj} (C_i-\hat C_i)^2 $
$ + \lambda_{noobj} \displaystyle{\sum_{i=0}^{s^2}} \displaystyle{\sum_{j=0}^{B}} \mathbb 1_{ij}^{noobj} (C_i-\hat C_i)^2 $&lt;br&gt;
$ + \displaystyle{\sum_{i=0}^{s^2}} \mathbb 1_{i}^{obj} \displaystyle{\sum_{c∈classwa}} (p_i(c)-\hat p_i(c))^2 $&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1&lt;sup&gt;obj&lt;/sup&gt;&lt;sub&gt;i&lt;/sub&gt;: cell i에 object가 나타났는지 여부를 의미합니다.&lt;/li&gt;
&lt;li&gt;1&lt;sup&gt;obj&lt;/sup&gt;&lt;sub&gt;ij&lt;/sub&gt;: cell i에 있는 j번째 bounding box가 cell i에 있는 GT에 해당하는 예측값인지 여부입니다. 즉, cell i에 있는 j번째 bounding box가 GT와의 IOU가 가장 큰 지 여부입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;테스트&#34;&gt;테스트&lt;/h2&gt;
&lt;p&gt;$$ Pr(Class_i|Object) * Pr(Object) * IOU^{truth}&lt;em&gt;{pred} = Pr(Class_i) * IOU^{truth}&lt;/em&gt;{pred} $$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/yolov1_review/notation4.png&#34;
	width=&#34;564&#34;
	height=&#34;40&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov1_review/notation4_hu013f73fd8add0c13dbd52f20ed59f69f_8925_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov1_review/notation4_hu013f73fd8add0c13dbd52f20ed59f69f_8925_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 4&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;1410&#34;
		data-flex-basis=&#34;3384px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;테스트할 때는 class probability와 confidence score를 곱해서 최종 score를 계산합니다.&lt;/li&gt;
&lt;li&gt;최종 score는 해당 박스에 class가 나타날 확률과 예측한 box가 실제 object에 얼마나 잘 맞는 지 두 가지 의미를 모두 갖고 있습니다.&lt;/li&gt;
&lt;li&gt;아래 그림의 bounding box 선의 굵기가 confidence를 의미합니다. 이 confidence와 class probability를 곱하여 threshold 이상의 결과가 최종 결과가 됩니다.
&lt;img src=&#34;https://kimberlykang.github.io/p/yolov1_review/figure2.png&#34;
	width=&#34;642&#34;
	height=&#34;404&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov1_review/figure2_hud5b2df22550bbf7449e31736c4696048_251487_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov1_review/figure2_hud5b2df22550bbf7449e31736c4696048_251487_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 5&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;158&#34;
		data-flex-basis=&#34;381px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, You Only Look Once: Unified, Real-Time Object Detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp.780, Figure 2&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
