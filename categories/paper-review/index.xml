<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Paper review on Kimberly&#39;s Blog</title>
        <link>https://kimberlykang.github.io/categories/paper-review/</link>
        <description>Recent content in Paper review on Kimberly&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ko-kr</language>
        <lastBuildDate>Thu, 16 Feb 2023 00:09:40 +0900</lastBuildDate><atom:link href="https://kimberlykang.github.io/categories/paper-review/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>YOLOv2 리뷰</title>
        <link>https://kimberlykang.github.io/p/yolov2_review/</link>
        <pubDate>Thu, 16 Feb 2023 00:09:40 +0900</pubDate>
        
        <guid>https://kimberlykang.github.io/p/yolov2_review/</guid>
        <description>&lt;p&gt;본 포스트에서는 2017년 CVPR에 발표된 논문 &amp;ldquo;YOLO9000: Better, Faster, Stronger&amp;quot;을 살펴보겠습니다.&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1612.08242v1.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;YOLO9000: Better, Faster, Stronger&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;목차&#34;&gt;목차&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%eb%8f%99%ea%b8%b0&#34; &gt;동기&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#batch-normalization&#34; &gt;Batch Normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%eb%86%92%ec%9d%80-resolution%ec%97%90%ec%84%9c-classifier-%ed%95%99%ec%8a%b5&#34; &gt;높은 Resolution에서 classifier 학습&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#anchor-box-%ec%82%ac%ec%9a%a9&#34; &gt;Anchor Box 사용&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#dimension-clusters&#34; &gt;Dimension Clusters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%ec%98%88%ec%b8%a1%ea%b0%92&#34; &gt;예측값&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#fine-grained-features&#34; &gt;Fine-Grained Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%eb%84%a4%ed%8a%b8%ec%9b%8c%ed%81%ac&#34; &gt;네트워크&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;동기&#34;&gt;동기&lt;/h2&gt;
&lt;p&gt;YOLOv1는 그 당시의 State-of-the-art(SOTA)였던 Fast-RCNN과 비교하여 localization error가 상당히 컸습니다. 또한, recall도 상대적으로 굉장히 낮았습니다. 즉, object를 빼먹고 찾은 경우가 많았습니다. YOLOv2에서는 네트워크 크기를 키우지 않고 빠른 속도를 유지하면서 localization error와 낮은 recall 문제를 해결한 방법들을 아래와 같이 고안하게 됩니다.&lt;/p&gt;
&lt;h2 id=&#34;batch-normalization&#34;&gt;Batch Normalization&lt;/h2&gt;
&lt;p&gt;Batch Normalization은 covariate shift를 해결하기 위해 고안된 방법입니다. 이를 위해 한 layer에서 다음 layer로 넘어가기 전에, mini-batch 안의 데이터를 그 mini-batch의 평균과 분산으로 normalize 해 줍니다. 2015년 Proceedings of Machine Learning Research(PMLR)에 발표된 이후로 지금까지 활발하게 사용되고 있습니다.&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;http://proceedings.mlr.press/v37/ioffe15.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;YOLOv2에서는 Batch Normalization을 사용 하여 mAP가 2% 상승했습니다. 또한 Batch Normalization이 regularization 역할을 해 주어서 네트워크에서 dropout 을 제거하여도 overfitting이 발생하지 않았습니다.&lt;/p&gt;
&lt;h2 id=&#34;높은-resolution에서-classifier-학습&#34;&gt;높은 Resolution에서 classifier 학습&lt;/h2&gt;
&lt;p&gt;object detection 모델들은 보통 ImageNet 데이터셋으로 classifier를 학습한 뒤, 학습한 네트워크를 object detection을 하도록 transfer learning을 해서 사용합니다. YOLOv1도 이 방법을 사용했는데, classification 학습에는 224×224 크기의 이미지를 사용했고, detection 학습에는 448×448 크기의 이미지를 사용했습니다. 그럼 네트워크는 object detection을 학습 할 때 object detectino 뿐만 아니라 달라진 input 해상도도 학습을 해야 하게 됩니다.&lt;/p&gt;
&lt;p&gt;위와 같은 현상을 방지하기 위해 YOLOv2는 object detection 학습 전 &lt;strong&gt;448×448&lt;/strong&gt; 사이즈의 ImageNet 데이터에 대해 &lt;strong&gt;classification&lt;/strong&gt; 학습을 &lt;strong&gt;10 epoch&lt;/strong&gt; 수행했습니다. 그 결과 mAP가 4%가 증가했습니다.&lt;/p&gt;
&lt;h2 id=&#34;anchor-box-사용&#34;&gt;Anchor Box 사용&lt;/h2&gt;
&lt;p&gt;YOLOv1은 feature extractor에 Fully connected layer를 붙여서 bbox 좌표를 바로 예측했습니다. YOLOv2는 Fully connected layer를 제거하고, anchor box를 사용하여 offset을 학습했습니다. 그리고 각 anchor마다 class와 objectness를 예측했습니다.&lt;br&gt;
anchor box 사용 결과 mAP는 69.5에서 69.2로 살짝 떨어졌지만, recall은 81%에서 88%로 상승했습니다.&lt;/p&gt;
&lt;h2 id=&#34;dimension-clusters&#34;&gt;Dimension Clusters&lt;/h2&gt;
&lt;p&gt;YOLOv2에서는 anchor box를 직접 고르는 것 대신에 training set에 있는 bounding box에 k-means clustering을 수행해 anchor box를 골랐습니다. 거리를 구하는 metric으로 Euclidian distance를 사용하면 anchor box가 클 수록 error가 더 커지기 때문에, 이를 방지하기 위해 거리 metric으로 아래를 사용했습니다.
$$ d(box,centroid)=1-IOU(box, centroid) $$&lt;/p&gt;
&lt;p&gt;아래는 다양한 k값에 대하여 학습 데이터에 있는 bounding box와 가장 가까운 centroid와의 IOU 값의 평균을 그린 그래프와, k=5일 때의 anchor box입니다.&lt;br&gt;
&lt;img src=&#34;https://kimberlykang.github.io/p/yolov2_review/figure1.png&#34;
	width=&#34;405&#34;
	height=&#34;205&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov2_review/figure1_hu84c5c7f5411da2afaeabbc60e0c8b938_14216_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov2_review/figure1_hu84c5c7f5411da2afaeabbc60e0c8b938_14216_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;197&#34;
		data-flex-basis=&#34;474px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1612.08242v1.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Joseph Redmon, Ali Farhadi; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp.7265, Figure2&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;k가 커질수록 모델은 더 복잡해지지만 성능은 더 좋아지는데, 두 가지를 고려해 논문에서는 k=5를 선택했습니다. k=5일 때 k-means clustering이 찾은 VOC 2007 데이터의 anchor box가 위 그림의 흰 색 박스, COCO 데이터의 anchor box가 위 그림의 파란색 박스입니다. 데이터에 따라 다른 모양의 anchor box가 선택되는 걸 볼 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/yolov2_review/figure2.png&#34;
	width=&#34;272&#34;
	height=&#34;102&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov2_review/figure2_hu9f6c5b9270229f8ac689cde3b91a7139_11755_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov2_review/figure2_hu9f6c5b9270229f8ac689cde3b91a7139_11755_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;266&#34;
		data-flex-basis=&#34;640px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1612.08242v1.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Joseph Redmon, Ali Farhadi; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp.7265, Table1&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;위 표는 다양한 방법으로 anchor box를 생성했을 때 VOX 2007 데이터의 bounding box와 가장 가까운 anchor box와의 평균 IOU입니다. Cluster SSE는 거리 metric으로 Sum of Squared Error(Euclidean distance)를 사용한 k-means, Cluster IOU는 거리 metric으로 위에 나온 IOU를 이용한 식을 사용한 k-means, Anchor Boxes는 Anchor가 처음 소개된 논문 &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1506.01497.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[15]&lt;/a&gt;에서 사용한 anchor box입니다. [15]에서는 직접 뽑은 9개의 anchor를 사용했습니다.&lt;/p&gt;
&lt;p&gt;위 표에서 평균 IOU 값을 살펴보면, k-means를 사용 시 거리 metric으로 SSE를 사용했을 때보다 IOU를 사용했을 때 IOU 값이 높았습니다. 또한 직접 뽑은 Achorbox를 사용했을 때보다 IOU 사용 k-means를 사용했을 때가 평균 IOU 값이 높았습니다.&lt;/p&gt;
&lt;h2 id=&#34;예측값&#34;&gt;예측값&lt;/h2&gt;
&lt;p&gt;YOLOv2가 예측 값을 알아보기 전에 먼저 예측에 사용된 용어를 살펴보겠습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ (c_x, c_y) $: 셀의 좌 상단 꼭지점 좌표입니다.&lt;/li&gt;
&lt;li&gt;$ (b_x, b_y) $: bounding box 중심의 x, y 좌표입니다.&lt;/li&gt;
&lt;li&gt;$ (b_w, b_h) $: bounding box의 가로, 세로입니다.&lt;/li&gt;
&lt;li&gt;$ (p_w, p_h) $: box prior(anchor)의 가로, 세로입니다.&lt;/li&gt;
&lt;li&gt;$ (t_x, t_y, t_w, t_h, t_o) $: YOLOv2의 네트워크의 output 값입니다. target의 t를 사용합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;YOLOv2가 예측하는 bounding box의 중심 x, y좌표, bounding box의 가로, 세로, confindence score는 아래와 같습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ b_x=\sigma(t_x) + c_x $&lt;/li&gt;
&lt;li&gt;$ b_y=\sigma(t_y) + c_y $&lt;/li&gt;
&lt;li&gt;$ b_w=p_w e^{t_w}$&lt;/li&gt;
&lt;li&gt;$ b_h=p_h e^{t_h}$&lt;/li&gt;
&lt;li&gt;$ Pr(object) * IOU(b, object)=\sigma(t_o)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/yolov2_review/figure3.png&#34;
	width=&#34;402&#34;
	height=&#34;301&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov2_review/figure3_hu0e3241b5964dc481420d4fecde7e67db_16677_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov2_review/figure3_hu0e3241b5964dc481420d4fecde7e67db_16677_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 3&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;&lt;br&gt;
출처: &lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1612.08242v1.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;&lt;em&gt;Joseph Redmon, Ali Farhadi; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp.7266, Figure3&lt;/em&gt;&lt;/a&gt;&lt;br&gt;
bounding box의 center 좌표를 예측할 때, logistic activation($\sigma$)를 사용하여 예측값이 0과 1 사이 값이 되어 셀 안에서의 상대 좌표를 나타내게 됩니다. bounding box의 가로, 세로를 예측할 때는 prior로 사용되는 anchor box의 가로, 세로에 예측값을 곱합니다.&lt;/p&gt;
&lt;h2 id=&#34;fine-grained-features&#34;&gt;Fine-Grained Features&lt;/h2&gt;
&lt;p&gt;passthrough layer를 추가하여, 크기가 다른 feature map을 detection에 사용할 수 있도록 하였습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;detection에 사용되는 13×13 feature map과 바로 전 26×26 feature map을 passthrough layer를 통해 연결했습니다.&lt;/li&gt;
&lt;li&gt;(26×26×512) 크기의 feature map을 (13×13×2048)로 reshape한 뒤, 13×13 feature map에 concatenate해서 사용했습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;네트워크&#34;&gt;네트워크&lt;/h2&gt;
&lt;p&gt;YOLO는 VGG-16을 base feature extractor로 사용했습니다. VGG-16은 한 이미지에 대한 결과를 계산할 때 30.69 billion floating point operation을 합니다.
본 논문에서는 19개의 convolutional layer를 가진 Darknet 19를 사용했습니다. Darknet 19는 5.58 billion operation을 합니다.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>YOLOv1 리뷰</title>
        <link>https://kimberlykang.github.io/p/yolov1_review/</link>
        <pubDate>Mon, 06 Feb 2023 00:20:00 +0900</pubDate>
        
        <guid>https://kimberlykang.github.io/p/yolov1_review/</guid>
        <description>&lt;p&gt;본 포스트에서는 2016년 CVPR에 발표된 논문 &amp;ldquo;You Only Look Once: Unified, Real-Time Object Detection&amp;quot;을 살펴보겠습니다.&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;목차&#34;&gt;목차&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%ea%b0%9c%eb%85%90-%ec%9d%b4%ed%95%b4&#34; &gt;개념 이해&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#bounding-box&#34; &gt;Bounding Box&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#class-probability&#34; &gt;Class Probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#output&#34; &gt;Output&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%eb%84%a4%ed%8a%b8%ec%9b%8c%ed%81%ac-%ea%b5%ac%ec%a1%b0&#34; &gt;네트워크 구조&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%ed%95%99%ec%8a%b5&#34; &gt;학습&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;#%ed%85%8c%ec%8a%a4%ed%8a%b8&#34; &gt;테스트&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;개념-이해&#34;&gt;개념 이해&lt;/h2&gt;
&lt;p&gt;YOLO에서는 이미지에서 object를 찾을 때, grid를 사용합니다. 이를 간략하게 살펴보면&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;이미지를 &lt;strong&gt;S×S&lt;/strong&gt;개의 grid로 나눕니다.&lt;/li&gt;
&lt;li&gt;각 grid cell에서 &lt;strong&gt;B&lt;/strong&gt;개의 bounding box의 좌표(x, y, w, h)와 그 bounding box의 confidence score(c)를 예측합니다.&lt;/li&gt;
&lt;li&gt;각 grid cell마다 &lt;strong&gt;C&lt;/strong&gt;개의 class probability를 예측합니다.&lt;/li&gt;
&lt;li&gt;confidence score과 class probability로 최종 score와 class를 결정하고, 이 때 score가 threshold보다 높은 bounding box가 최종 object detection의 결과가 됩니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/yolov1_review/figure2.png&#34;
	width=&#34;642&#34;
	height=&#34;404&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov1_review/figure2_hud5b2df22550bbf7449e31736c4696048_251487_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov1_review/figure2_hud5b2df22550bbf7449e31736c4696048_251487_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 5&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;158&#34;
		data-flex-basis=&#34;381px&#34;
	
&gt;&lt;br&gt;
출처: &lt;em&gt;Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp.780, Figure2&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;논문에서 PASCAL VOC 데이터를 학습 실험의 예를 들어 살펴보면,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S는 하이퍼파라미터입니다. 해당 실험에서는 &lt;strong&gt;S=7&lt;/strong&gt;을 사용합니다.&lt;/li&gt;
&lt;li&gt;B 또한 하이퍼파라미터입니다. 해당 실험에서는 &lt;strong&gt;B=2&lt;/strong&gt;를 사용합니다. 따라서, 각 grid cell마다 bounding box(x, y, w, h, c)를 두 세트씩 예측합니다.&lt;/li&gt;
&lt;li&gt;PASCAL VOC 데이터는 총 class 수가 20개인 데이터입니다. 따라서 &lt;strong&gt;C=20&lt;/strong&gt;이 됩니다. 각 grid cell마다 20개의 class에 대한 class probability를 예측합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;지금까지 YOLO가 어떻게 object를 찾는지 대략적으로 살펴봤습니다. 아래에서는 각 예측값들이 어떻게 계산되고 어떤 의미를 갖는지 살펴보겠습니다.&lt;/p&gt;
&lt;h2 id=&#34;bounding-box&#34;&gt;Bounding Box&lt;/h2&gt;
&lt;p&gt;bounding box는 (x, y, w, h, c)로 이루어져 있습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(x, y): bounding box &lt;strong&gt;중심의 좌표&lt;/strong&gt;.
&lt;ul&gt;
&lt;li&gt;각 grid cell마다 B개의 bounding box를 가진다는 의미는, 이 B개의 bounding box의 중심이 해당 grid cell 안에 있다는 의미입니다.&lt;/li&gt;
&lt;li&gt;box의 네 꼭지점은 bounding box 밖에 있어도 상관이 없습니다.&lt;/li&gt;
&lt;li&gt;grid cell 안에서 상대적으로 어느 위치에 있는지를 표시하며, 0~1 사이의 값을 가집니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(w, h): bounding box의 &lt;strong&gt;너비, 높이&lt;/strong&gt;.
&lt;ul&gt;
&lt;li&gt;전체 이미지 너비, 높이에 대해 상대적인 값을 사용하며 0~1 사이의 값을 가집니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;c: box가 &lt;strong&gt;object를 갖는 것&lt;/strong&gt;에 대해 얼마나 확신하고, 그 box의 &lt;strong&gt;좌표가 얼마나 정확한&lt;/strong&gt; 가에 대한 값입니다.
&lt;ul&gt;
&lt;li&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/yolov1_review/notation1.png&#34;
	width=&#34;235&#34;
	height=&#34;37&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov1_review/notation1_hu8656dbe4c7f6134625307222faab044e_4879_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov1_review/notation1_hu8656dbe4c7f6134625307222faab044e_4879_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;635&#34;
		data-flex-basis=&#34;1524px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;좌표가 얼마나 정확한지를 계산할 때에는 &lt;strong&gt;예측한 좌표와 Ground Truth&lt;/strong&gt;(&lt;strong&gt;GT&lt;/strong&gt;) &lt;strong&gt;사이의 Intersection Over Union&lt;/strong&gt;(&lt;strong&gt;IOU&lt;/strong&gt;)를 계산합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;class-probability&#34;&gt;Class Probability&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;C: 각 grid cell이 object를 가지고 있을 때의 class probability입니다.
&lt;ul&gt;
&lt;li&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/yolov1_review/notation2.png&#34;
	width=&#34;198&#34;
	height=&#34;33&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov1_review/notation2_huf134703bb89a453953094e7423720719_4036_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov1_review/notation2_huf134703bb89a453953094e7423720719_4036_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 2&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;600&#34;
		data-flex-basis=&#34;1440px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;bounding box 갯수와 상관 없이 grid cell마다 한 세트의 class probability만 예측합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;output&#34;&gt;Output&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;이미지를 S×S grid로 나누고, 각 grid마다 B개의 bounding box와 C개 class에 대한 class probability가 있다고 할 때, 최종 output tensor의 shape은 &lt;strong&gt;S×S×&lt;/strong&gt;(&lt;strong&gt;B*5+C&lt;/strong&gt;)가 됩니다.&lt;/li&gt;
&lt;li&gt;PASCAL VOC 실험 예를 들면, S=7, B=2, C=20입니다. 따라서 총 49개의 grid가 있고, 각 grid cell마다 30개의 예측값을 갖게 됩니다. 아래는 30개의 예측값 중 하나의 예시입니다.
&lt;ul&gt;
&lt;li&gt;[x, y, w, h, c, 배경일 확률, 비행기일 확률, &amp;hellip;, 모니터일 확률]&lt;/li&gt;
&lt;li&gt;[0.4, 0.3, 0.8, 0.7, 0.9, 0.003, 0.8, &amp;hellip; , 0.012]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;네트워크-구조&#34;&gt;네트워크 구조&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;TBU&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;학습&#34;&gt;학습&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;학습할 때는, 하나의 bounding box predictor로 각 object를 예측하기 위해, 각 GT와 가장 IOU가 높은 bounding box에 해당 GT를 할당합니다. 이를 통해 bounding box들은 서로 다른 특성을 학습하게 되고, 특정 사이즈, 특정 비율, 특정 클래스를 더 잘 예측할 수 있게 됩니다.&lt;/li&gt;
&lt;li&gt;Loss
&lt;img src=&#34;https://kimberlykang.github.io/p/yolov1_review/notation3.png&#34;
	width=&#34;561&#34;
	height=&#34;380&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov1_review/notation3_hu7e88875320d34c1a084163241195336b_32557_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov1_review/notation3_hu7e88875320d34c1a084163241195336b_32557_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 3&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;354px&#34;
	
&gt;
&lt;ul&gt;
&lt;li&gt;1&lt;sup&gt;obj&lt;/sup&gt;&lt;sub&gt;i&lt;/sub&gt;: cell i에 object가 나타났는지 여부를 의미합니다.&lt;/li&gt;
&lt;li&gt;1&lt;sup&gt;obj&lt;/sup&gt;&lt;sub&gt;ij&lt;/sub&gt;: cell i에 있는 j번째 bounding box가 cell i에 있는 GT에 해당하는 예측값인지 여부입니다. 즉, cell i에 있는 j번째 bounding box가 GT와의 IOU가 가장 큰 지 여부입니다.&lt;/li&gt;
&lt;li&gt;큰 box는 편차가 조금 있어도 크게 상관 없지만 작은 box에는 큰 영향을 끼칩니다. 이 문제를 해결하기 위해 bounding box 너비와 높이 값에 루트를 씌워 줬습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;테스트&#34;&gt;테스트&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;img src=&#34;https://kimberlykang.github.io/p/yolov1_review/notation4.png&#34;
	width=&#34;564&#34;
	height=&#34;40&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov1_review/notation4_hu013f73fd8add0c13dbd52f20ed59f69f_8925_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov1_review/notation4_hu013f73fd8add0c13dbd52f20ed59f69f_8925_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 4&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;1410&#34;
		data-flex-basis=&#34;3384px&#34;
	
&gt;&lt;/li&gt;
&lt;li&gt;테스트할 때는 class probability와 confidence score를 곱해서 최종 score를 계산합니다.&lt;/li&gt;
&lt;li&gt;최종 score는 해당 박스에 class가 나타날 확률과 예측한 box가 실제 object에 얼마나 잘 맞는 지 두 가지 의미를 모두 갖고 있습니다.&lt;/li&gt;
&lt;li&gt;아래 그림의 bounding box 선의 굵기가 confidence를 의미합니다. 이 confidence와 class probability를 곱하여 threshold 이상의 결과가 최종 결과가 됩니다.
&lt;img src=&#34;https://kimberlykang.github.io/p/yolov1_review/figure2.png&#34;
	width=&#34;642&#34;
	height=&#34;404&#34;
	srcset=&#34;https://kimberlykang.github.io/p/yolov1_review/figure2_hud5b2df22550bbf7449e31736c4696048_251487_480x0_resize_box_3.png 480w, https://kimberlykang.github.io/p/yolov1_review/figure2_hud5b2df22550bbf7449e31736c4696048_251487_1024x0_resize_box_3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Image 5&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;158&#34;
		data-flex-basis=&#34;381px&#34;
	
&gt;&lt;br&gt;
출처: &lt;em&gt;Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp.780, Figure2&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
